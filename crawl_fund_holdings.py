# -*- coding: utf-8 -*-
"""
ä¸€ä¸ªç”¨äºçˆ¬å–å¤©å¤©åŸºé‡‘ç½‘å…¨å¸‚åœºåŸºé‡‘æŒä»“æ•°æ®çš„Pythonè„šæœ¬
è¯¥ç‰ˆæœ¬å¢åŠ äº†ä»æœ¬åœ°Markdownæ–‡ä»¶è§£ææŒ‡å®šåŸºé‡‘ä»£ç çš„åŠŸèƒ½
å¹¶å¢åŠ äº†çˆ¬å–è‚¡ç¥¨æ‰€å±è¡Œä¸šå’Œä¸»é¢˜ä¿¡æ¯çš„åŠŸèƒ½
ä¼˜åŒ–äº†é’ˆå¯¹å½“å‰é¡µé¢ç»“æ„çš„è§£æé€»è¾‘
"""
import os
import time
import requests
import pandas as pd
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException, StaleElementReferenceException
from bs4 import BeautifulSoup
import logging
import random
from typing import List, Dict, Optional

# --- é…ç½®æ—¥å¿—ç³»ç»Ÿ ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- æ–°å¢ï¼šè§£æMarkdownæ–‡ä»¶ï¼Œæå–åŸºé‡‘ä»£ç  ---
def parse_markdown_file(file_path: str) -> List[Dict[str, str]]:
    """
    è§£æMarkdownæ–‡ä»¶ï¼Œæå–"å¼±ä¹°å…¥"æˆ–"å¼ºä¹°å…¥"çš„åŸºé‡‘ä»£ç ã€‚
    """
    if not os.path.exists(file_path):
        logging.error(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶æœªæ‰¾åˆ° -> {file_path}")
        return []
    
    fund_codes = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        lines = content.strip().split('\n')
        for line in lines:
            if '| è¡ŒåŠ¨ä¿¡å·' in line:
                continue
            
            parts = [p.strip() for p in line.split('|')]
            if len(parts) >= 9:
                fund_code = parts[1]
                action_signal = parts[8].lower()

                if "å¼±ä¹°å…¥" in action_signal or "å¼ºä¹°å…¥" in action_signal:
                    fund_codes.append({'code': fund_code, 'name': 'N/A'})
        
        logging.info(f"âœ… ä»æŠ¥å‘Šä¸­æˆåŠŸæå–äº† {len(fund_codes)} ä¸ªç›®æ ‡åŸºé‡‘ä»£ç ã€‚")
        return fund_codes

    except Exception as e:
        logging.error(f"âŒ è§£æMarkdownæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
        return []

# --- æ–°å¢ï¼šè‚¡ç¥¨ä¿¡æ¯ç¼“å­˜ ---
stock_info_cache = {}

# --- æ–°å¢ï¼šUser-Agentæ±  ---
user_agent_pool = [
    "Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36"
]

# --- ä¼˜åŒ–ï¼šè·å–è‚¡ç¥¨è¡Œä¸šå’Œä¸»é¢˜ä¿¡æ¯ ---
def get_stock_info(stock_code: str) -> Dict[str, str]:
    """
    æ ¹æ®è‚¡ç¥¨ä»£ç çˆ¬å–ä¸œæ–¹è´¢å¯Œç½‘ï¼Œè·å–æ‰€å±è¡Œä¸šå’Œæ¦‚å¿µä¸»é¢˜ã€‚
    """
    if not stock_code or stock_code == '':
        return {'æ‰€å±è¡Œä¸š': 'æœªçŸ¥', 'æ¦‚å¿µä¸»é¢˜': 'æœªçŸ¥'}
    
    if stock_code in stock_info_cache:
        return stock_info_cache[stock_code]
    
    info = {'æ‰€å±è¡Œä¸š': 'æœªçŸ¥', 'æ¦‚å¿µä¸»é¢˜': 'æœªçŸ¥'}
    
    # éªŒè¯è‚¡ç¥¨ä»£ç æ ¼å¼ (6ä½æ•°å­—)
    if not re.match(r'^\d{6}$', stock_code):
        stock_info_cache[stock_code] = info
        return info
    
    url = f"https://wap.eastmoney.com/quote/stock/{stock_code}.html"
    headers = {
        "User-Agent": random.choice(user_agent_pool),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Accept-Encoding": "gzip, deflate",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'lxml')

        # ä¼˜åŒ–ï¼šå¤šç§æ–¹å¼å°è¯•è·å–æ‰€å±è¡Œä¸š
        industry_patterns = [
            ('div', re.compile(r'æ‰€å±è¡Œä¸š')),
            ('span', re.compile(r'æ‰€å±è¡Œä¸š')),
            ('label', re.compile(r'æ‰€å±è¡Œä¸š')),
        ]
        
        for tag, pattern in industry_patterns:
            industry_div = soup.find(tag, string=pattern)
            if industry_div:
                # å°è¯•å¤šç§æ–¹å¼è·å–è¡Œä¸šåç§°
                next_elem = industry_div.find_next_sibling()
                if next_elem:
                    info['æ‰€å±è¡Œä¸š'] = next_elem.get_text().strip()
                elif industry_div.find_next():
                    info['æ‰€å±è¡Œä¸š'] = industry_div.find_next().get_text().strip()
                break
        
        # ä¼˜åŒ–ï¼šè·å–æ¦‚å¿µä¸»é¢˜
        theme_patterns = [
            ('div', re.compile(r'æ¦‚å¿µ')),
            ('span', re.compile(r'æ¦‚å¿µ')),
            ('li', re.compile(r'æ¦‚å¿µ')),
        ]
        
        for tag, pattern in theme_patterns:
            theme_div = soup.find(tag, string=pattern)
            if theme_div:
                # æŸ¥æ‰¾ä¸»é¢˜é“¾æ¥
                theme_links = theme_div.find_all_next('a', limit=10)
                themes = []
                for link in theme_links:
                    if link.get_text().strip() and len(link.get_text().strip()) < 20:
                        themes.append(link.get_text().strip())
                if themes:
                    info['æ¦‚å¿µä¸»é¢˜'] = ', '.join(themes[:5])  # å–å‰5ä¸ªæ¦‚å¿µ
                break

        stock_info_cache[stock_code] = info
        logging.debug(f"âœ… æˆåŠŸè·å–è‚¡ç¥¨ {stock_code} ä¿¡æ¯: {info}")

    except requests.exceptions.RequestException as e:
        logging.warning(f"âŒ çˆ¬å–è‚¡ç¥¨ {stock_code} ä¿¡æ¯å¤±è´¥: {e}")
    except Exception as e:
        logging.warning(f"âŒ è§£æè‚¡ç¥¨ {stock_code} é¡µé¢å¤±è´¥: {e}")
    
    # åŠ¨æ€å»¶æ—¶ç­–ç•¥
    time.sleep(random.uniform(0.8, 2.0))
    
    return info

# --- é…ç½®Selenium ---
def setup_driver() -> Optional[webdriver.Chrome]:
    """é…ç½®å¹¶è¿”å›ä¸€ä¸ªæ— å¤´æ¨¡å¼çš„Chromeæµè§ˆå™¨é©±åŠ¨ã€‚"""
    logging.info("--- æ­£åœ¨å¯åŠ¨ ChromeDriver ---")
    try:
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--disable-extensions')
        chrome_options.add_argument('--disable-plugins')
        chrome_options.add_argument('--disable-images')  # ç¦ç”¨å›¾ç‰‡åŠ è½½åŠ é€Ÿ
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        
        chromedriver_path = os.getenv('CHROMEDRIVER_PATH', '/usr/lib/chromium-browser/chromedriver')
        service = Service(chromedriver_path)
        
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        # è®¾ç½®éšå¼ç­‰å¾…
        driver.implicitly_wait(10)
        
        logging.info("ğŸ‰ ChromeDriver å¯åŠ¨æˆåŠŸï¼")
        return driver
    except WebDriverException as e:
        logging.error(f"âŒ ChromeDriver å¯åŠ¨å¤±è´¥ï¼š{e}")
        logging.error("è¯·æ£€æŸ¥ ChromeDriver è·¯å¾„ã€ç‰ˆæœ¬æ˜¯å¦ä¸ Chrome æµè§ˆå™¨åŒ¹é…ï¼Œä»¥åŠç³»ç»Ÿä¾èµ–æ˜¯å¦å®‰è£…ã€‚")
        return None

# --- çˆ¬å–å…¨å¸‚åœºåŸºé‡‘ä»£ç åˆ—è¡¨ï¼ˆä¿ç•™åŸåŠŸèƒ½ï¼Œä½†æ–°ç‰ˆæœ¬ä¸ä¼šè°ƒç”¨ï¼‰ ---
def get_all_fund_codes() -> List[Dict[str, str]]:
    """ä»å¤©å¤©åŸºé‡‘ç½‘è·å–æ‰€æœ‰åŸºé‡‘çš„ä»£ç åˆ—è¡¨ï¼Œå¹¶ç­›é€‰å‡ºCç±»åŸºé‡‘ã€‚"""
    logging.info("æ­£åœ¨çˆ¬å–å…¨å¸‚åœºåŸºé‡‘ä»£ç åˆ—è¡¨...")
    url = "http://fund.eastmoney.com/allfund.html"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        html = response.text
        soup = BeautifulSoup(html, 'lxml')
        
        fund_list = []
        for a_tag in soup.select('#code_content a'):
            code_name_text = a_tag.get_text(strip=True)
            match = re.match(r'\((\d{6})\)(.+)', code_name_text)
            if match:
                code, name = match.groups()
                fund_list.append({'code': code, 'name': name.strip()})
        
        logging.info(f"å·²è·å– {len(fund_list)} åªåŸºé‡‘çš„ä»£ç ã€‚")
        
        c_fund_list = [fund for fund in fund_list if fund['name'].endswith('C')]
        logging.info(f"å·²ç­›é€‰å‡º {len(c_fund_list)} åªåœºå¤–Cç±»åŸºé‡‘ã€‚")
        return c_fund_list

    except requests.exceptions.RequestException as e:
        logging.error(f"âŒ çˆ¬å–åŸºé‡‘ä»£ç åˆ—è¡¨å¤±è´¥ï¼š{e}")
        return []

# --- ä¼˜åŒ–ï¼šä¸“é—¨è§£ææŒä»“è¡¨æ ¼çš„å‡½æ•° ---
def parse_holdings_table(soup: BeautifulSoup, fund_code: str, year: str) -> List[Dict]:
    """ä¸“é—¨è§£ææŒä»“è¡¨æ ¼çš„å‡½æ•°ï¼Œä¼˜åŒ–äº†é’ˆå¯¹å½“å‰é¡µé¢ç»“æ„çš„è§£æé€»è¾‘"""
    holdings_table = soup.find(id="cctable")
    if not holdings_table:
        logging.warning(f"æœªæ‰¾åˆ°æŒä»“è¡¨æ ¼ #cctable")
        return []
    
    # æ£€æŸ¥æ˜¯å¦è¿˜åœ¨åŠ è½½çŠ¶æ€
    loading_div = holdings_table.find('div', style=re.compile(r'text-align:\s*center'))
    if loading_div:
        logging.warning(f"æŒä»“è¡¨æ ¼ä»åœ¨åŠ è½½ä¸­ï¼Œè·³è¿‡ {fund_code} {year} å¹´æ•°æ®")
        return []
    
    # æŸ¥æ‰¾è¡¨æ ¼ç»“æ„
    table_patterns = [
        holdings_table.find('table'),  # æ ‡å‡†è¡¨æ ¼
        holdings_table.find('div', class_=re.compile(r'table|grid')),  # divè¡¨æ ¼
        holdings_table.find_all('tr'),  # ç›´æ¥æŸ¥æ‰¾è¡Œ
    ]
    
    holdings = []
    table_rows = None
    
    # å°è¯•å¤šç§æ–¹å¼è·å–è¡¨æ ¼è¡Œ
    for pattern in table_patterns:
        if pattern:
            if hasattr(pattern, 'find_all') and callable(pattern.find_all):
                table_rows = pattern.find_all('tr')
            elif isinstance(pattern, list):
                table_rows = pattern
            break
    
    if not table_rows or len(table_rows) <= 1:
        logging.warning(f"æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¡¨æ ¼è¡Œæ•°æ®ï¼Œfund_code: {fund_code}, year: {year}")
        return []
    
    logging.debug(f"æ‰¾åˆ° {len(table_rows)} è¡Œè¡¨æ ¼æ•°æ®")
    
    for i, row in enumerate(table_rows[1:], 1):  # è·³è¿‡è¡¨å¤´
        try:
            # å°è¯•å¤šç§æ–¹å¼è§£æåˆ—
            cols = row.find_all(['td', 'div', 'span'])
            if len(cols) < 5:
                continue
                
            # æå–è‚¡ç¥¨ä»£ç ï¼ˆé€šå¸¸åœ¨ç¬¬2åˆ—ï¼‰
            stock_code = ''
            stock_name = ''
            
            # æŸ¥æ‰¾è‚¡ç¥¨ä»£ç ï¼ˆ6ä½æ•°å­—æ ¼å¼ï¼‰
            for col in cols[1:3]:  # é€šå¸¸åœ¨ç¬¬2æˆ–ç¬¬3åˆ—
                col_text = col.get_text().strip()
                code_match = re.search(r'(\d{6})', col_text)
                if code_match:
                    stock_code = code_match.group(1)
                    # æå–è‚¡ç¥¨åç§°
                    name_match = re.search(r'([^\d\s]+)(?:\s*\d{6})?', col_text)
                    if name_match:
                        stock_name = name_match.group(1).strip()
                    break
            
            if not stock_code:
                continue
            
            # è·å–è‚¡ç¥¨è¡Œä¸šå’Œä¸»é¢˜ä¿¡æ¯
            stock_info = get_stock_info(stock_code)
            
            # æå–æŒä»“æ•°æ®
            position_ratio = ''
            shares_held = ''
            market_value = ''
            report_date = ''
            
            # å°è¯•æå–æŒä»“å æ¯”ï¼ˆé€šå¸¸åœ¨ç¬¬4åˆ—ï¼‰
            for j, col in enumerate(cols[3:6]):
                col_text = col.get_text().strip()
                if '%' in col_text or re.match(r'\d+\.?\d*%', col_text):
                    position_ratio = col_text
                elif re.match(r'\d+,\d{3}', col_text) or 'ä¸‡' in col_text or 'äº¿' in col_text:
                    if not shares_held:
                        shares_held = col_text
                    else:
                        market_value = col_text
                elif re.match(r'\d{4}-\d{2}-\d{2}', col_text):
                    report_date = col_text
            
            data = {
                'åŸºé‡‘ä»£ç ': fund_code,
                'å¹´ä»½': year,
                'è‚¡ç¥¨ä»£ç ': stock_code,
                'è‚¡ç¥¨åç§°': stock_name or cols[2].get_text().strip() if len(cols) > 2 else '',
                'æ‰€å±è¡Œä¸š': stock_info['æ‰€å±è¡Œä¸š'],
                'æ¦‚å¿µä¸»é¢˜': stock_info['æ¦‚å¿µä¸»é¢˜'],
                'æŒä»“å æ¯”': position_ratio,
                'æŒè‚¡æ•°': shares_held,
                'å¸‚å€¼': market_value,
                'æŠ¥å‘Šæ—¥æœŸ': report_date or cols[0].get_text().strip() if len(cols) > 0 else ''
            }
            
            # æ•°æ®æ¸…æ´—
            data = {k: v.strip() if isinstance(v, str) and v else '' for k, v in data.items()}
            
            # éªŒè¯å…³é”®å­—æ®µ
            if stock_code and stock_name:
                holdings.append(data)
                
        except Exception as e:
            logging.warning(f"è§£æç¬¬ {i} è¡Œæ•°æ®å¤±è´¥: {e}")
            continue
    
    logging.info(f"è§£æå®Œæˆ: {len(holdings)} æ¡æŒä»“è®°å½•")
    return holdings

# --- ä¼˜åŒ–ï¼šçˆ¬å–æŒ‡å®šåŸºé‡‘æŒä»“æ•°æ® ---
def get_fund_holdings(driver: webdriver.Chrome, fund_code: str, years_to_crawl: List[str], max_retries: int = 3) -> pd.DataFrame:
    """
    çˆ¬å–æŒ‡å®šåŸºé‡‘åœ¨è¿‘Nå¹´å†…çš„æŒä»“æ•°æ®ã€‚
    ä¼˜åŒ–äº†é’ˆå¯¹å½“å‰é¡µé¢ç»“æ„çš„ç­‰å¾…å’Œç‚¹å‡»é€»è¾‘
    """
    if driver is None:
        logging.error("WebDriver å®ä¾‹ä¸å­˜åœ¨ï¼Œè·³è¿‡çˆ¬å–ã€‚")
        return pd.DataFrame()

    fund_holdings = []
    base_url = f"https://fundf10.eastmoney.com/ccmx_{fund_code}.html"

    logging.info(f"è®¿é—®åŸºé‡‘ {fund_code} é¡µé¢: {base_url}")
    
    # é¡µé¢åŠ è½½é‡è¯•æœºåˆ¶
    for attempt in range(max_retries):
        try:
            logging.info(f"å°è¯•è®¿é—®é¡µé¢ (ç¬¬{attempt+1}æ¬¡)...")
            driver.get(base_url)
            
            # ä¼˜åŒ–ç­‰å¾…æ¡ä»¶ï¼šç­‰å¾…é¡µé¢ä¸»ä½“åŠ è½½å®Œæˆ
            wait = WebDriverWait(driver, 30)
            
            # ç­‰å¾…å…³é”®å…ƒç´ åŠ è½½
            wait.until(
                EC.any_of(
                    EC.presence_of_element_located((By.ID, "cctable")),
                    EC.presence_of_element_located((By.ID, "pagebar")),
                    EC.presence_of_element_located((By.CLASS_NAME, "tit_h3"))
                )
            )
            
            # é¢å¤–ç­‰å¾…JavaScriptæ‰§è¡Œå®Œæˆ
            time.sleep(3)
            
            page_source_check = driver.page_source
            if "æš‚æ— æ•°æ®" in page_source_check or "æ²¡æœ‰æ‰¾åˆ°" in page_source_check:
                logging.info(f"åŸºé‡‘ {fund_code} æš‚æ— æŒä»“æ•°æ®")
                return pd.DataFrame()
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸåŠ è½½åˆ°æ­£ç¡®çš„åŸºé‡‘é¡µé¢
            if f"ccmx_{fund_code}" not in page_source_check:
                logging.warning(f"é¡µé¢åŠ è½½å¯èƒ½å¼‚å¸¸ï¼Œæœªæ‰¾åˆ°åŸºé‡‘ {fund_code} çš„æŒä»“æ ‡è¯†")
                if attempt == max_retries - 1:
                    logging.error(f"åŸºé‡‘ {fund_code} é¡µé¢åŠ è½½å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡ï¼Œè·³è¿‡ã€‚")
                    return pd.DataFrame()
                time.sleep(2)
                continue
            
            logging.info("é¡µé¢åŠ è½½æˆåŠŸï¼Œå‡†å¤‡è§£ææ•°æ®ã€‚")
            break
            
        except TimeoutException:
            logging.warning(f"é¡µé¢åŠ è½½è¶…æ—¶ï¼ŒåŸºé‡‘ {fund_code} (ç¬¬{attempt+1}/{max_retries}æ¬¡é‡è¯•)")
            if attempt == max_retries - 1:
                logging.error(f"åŸºé‡‘ {fund_code} é¡µé¢åŠ è½½å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡ï¼Œè·³è¿‡ã€‚")
                return pd.DataFrame()
            time.sleep(2 ** attempt)
        except Exception as e:
            logging.error(f"è®¿é—®åŸºé‡‘ {fund_code} é¡µé¢æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ï¼š{e}")
            if attempt == max_retries - 1:
                return pd.DataFrame()
            time.sleep(2 ** attempt)

    # å¹´ä»½æ•°æ®çˆ¬å–
    for year in years_to_crawl:
        try:
            logging.info(f"æ­£åœ¨çˆ¬å– {year} å¹´æŒä»“æ•°æ®...")
            
            # é‡è¯•æœºåˆ¶
            retries = 3
            success = False
            
            for retry in range(retries):
                try:
                    # ä¼˜åŒ–ï¼šå¤šç§XPathé€‰æ‹©å™¨
                    year_selectors = [
                        f"//label[@value='{year}']",
                        f"//input[@value='{year}']",
                        f"//select[@id='jjcc']//option[@value='{year}']",
                        f"//div[contains(@class, 'pagebar')]//label[@value='{year}']",
                        f"//div[contains(@class, 'pagebar')]//input[@value='{year}']",
                        f"//a[contains(text(), '{year}')]",
                    ]
                    
                    year_element = None
                    for selector in year_selectors:
                        try:
                            elements = driver.find_elements(By.XPATH, selector)
                            for element in elements:
                                if element.is_displayed() and element.is_enabled():
                                    year_element = element
                                    break
                            if year_element:
                                break
                        except:
                            continue
                    
                    if year_element:
                        # æ»šåŠ¨åˆ°å…ƒç´ ä½ç½®
                        driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", year_element)
                        time.sleep(1)
                        
                        # å°è¯•ç‚¹å‡»
                        driver.execute_script("arguments[0].click();", year_element)
                        time.sleep(2)
                        
                        # éªŒè¯ç‚¹å‡»æˆåŠŸ
                        if "åŠ è½½ä¸­" not in driver.page_source:
                            success = True
                            break
                    else:
                        logging.warning(f"æœªæ‰¾åˆ° {year} å¹´çš„é€‰æ‹©å™¨")
                        
                except StaleElementReferenceException:
                    logging.warning(f"æ£€æµ‹åˆ° StaleElementReferenceExceptionï¼Œæ­£åœ¨é‡æ–°å°è¯•... (ç¬¬ {retry+1}/{retries} æ¬¡)")
                    time.sleep(2)
                except Exception as e:
                    logging.debug(f"ç‚¹å‡» {year} å¹´æŒ‰é’®æ—¶å‘ç”Ÿé”™è¯¯: {e}")
                    time.sleep(1)
            
            if not success:
                logging.warning(f"æ— æ³•é€‰æ‹© {year} å¹´æ•°æ®ï¼Œè·³è¿‡")
                continue

            # ç­‰å¾…æ•°æ®åŠ è½½å®Œæˆ
            wait = WebDriverWait(driver, 15)
            wait.until_not(
                EC.presence_of_element_located((By.XPATH, "//img[@src*='loading2.gif']"))
            )
            
            # è§£æé¡µé¢
            page_source = driver.page_source
            soup = BeautifulSoup(page_source, 'lxml')
            
            holdings = parse_holdings_table(soup, fund_code, year)
            fund_holdings.extend(holdings)
            logging.info(f"âœ… æˆåŠŸè·å– {len(holdings)} æ¡ {year} å¹´çš„æŒä»“è®°å½•ã€‚")
            
            # éšæœºå»¶æ—¶
            time.sleep(random.uniform(1, 2))
            
        except TimeoutException:
            logging.warning(f"åŸºé‡‘ {fund_code} åœ¨ {year} å¹´çš„æ•°æ®åŠ è½½è¶…æ—¶ï¼Œè·³è¿‡ã€‚")
            continue
        except Exception as e:
            logging.error(f"çˆ¬å–åŸºé‡‘ {fund_code} çš„ {year} å¹´æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
            continue
            
    return pd.DataFrame(fund_holdings)


def main():
    """ä¸»å‡½æ•°ï¼Œæ‰§è¡Œçˆ¬å–ä»»åŠ¡ã€‚"""
    current_year = time.localtime().tm_year
    years_to_crawl = [str(current_year), str(current_year - 1), str(current_year - 2)]
    
    # åŠ¨æ€å»¶æ—¶è®¾ç½®
    request_delay = random.uniform(1.5, 3.0)

    logging.info("=== å¤©å¤©åŸºé‡‘æŒä»“æ•°æ®çˆ¬å–å™¨ ===")
    logging.info(f"ç›®æ ‡å¹´ä»½: {', '.join(years_to_crawl)}")
    logging.info(f"éšæœºå»¶æ—¶: {request_delay:.1f}ç§’")
    
    report_file = 'market_monitor_report.md'
    fund_list_to_crawl = parse_markdown_file(report_file)
    if not fund_list_to_crawl:
        logging.error(f"æ— æ³•ä»æ–‡ä»¶ '{report_file}' è·å–åŸºé‡‘ä»£ç åˆ—è¡¨ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    logging.info(f"ğŸ“Š å‡†å¤‡çˆ¬å– {len(fund_list_to_crawl)} åªæŒ‡å®šåŸºé‡‘")
    
    output_dir = "fund_data"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    timestamp = time.strftime('%Y%m%d_%H%M%S')
    output_filename = os.path.join(output_dir, f"target_fund_holdings_with_info_{timestamp}.csv")
    
    driver = setup_driver()
    if driver is None:
        return
        
    all_holdings_df = pd.DataFrame()
    successful_funds = 0
    
    try:
        for i, fund in enumerate(fund_list_to_crawl, 1):
            fund_code = fund['code']
            fund_name = fund['name']
            
            logging.info(f"\n--- [{i}/{len(fund_list_to_crawl)}] æ­£åœ¨å¤„ç†: {fund_name} ({fund_code}) ---")
            
            holdings_df = get_fund_holdings(driver, fund_code, years_to_crawl)
            if not holdings_df.empty:
                all_holdings_df = pd.concat([all_holdings_df, holdings_df], ignore_index=True)
                successful_funds += 1
                logging.info(f"âœ… æˆåŠŸè·å– {len(holdings_df)} æ¡æŒä»“è®°å½•")
            else:
                logging.info("âŒ æœªè·å–åˆ°æ•°æ®ï¼Œç»§ç»­ä¸‹ä¸€åªåŸºé‡‘ã€‚")
            
            # åŸºé‡‘é—´å»¶æ—¶
            time.sleep(request_delay)
            
    finally:
        logging.info("çˆ¬å–ä»»åŠ¡ç»“æŸï¼Œå…³é—­ WebDriverã€‚")
        if driver:
            driver.quit()
    
    # æ•°æ®ä¿å­˜å’Œç»Ÿè®¡
    if not all_holdings_df.empty:
        logging.info("\nğŸ‰ æ•°æ®çˆ¬å–å®Œæˆ!")
        logging.info(f"ğŸ“ å·²ä¿å­˜åˆ°æ–‡ä»¶ï¼š{output_filename}")
        logging.info(f"ğŸ“ˆ æ€»è®°å½•æ•°: {len(all_holdings_df)}")
        logging.info(f"âœ… æˆåŠŸåŸºé‡‘: {successful_funds}/{len(fund_list_to_crawl)}")
        
        # æ•°æ®è´¨é‡ç»Ÿè®¡
        unique_stocks = all_holdings_df['è‚¡ç¥¨ä»£ç '].nunique()
        avg_holdings_per_fund = len(all_holdings_df) / len(fund_list_to_crawl)
        logging.info(f"ğŸ“Š å”¯ä¸€è‚¡ç¥¨æ•°: {unique_stocks}")
        logging.info(f"ğŸ“Š å¹³å‡æ¯åŸºé‡‘æŒä»“æ•°: {avg_holdings_per_fund:.1f}")
        
        try:
            all_holdings_df.to_csv(output_filename, index=False, encoding='utf-8-sig')
            logging.info(f"ğŸ’¾ CSVæ–‡ä»¶ä¿å­˜æˆåŠŸï¼Œå¤§å°: {os.path.getsize(output_filename) / 1024:.1f} KB")
        except Exception as e:
            logging.error(f"ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
            
    else:
        logging.info("âŒ æ²¡æœ‰çˆ¬å–åˆ°ä»»ä½•æ•°æ®ã€‚")
        # åˆ›å»ºç©ºç»“æœæ–‡ä»¶
        empty_df = pd.DataFrame(columns=['åŸºé‡‘ä»£ç ', 'å¹´ä»½', 'è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨åç§°', 'æ‰€å±è¡Œä¸š', 
                                       'æ¦‚å¿µä¸»é¢˜', 'æŒä»“å æ¯”', 'æŒè‚¡æ•°', 'å¸‚å€¼', 'æŠ¥å‘Šæ—¥æœŸ'])
        empty_df.to_csv(output_filename, index=False, encoding='utf-8-sig')
        logging.info(f"ğŸ“ åˆ›å»ºç©ºç»“æœæ–‡ä»¶: {output_filename}")


if __name__ == '__main__':
    main()
