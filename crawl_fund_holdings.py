# -*- coding: utf-8 -*-
"""
ä¸€ä¸ªç”¨äºçˆ¬å–å¤©å¤©åŸºé‡‘ç½‘å…¨å¸‚åœºåŸºé‡‘æŒä»“æ•°æ®çš„Pythonè„šæœ¬
å¢å¼ºç‰ˆï¼šæ·»åŠ å¹´ä»½é€‰æ‹©å™¨è¯Šæ–­å’Œè‡ªé€‚åº”å®šä½åŠŸèƒ½
"""
import os
import time
import requests
import pandas as pd
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException, StaleElementReferenceException
from bs4 import BeautifulSoup
import logging
import random
from typing import List, Dict, Optional, Tuple
from selenium.webdriver.common.action_chains import ActionChains

# --- é…ç½®æ—¥å¿—ç³»ç»Ÿï¼ˆå¢å¼ºç‰ˆï¼‰ ---
logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('crawler_debug.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# --- æ–°å¢ï¼šè§£æMarkdownæ–‡ä»¶ï¼Œæå–åŸºé‡‘ä»£ç  ---
def parse_markdown_file(file_path: str) -> List[Dict[str, str]]:
    """
    è§£æMarkdownæ–‡ä»¶ï¼Œæå–"å¼±ä¹°å…¥"æˆ–"å¼ºä¹°å…¥"çš„åŸºé‡‘ä»£ç ã€‚
    """
    if not os.path.exists(file_path):
        logging.error(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶æœªæ‰¾åˆ° -> {file_path}")
        return []
    
    fund_codes = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        lines = content.strip().split('\n')
        for line in lines:
            if '| è¡ŒåŠ¨ä¿¡å·' in line:
                continue
            
            parts = [p.strip() for p in line.split('|')]
            if len(parts) >= 9:
                fund_code = parts[1]
                action_signal = parts[8].lower()

                if "å¼±ä¹°å…¥" in action_signal or "å¼ºä¹°å…¥" in action_signal:
                    fund_codes.append({'code': fund_code, 'name': 'N/A'})
        
        logging.info(f"âœ… ä»æŠ¥å‘Šä¸­æˆåŠŸæå–äº† {len(fund_codes)} ä¸ªç›®æ ‡åŸºé‡‘ä»£ç ã€‚")
        return fund_codes

    except Exception as e:
        logging.error(f"âŒ è§£æMarkdownæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
        return []

# --- æ–°å¢ï¼šè‚¡ç¥¨ä¿¡æ¯ç¼“å­˜ ---
stock_info_cache = {}

# --- æ–°å¢ï¼šUser-Agentæ±  ---
user_agent_pool = [
    "Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
    "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36"
]

# --- è·å–è‚¡ç¥¨è¡Œä¸šå’Œä¸»é¢˜ä¿¡æ¯ ---
def get_stock_info(stock_code: str) -> Dict[str, str]:
    """
    æ ¹æ®è‚¡ç¥¨ä»£ç çˆ¬å–ä¸œæ–¹è´¢å¯Œç½‘ï¼Œè·å–æ‰€å±è¡Œä¸šå’Œæ¦‚å¿µä¸»é¢˜ã€‚
    """
    if not stock_code or stock_code == '':
        return {'æ‰€å±è¡Œä¸š': 'æœªçŸ¥', 'æ¦‚å¿µä¸»é¢˜': 'æœªçŸ¥'}
    
    if stock_code in stock_info_cache:
        return stock_info_cache[stock_code]
    
    info = {'æ‰€å±è¡Œä¸š': 'æœªçŸ¥', 'æ¦‚å¿µä¸»é¢˜': 'æœªçŸ¥'}
    
    # éªŒè¯è‚¡ç¥¨ä»£ç æ ¼å¼ (6ä½æ•°å­—)
    if not re.match(r'^\d{6}$', stock_code):
        stock_info_cache[stock_code] = info
        return info
    
    url = f"https://wap.eastmoney.com/quote/stock/{stock_code}.html"
    headers = {
        "User-Agent": random.choice(user_agent_pool),
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Accept-Language": "zh-CN,zh;q=0.9,en;q=0.8",
        "Accept-Encoding": "gzip, deflate",
        "Connection": "keep-alive",
        "Upgrade-Insecure-Requests": "1",
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'lxml')

        # å°è¯•è·å–æ‰€å±è¡Œä¸š
        industry_div = soup.find('div', string=re.compile(r'æ‰€å±è¡Œä¸š'))
        if industry_div and industry_div.find_next_sibling('div'):
            info['æ‰€å±è¡Œä¸š'] = industry_div.find_next_sibling('div').text.strip()
        
        # å°è¯•è·å–æ¦‚å¿µä¸»é¢˜
        theme_div = soup.find('div', string=re.compile(r'æ¦‚å¿µä¸»é¢˜'))
        if theme_div and theme_div.find_next_sibling('div'):
            theme_links = theme_div.find_next_sibling('div').find_all('a')
            themes = [link.text.strip() for link in theme_links]
            info['æ¦‚å¿µä¸»é¢˜'] = ', '.join(themes)

        stock_info_cache[stock_code] = info

    except requests.exceptions.RequestException as e:
        logging.warning(f"âŒ çˆ¬å–è‚¡ç¥¨ {stock_code} ä¿¡æ¯å¤±è´¥: {e}")
    except Exception as e:
        logging.warning(f"âŒ è§£æè‚¡ç¥¨ {stock_code} é¡µé¢å¤±è´¥: {e}")
    
    # åŠ¨æ€å»¶æ—¶
    time.sleep(random.uniform(0.5, 1.5))
    
    return info

# --- é…ç½®Selenium ---
def setup_driver() -> Optional[webdriver.Chrome]:
    """é…ç½®å¹¶è¿”å›ä¸€ä¸ªæ— å¤´æ¨¡å¼çš„Chromeæµè§ˆå™¨é©±åŠ¨ã€‚"""
    logging.info("--- æ­£åœ¨å¯åŠ¨ ChromeDriver ---")
    try:
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--disable-extensions')
        chrome_options.add_argument('--disable-plugins')
        chrome_options.add_argument('--disable-images')
        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36')
        # å¯ç”¨JavaScriptæ€§èƒ½æ—¥å¿—
        chrome_options.set_capability('goog:loggingPrefs', {'performance': 'ALL'})
        
        chromedriver_path = os.getenv('CHROMEDRIVER_PATH', '/usr/lib/chromium-browser/chromedriver')
        service = Service(chromedriver_path)
        
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.implicitly_wait(10)
        
        logging.info("ğŸ‰ ChromeDriver å¯åŠ¨æˆåŠŸï¼")
        return driver
    except WebDriverException as e:
        logging.error(f"âŒ ChromeDriver å¯åŠ¨å¤±è´¥ï¼š{e}")
        return None

# --- æ–°å¢ï¼šå¹´ä»½é€‰æ‹©å™¨è¯Šæ–­å‡½æ•° ---
def diagnose_year_selectors(driver: webdriver.Chrome, fund_code: str, year: str) -> Tuple[bool, str]:
    """
    è¯Šæ–­å¹´ä»½é€‰æ‹©å™¨å¹¶å°è¯•æ™ºèƒ½å®šä½
    è¿”å› (æˆåŠŸ, è°ƒè¯•ä¿¡æ¯)
    """
    logging.info(f"ğŸ” å¼€å§‹è¯Šæ–­ {year} å¹´é€‰æ‹©å™¨...")
    
    # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
    time.sleep(5)
    
    # 1. ä¿å­˜é¡µé¢æºç ç”¨äºåˆ†æ
    with open(f'debug_page_{fund_code}_{year}.html', 'w', encoding='utf-8') as f:
        f.write(driver.page_source)
    logging.info(f"ğŸ’¾ å·²ä¿å­˜è°ƒè¯•é¡µé¢: debug_page_{fund_code}_{year}.html")
    
    # 2. å¤šç§é€‰æ‹©å™¨ç­–ç•¥
    selector_strategies = [
        # ç­–ç•¥1ï¼šç²¾ç¡®åŒ¹é…
        [
            (By.XPATH, f"//label[@value='{year}']"),
            (By.XPATH, f"//input[@value='{year}']"),
            (By.XPATH, f"//option[@value='{year}']"),
        ],
        # ç­–ç•¥2ï¼šæ–‡æœ¬åŒ¹é…
        [
            (By.XPATH, f"//*[contains(text(), '{year}') and (@class='active' or @class='current')]"),
            (By.XPATH, f"//a[contains(text(), '{year}')]"),
            (By.XPATH, f"//span[contains(text(), '{year}')]"),
            (By.XPATH, f"//div[contains(text(), '{year}')]"),
        ],
        # ç­–ç•¥3ï¼šæ¨¡ç³ŠåŒ¹é…
        [
            (By.XPATH, f"//*[contains(@class, 'year') or contains(@class, 'select') or contains(@class, 'tab')]/*[contains(text(), '{year}')]"),
            (By.CSS_SELECTOR, f"[data-year='{year}']"),
            (By.CSS_SELECTOR, f".year-{year}"),
        ],
        # ç­–ç•¥4ï¼šé€šç”¨é€‰æ‹©å™¨
        [
            (By.ID, "jjcc"),
            (By.ID, "pagebar"),
            (By.CLASS_NAME, "selcc"),
        ],
    ]
    
    all_elements = []
    
    # å°è¯•æ‰€æœ‰ç­–ç•¥
    for strategy_id, selectors in enumerate(selector_strategies, 1):
        logging.info(f"  ç­–ç•¥ {strategy_id}: æµ‹è¯• {len(selectors)} ä¸ªé€‰æ‹©å™¨")
        
        for selector_id, (by, value) in enumerate(selectors, 1):
            try:
                elements = driver.find_elements(by, value)
                logging.info(f"    é€‰æ‹©å™¨ {selector_id}: {by}={value[:50]}... -> æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
                
                for i, element in enumerate(elements):
                    try:
                        text = element.text.strip()
                        is_displayed = element.is_displayed()
                        is_enabled = element.is_enabled()
                        
                        element_info = {
                            'text': text,
                            'tag': element.tag_name,
                            'class': element.get_attribute('class'),
                            'value': element.get_attribute('value'),
                            'id': element.get_attribute('id'),
                            'displayed': is_displayed,
                            'enabled': is_enabled
                        }
                        
                        all_elements.append((element_info, element))
                        
                        # å¦‚æœå…ƒç´ åŒ…å«ç›®æ ‡å¹´ä»½ä¸”å¯äº¤äº’ï¼Œä¼˜å…ˆé€‰æ‹©
                        if year in text and is_displayed and is_enabled:
                            logging.info(f"    ğŸ¯ æ‰¾åˆ°ç›®æ ‡å…ƒç´ : {text} (æ˜¾ç¤º:{is_displayed}, å¯ç”¨:{is_enabled})")
                            return True, f"ç­–ç•¥{strategy_id}-{selector_id} æ‰¾åˆ°ç›®æ ‡å…ƒç´ : {text}"
                            
                    except Exception as e:
                        logging.debug(f"    å…ƒç´  {i} åˆ†æå¤±è´¥: {e}")
                        
            except Exception as e:
                logging.debug(f"    é€‰æ‹©å™¨ {selector_id} æ‰§è¡Œå¤±è´¥: {e}")
    
    # 3. å¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•é€šç”¨äº¤äº’
    logging.info("ğŸ”„ å°è¯•é€šç”¨äº¤äº’ç­–ç•¥...")
    
    # æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„å¯ç‚¹å‡»å…ƒç´ 
    clickable_selectors = [
        (By.TAG_NAME, "select"),
        (By.TAG_NAME, "button"),
        (By.CSS_SELECTOR, "input[type='button'], input[type='submit']"),
        (By.CSS_SELECTOR, ".btn, .button, [role='button']"),
        (By.XPATH, "//*[contains(@onclick, 'year') or contains(@onclick, 'select')]"),
    ]
    
    for by, value in clickable_selectors:
        try:
            elements = driver.find_elements(by, value)
            for element in elements:
                if element.is_displayed() and element.is_enabled():
                    text = element.text.strip()
                    logging.info(f"ğŸ”˜ å‘ç°å¯ç‚¹å‡»å…ƒç´ : {text} (ç±»å‹: {element.tag_name})")
                    
                    # å°è¯•ç‚¹å‡»
                    try:
                        driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", element)
                        time.sleep(1)
                        ActionChains(driver).move_to_element(element).click().perform()
                        time.sleep(2)
                        
                        # æ£€æŸ¥ç‚¹å‡»åæ˜¯å¦å‡ºç°å¹´ä»½é€‰é¡¹
                        new_elements = driver.find_elements(By.XPATH, f"//*[contains(text(), '{year}')]")
                        if new_elements:
                            logging.info(f"âœ… é€šç”¨ç‚¹å‡»æˆåŠŸï¼Œæ‰¾åˆ° {len(new_elements)} ä¸ªå¹´ä»½å…ƒç´ ")
                            return True, f"é€šç”¨ç‚¹å‡»æˆåŠŸæ‰¾åˆ°å¹´ä»½é€‰é¡¹"
                            
                    except Exception as click_error:
                        logging.debug(f"ç‚¹å‡»å…ƒç´ å¤±è´¥: {click_error}")
                        
        except Exception as e:
            logging.debug(f"é€šç”¨é€‰æ‹©å™¨æ‰§è¡Œå¤±è´¥: {e}")
    
    # 4. æœ€åçš„è°ƒè¯•ä¿¡æ¯
    debug_info = f"æœªæ‰¾åˆ° {year} å¹´é€‰æ‹©å™¨\n"
    debug_info += f"æ€»å…±å‘ç° {len(all_elements)} ä¸ªç›¸å…³å…ƒç´ \n"
    
    if all_elements:
        debug_info += "\nå‰5ä¸ªå‘ç°çš„å…ƒç´ :\n"
        for i, (elem_info, _) in enumerate(all_elements[:5]):
            debug_info += f"  {i+1}. {elem_info['tag']}[{elem_info['text'][:30]}...] "
            debug_info += f"(æ˜¾ç¤º:{elem_info['displayed']}, å¯ç”¨:{elem_info['enabled']})\n"
    
    logging.warning(f"âŒ {debug_info}")
    return False, debug_info

# --- ä¸“é—¨è§£ææŒä»“è¡¨æ ¼çš„å‡½æ•° ---
def parse_holdings_table(soup: BeautifulSoup, fund_code: str, year: str) -> List[Dict]:
    """ä¸“é—¨è§£ææŒä»“è¡¨æ ¼çš„å‡½æ•°"""
    holdings_table = soup.find(id="cctable")
    if not holdings_table:
        logging.warning(f"æœªæ‰¾åˆ°æŒä»“è¡¨æ ¼ #cctable")
        return []
    
    # æ£€æŸ¥æ˜¯å¦è¿˜åœ¨åŠ è½½çŠ¶æ€
    loading_div = holdings_table.find('div', style=re.compile(r'text-align:\s*center'))
    if loading_div and 'æ•°æ®åŠ è½½ä¸­' in loading_div.get_text():
        logging.warning(f"æŒä»“è¡¨æ ¼ä»åœ¨åŠ è½½ä¸­ï¼Œè·³è¿‡ {fund_code} {year} å¹´æ•°æ®")
        return []
    
    holdings = []
    rows = holdings_table.find_all('tr')
    if not rows or len(rows) <= 1:
        # å°è¯•å…¶ä»–å¯èƒ½çš„è¡¨æ ¼ç»“æ„
        div_rows = holdings_table.find_all('div', recursive=False)
        if div_rows:
            rows = [BeautifulSoup(f"<tr>{div_row}</tr>", 'lxml').find('tr') for div_row in div_rows]
        else:
            logging.warning(f"æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¡¨æ ¼è¡Œæ•°æ®")
            return []
    
    for i, row in enumerate(rows[1:], 1):
        cols = row.find_all('td')
        if len(cols) >= 5:
            try:
                stock_code = cols[1].text.strip() if len(cols) > 1 else ''
                
                # æå–6ä½è‚¡ç¥¨ä»£ç 
                code_match = re.search(r'(\d{6})', stock_code)
                if code_match:
                    stock_code = code_match.group(1)
                
                # è·å–è‚¡ç¥¨è¡Œä¸šå’Œä¸»é¢˜ä¿¡æ¯
                stock_info = get_stock_info(stock_code)
                
                data = {
                    'åŸºé‡‘ä»£ç ': fund_code,
                    'å¹´ä»½': year,
                    'è‚¡ç¥¨ä»£ç ': stock_code,
                    'è‚¡ç¥¨åç§°': cols[2].text.strip() if len(cols) > 2 else '',
                    'æ‰€å±è¡Œä¸š': stock_info['æ‰€å±è¡Œä¸š'],
                    'æ¦‚å¿µä¸»é¢˜': stock_info['æ¦‚å¿µä¸»é¢˜'],
                    'æŒä»“å æ¯”': cols[3].text.strip() if len(cols) > 3 else '',
                    'æŒè‚¡æ•°': cols[4].text.strip() if len(cols) > 4 else '',
                    'å¸‚å€¼': cols[5].text.strip() if len(cols) > 5 else '',
                    'æŠ¥å‘Šæ—¥æœŸ': cols[0].text.strip() if len(cols) > 0 else ''
                }
                holdings.append(data)
            except Exception as e:
                logging.warning(f"è§£æè¡Œæ•°æ®å¤±è´¥: {e}")
                continue
    
    return holdings

# --- ä¼˜åŒ–ï¼šçˆ¬å–æŒ‡å®šåŸºé‡‘æŒä»“æ•°æ®ï¼ˆå¢å¼ºç‰ˆï¼‰ ---
def get_fund_holdings(driver: webdriver.Chrome, fund_code: str, years_to_crawl: List[str], max_retries: int = 3) -> pd.DataFrame:
    """
    çˆ¬å–æŒ‡å®šåŸºé‡‘åœ¨è¿‘Nå¹´å†…çš„æŒä»“æ•°æ®ã€‚
    å¢å¼ºç‰ˆï¼šæ·»åŠ é€‰æ‹©å™¨è¯Šæ–­å’Œè‡ªé€‚åº”ç‚¹å‡»
    """
    if driver is None:
        logging.error("WebDriver å®ä¾‹ä¸å­˜åœ¨ï¼Œè·³è¿‡çˆ¬å–ã€‚")
        return pd.DataFrame()

    fund_holdings = []
    base_url = f"https://fundf10.eastmoney.com/ccmx_{fund_code}.html"

    logging.info(f"è®¿é—®åŸºé‡‘ {fund_code} é¡µé¢: {base_url}")
    
    # é¡µé¢åŠ è½½
    for attempt in range(max_retries):
        try:
            logging.info(f"å°è¯•è®¿é—®é¡µé¢ (ç¬¬{attempt+1}æ¬¡)...")
            driver.get(base_url)
            
            wait = WebDriverWait(driver, 30)
            wait.until(
                EC.any_of(
                    EC.presence_of_element_located((By.ID, "cctable")),
                    EC.presence_of_element_located((By.ID, "pagebar")),
                    EC.presence_of_element_located((By.CLASS_NAME, "tit_h3"))
                )
            )
            
            # ç­‰å¾…JavaScriptæ‰§è¡Œ
            time.sleep(5)
            
            page_source_check = driver.page_source
            if "æš‚æ— æ•°æ®" in page_source_check or "æ²¡æœ‰æ‰¾åˆ°" in page_source_check:
                logging.info(f"åŸºé‡‘ {fund_code} æš‚æ— æŒä»“æ•°æ®")
                return pd.DataFrame()
            
            logging.info("é¡µé¢åŠ è½½æˆåŠŸï¼Œå‡†å¤‡è§£ææ•°æ®ã€‚")
            break
            
        except TimeoutException:
            logging.warning(f"é¡µé¢åŠ è½½è¶…æ—¶ï¼ŒåŸºé‡‘ {fund_code} (ç¬¬{attempt+1}/{max_retries}æ¬¡é‡è¯•)")
            if attempt == max_retries - 1:
                logging.error(f"åŸºé‡‘ {fund_code} é¡µé¢åŠ è½½å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡ï¼Œè·³è¿‡ã€‚")
                return pd.DataFrame()
            time.sleep(2 ** attempt)
        except Exception as e:
            logging.error(f"è®¿é—®åŸºé‡‘ {fund_code} é¡µé¢æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ï¼š{e}")
            if attempt == max_retries - 1:
                return pd.DataFrame()
            time.sleep(2 ** attempt)

    # å¹´ä»½æ•°æ®çˆ¬å–ï¼ˆå¢å¼ºç‰ˆï¼‰
    for year in years_to_crawl:
        try:
            logging.info(f"æ­£åœ¨çˆ¬å– {year} å¹´æŒä»“æ•°æ®...")
            
            # è¯Šæ–­å¹¶å°è¯•é€‰æ‹©å¹´ä»½
            success, debug_info = diagnose_year_selectors(driver, fund_code, year)
            
            if not success:
                logging.warning(f"âŒ {debug_info}")
                
                # å°è¯•ç›´æ¥è§£æå½“å‰é¡µé¢ï¼ˆå¯èƒ½é»˜è®¤æ˜¾ç¤ºæœ€æ–°å¹´ä»½ï¼‰
                logging.info("ğŸ”„ å°è¯•è§£æå½“å‰æ˜¾ç¤ºçš„æŒä»“æ•°æ®...")
                page_source = driver.page_source
                soup = BeautifulSoup(page_source, 'lxml')
                
                current_holdings = parse_holdings_table(soup, fund_code, year)
                if current_holdings:
                    logging.info(f"âœ… ä»å½“å‰é¡µé¢è§£æåˆ° {len(current_holdings)} æ¡è®°å½•")
                    fund_holdings.extend(current_holdings)
                else:
                    logging.warning(f"å½“å‰é¡µé¢ä¹Ÿæ— æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡ {year} å¹´")
                continue
            else:
                logging.info(f"âœ… {debug_info}")
            
            # ç­‰å¾…æ•°æ®åŠ è½½
            wait = WebDriverWait(driver, 15)
            try:
                wait.until_not(
                    EC.presence_of_element_located((By.XPATH, "//img[@src*='loading2.gif']"))
                )
            except TimeoutException:
                logging.warning("åŠ è½½åŠ¨ç”»æœªæ¶ˆå¤±ï¼Œä½†ç»§ç»­è§£æ...")
            
            # è§£ææ•°æ®
            time.sleep(3)  # é¢å¤–ç­‰å¾…
            page_source = driver.page_source
            soup = BeautifulSoup(page_source, 'lxml')
            
            holdings = parse_holdings_table(soup, fund_code, year)
            fund_holdings.extend(holdings)
            logging.info(f"âœ… æˆåŠŸè·å– {len(holdings)} æ¡ {year} å¹´çš„æŒä»“è®°å½•ã€‚")
            
        except Exception as e:
            logging.error(f"çˆ¬å–åŸºé‡‘ {fund_code} çš„ {year} å¹´æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
            continue
            
    return pd.DataFrame(fund_holdings)


def main():
    """ä¸»å‡½æ•°ï¼Œæ‰§è¡Œçˆ¬å–ä»»åŠ¡ã€‚"""
    current_year = time.localtime().tm_year
    years_to_crawl = [str(current_year), str(current_year - 1), str(current_year - 2)]
    
    request_delay = random.uniform(1, 3)

    logging.info("=== å¤©å¤©åŸºé‡‘æŒä»“æ•°æ®çˆ¬å–å™¨ï¼ˆå¢å¼ºè¯Šæ–­ç‰ˆï¼‰ ===")
    logging.info(f"ç›®æ ‡å¹´ä»½: {', '.join(years_to_crawl)}")
    
    report_file = 'market_monitor_report.md'
    fund_list_to_crawl = parse_markdown_file(report_file)
    if not fund_list_to_crawl:
        logging.error(f"æ— æ³•ä»æ–‡ä»¶ '{report_file}' è·å–åŸºé‡‘ä»£ç åˆ—è¡¨ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    logging.info(f"ğŸ“Š å‡†å¤‡çˆ¬å– {len(fund_list_to_crawl)} åªæŒ‡å®šåŸºé‡‘")
    
    output_dir = "fund_data"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    timestamp = time.strftime('%Y%m%d_%H%M%S')
    output_filename = os.path.join(output_dir, f"target_fund_holdings_with_info_{timestamp}.csv")
    
    driver = setup_driver()
    if driver is None:
        return
        
    all_holdings_df = pd.DataFrame()
    successful_funds = 0
    
    try:
        for i, fund in enumerate(fund_list_to_crawl, 1):
            fund_code = fund['code']
            fund_name = fund['name']
            
            logging.info(f"\n--- [{i}/{len(fund_list_to_crawl)}] æ­£åœ¨å¤„ç†: {fund_name} ({fund_code}) ---")
            
            holdings_df = get_fund_holdings(driver, fund_code, years_to_crawl)
            if not holdings_df.empty:
                all_holdings_df = pd.concat([all_holdings_df, holdings_df], ignore_index=True)
                successful_funds += 1
                logging.info(f"âœ… æˆåŠŸè·å– {len(holdings_df)} æ¡æŒä»“è®°å½•")
            else:
                logging.info("âŒ æœªè·å–åˆ°æ•°æ®ï¼Œç»§ç»­ä¸‹ä¸€åªåŸºé‡‘ã€‚")
            
            time.sleep(request_delay)
            
    finally:
        logging.info("çˆ¬å–ä»»åŠ¡ç»“æŸï¼Œå…³é—­ WebDriverã€‚")
        if driver:
            driver.quit()
    
    # ç»“æœå¤„ç†
    if not all_holdings_df.empty:
        logging.info("\nğŸ‰ æ•°æ®çˆ¬å–å®Œæˆ!")
        logging.info(f"ğŸ“ å·²ä¿å­˜åˆ°æ–‡ä»¶ï¼š{output_filename}")
        logging.info(f"ğŸ“ˆ æ€»è®°å½•æ•°: {len(all_holdings_df)}")
        logging.info(f"âœ… æˆåŠŸåŸºé‡‘: {successful_funds}/{len(fund_list_to_crawl)}")
        
        try:
            all_holdings_df.to_csv(output_filename, index=False, encoding='utf-8-sig')
        except Exception as e:
            logging.error(f"ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
    else:
        logging.info("âŒ æ²¡æœ‰çˆ¬å–åˆ°ä»»ä½•æ•°æ®ã€‚")

if __name__ == '__main__':
    main()
