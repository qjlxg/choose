import requests
from lxml import etree
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import pandas as pd
import numpy as np
import time
import re
from fake_useragent import UserAgent
import os
from datetime import datetime
import json
import ast
import logging

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

class FundDataCrawler:
    def __init__(self, output_dir='fund_data'):
        self.session = requests.Session()
        self.ua = UserAgent()
        self.output_dir = output_dir
        self.setup_session()
        self.setup_driver()
        self.ensure_output_directory()
    
    def ensure_output_directory(self):
        """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
        if not os.path.exists(self.output_dir):
            os.makedirs(self.output_dir)
            logging.info(f"å·²åˆ›å»ºè¾“å‡ºç›®å½•: {self.output_dir}")
            
    def setup_session(self):
        """è®¾ç½®requestsä¼šè¯"""
        headers = {
            'User-Agent': self.ua.random,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'https://fundf10.eastmoney.com/',
        }
        self.session.headers.update(headers)
    
    def setup_driver(self):
        """åˆå§‹åŒ–seleniumæµè§ˆå™¨é©±åŠ¨"""
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument(f'--user-agent={self.ua.random}')
        
        try:
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            self.wait = WebDriverWait(self.driver, 10)
            logging.info("æµè§ˆå™¨é©±åŠ¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logging.error(f"æµè§ˆå™¨é©±åŠ¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.driver = None
    
    def close_driver(self):
        """å…³é—­æµè§ˆå™¨é©±åŠ¨"""
        if self.driver:
            self.driver.quit()
    
    def get_all_fund_codes(self):
        """
        çˆ¬å–å¤©å¤©åŸºé‡‘ç½‘æ‰€æœ‰åŸºé‡‘ä»£ç å’Œåç§°
        è¿”å›: DataFrameæ ¼å¼çš„åŸºé‡‘åˆ—è¡¨
        """
        url = "http://fund.eastmoney.com/allfund.html"
        
        try:
            logging.info("æ­£åœ¨è·å–å…¨å¸‚åœºåŸºé‡‘åˆ—è¡¨...")
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            # ä½¿ç”¨lxmlè§£æ
            html = etree.HTML(response.text)
            
            # XPathè·å–åŸºé‡‘ä»£ç å’Œåç§°
            fund_items = html.xpath('//*[@id="code_content"]/div/ul/li/div/a[1]/text()')
            
            fund_list = []
            for item in fund_items:
                # æå–6ä½åŸºé‡‘ä»£ç 
                code_match = re.search(r'\((\d{6})\)', item)
                if code_match:
                    code = code_match.group(1)
                    name = re.sub(r'^\(.*?ï¼‰', '', item).strip()
                    fund_list.append({
                        'fund_code': code,
                        'fund_name': name
                    })
            
            df = pd.DataFrame(fund_list)
            logging.info(f"æˆåŠŸè·å– {len(df)} åªåŸºé‡‘")
            
            # ä¿å­˜åˆ°æœ¬åœ°
            output_path = os.path.join(self.output_dir, 'all_fund_list.csv')
            df.to_csv(output_path, index=False, encoding='utf-8-sig')
            logging.info(f"åŸºé‡‘åˆ—è¡¨å·²ä¿å­˜è‡³: {output_path}")
            return df
            
        except Exception as e:
            logging.error(f"è·å–åŸºé‡‘åˆ—è¡¨å¤±è´¥: {e}")
            return pd.DataFrame()
    
    def get_fund_info(self, fund_code):
        """
        è·å–å•åªåŸºé‡‘çš„åŸºæœ¬ä¿¡æ¯
        """
        url = f"https://fundf10.eastmoney.com/jbgk_{fund_code}.html"
        
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æå–åŸºé‡‘åŸºæœ¬ä¿¡æ¯
            info = {}
            
            # åŸºé‡‘åç§°
            name_elem = soup.select_one('#bodydiv > div > div.fundInfo > div.title > h1')
            info['fund_name'] = name_elem.text.strip() if name_elem else ''
            
            # åŸºé‡‘ä»£ç 
            info['fund_code'] = fund_code
            
            # å…¶ä»–ä¿¡æ¯ï¼ˆæˆç«‹æ—¥æœŸã€åŸºé‡‘ç»ç†ç­‰ï¼‰
            info_table = soup.select_one('#bodydiv > div > div.fundInfo > div.info')
            if info_table:
                rows = info_table.find_all('p')
                for row in rows:
                    text = row.get_text().strip()
                    if 'æˆç«‹æ—¥æœŸ' in text:
                        info['establish_date'] = text.split('ï¼š')[-1].strip()
                    elif 'åŸºé‡‘ç»ç†' in text:
                        info['manager'] = text.split('ï¼š')[-1].strip()
                    elif 'åŸºé‡‘å…¬å¸' in text:
                        info['company'] = text.split('ï¼š')[-1].strip()
            
            return info
            
        except Exception as e:
            logging.error(f"è·å–åŸºé‡‘ {fund_code} ä¿¡æ¯å¤±è´¥: {e}")
            return {}
    
    def parse_holding_table_from_content(self, content, year, quarter=None):
        """
        ä»HTMLå†…å®¹ä¸­è§£ææŒä»“è¡¨æ ¼æ•°æ®
        """
        holdings = []
        
        # æ„å»ºå­£åº¦æ ‡è¯†
        if quarter:
            quarter_pattern = f'{year}å¹´{quarter}å­£åº¦è‚¡ç¥¨æŠ•èµ„æ˜ç»†'
        else:
            quarter_pattern = f'{year}å¹´.*?å­£åº¦è‚¡ç¥¨æŠ•èµ„æ˜ç»†'
        
        # æŒ‰å­£åº¦åˆ†å‰²å†…å®¹
        sections = re.split(quarter_pattern, content, flags=re.DOTALL)
        
        # å–åŒ¹é…çš„å­£åº¦å†…å®¹
        target_section = None
        for i, section in enumerate(sections):
            if re.search(quarter_pattern, section, re.DOTALL):
                target_section = sections[i + 1] if i + 1 < len(sections) else ''
                break
        
        if not target_section:
            # å¦‚æœæŒ‡å®šå­£åº¦æ²¡æ‰¾åˆ°ï¼Œå–æœ€æ–°çš„å­£åº¦
            quarter_matches = re.findall(r'(\d+)å¹´(\d+)å­£åº¦è‚¡ç¥¨æŠ•èµ„æ˜ç»†', content)
            if quarter_matches:
                latest_quarter = max(quarter_matches, key=lambda x: (int(x[0]), int(x[1])))
                target_section = re.split(f'{latest_quarter[0]}å¹´{latest_quarter[1]}å­£åº¦è‚¡ç¥¨æŠ•èµ„æ˜ç»†', content)[-1]
        
        if not target_section:
            logging.warning(f"æœªæ‰¾åˆ° {year} å¹´çš„æŒä»“æ•°æ®")
            return holdings
        
        # è§£æè¡¨æ ¼è¡Œ
        # åŒ¹é…è¡¨æ ¼è¡Œï¼šåºå·|è‚¡ç¥¨ä»£ç |è‚¡ç¥¨åç§°|...|å å‡€å€¼æ¯”ä¾‹|æŒè‚¡æ•°|æŒä»“å¸‚å€¼
        table_row_pattern = r'<tr>.*?<td>(\d+)</td>.*?<td>(\d{5,6})</td>.*?<td>(.*?)</td>.*?<td>([\d.]+%)</td>.*?<td>([\d.,]+)</td>.*?<td>([\d.,]+)</td>'
        
        # æŸ¥æ‰¾æ‰€æœ‰åŒ¹é…çš„è¡Œ
        rows = re.findall(table_row_pattern, target_section, re.DOTALL)
        
        for row in rows:
            try:
                holding = {
                    'year': year,
                    'quarter': quarter if quarter else None,
                    'rank': int(row[0]),
                    'stock_code': row[1].strip(),
                    'stock_name': re.sub(r'<.*?>', '', row[2]).strip(),  # ç§»é™¤HTMLæ ‡ç­¾
                    'hold_ratio': float(row[3].replace('%', '')),
                    'hold_shares': float(row[4].replace(',', '')),
                    'hold_value': float(row[5].replace(',', '')),
                }
                holdings.append(holding)
            except (ValueError, IndexError) as e:
                logging.debug(f"è§£æè¡Œæ•°æ®å¤±è´¥: {row}, é”™è¯¯: {e}")
                continue
        
        logging.info(f"è§£æåˆ° {len(holdings)} æ¡ {year}å¹´æŒä»“è®°å½•")
        return holdings
    
    def get_fund_holdings_from_api(self, fund_code, years=None, quarter=None):
        """
        é€šè¿‡APIé“¾æ¥çˆ¬å–åŸºé‡‘æŒä»“æ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬
        """
        if years is None:
            years = [datetime.now().year]
        
        all_holdings = []
        
        for year in years:
            try:
                logging.info(f"æ­£åœ¨é€šè¿‡APIçˆ¬å–åŸºé‡‘ {fund_code} {year}å¹´æŒä»“...")
                
                # æ„å»ºAPIé“¾æ¥
                url = f"https://fundf10.eastmoney.com/FundArchivesDatas.aspx?type=jjcc&code={fund_code}&topline=10&year={year}"
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                
                # è§£æè¿”å›çš„JavaScriptæ•°æ®
                match = re.search(r'var apidata=(.*?);', response.text, re.DOTALL)
                if not match:
                    logging.warning(f"æœªåœ¨å“åº”ä¸­æ‰¾åˆ°åŸºé‡‘ {fund_code} {year}å¹´çš„æ•°æ®")
                    continue
                
                # æå–æ•°æ®å­—ç¬¦ä¸²
                api_data_str = match.group(1).strip()
                
                # æ›´robustçš„JSONä¿®å¤æ–¹æ³•
                try:
                    # æå–contentéƒ¨åˆ†
                    content_match = re.search(r'content:"(.*?)"', api_data_str, re.DOTALL)
                    if content_match:
                        content_value = content_match.group(1)
                        # æ¸…ç†contentä¸­çš„HTMLæ ‡ç­¾å’Œç‰¹æ®Šå­—ç¬¦
                        content_cleaned = content_value.replace('\r', '').replace('\t', ' ')
                        
                        # æ„å»ºå®Œæ•´çš„JSONå­—ç¬¦ä¸²
                        arryear_match = re.search(r'arryear:\[(.*?)\]', api_data_str)
                        arryear_value = arryear_match.group(1) if arryear_match else '[]'
                        
                        cur_year_match = re.search(r'curyear:(\d+)', api_data_str)
                        cur_year_value = cur_year_match.group(1) if cur_year_match else str(year)
                        
                        # æ„å»ºJSON
                        json_data = {
                            "content": content_cleaned,
                            "arryear": [int(y.strip()) for y in arryear_value.split(',') if y.strip().isdigit()],
                            "curyear": int(cur_year_value)
                        }
                    else:
                        logging.warning(f"æ— æ³•æå–contentæ•°æ®: {fund_code} {year}")
                        continue
                        
                except Exception as json_error:
                    logging.error(f"JSONè§£æå¤±è´¥ {fund_code} {year}: {json_error}")
                    continue
                
                # è§£ææŒä»“æ•°æ®
                holdings = self.parse_holding_table_from_content(json_data['content'], year, quarter)
                
                # æ·»åŠ åŸºé‡‘ä¿¡æ¯
                for holding in holdings:
                    holding['fund_code'] = fund_code
                    holding['fund_name'] = ''  # åç»­åˆå¹¶æ—¶æ·»åŠ 
                
                all_holdings.extend(holdings)
                time.sleep(1)  # é¿å…è¯·æ±‚è¿‡å¿«
                
            except Exception as e:
                logging.error(f"çˆ¬å–åŸºé‡‘ {fund_code} {year}å¹´æŒä»“å¤±è´¥: {e}")
                continue
        
        if all_holdings:
            df = pd.DataFrame(all_holdings)
            # ç¡®ä¿æ•°å€¼åˆ—ä¸ºæ•°å€¼ç±»å‹
            numeric_columns = ['hold_ratio', 'hold_shares', 'hold_value']
            for col in numeric_columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
            
            logging.info(f"åŸºé‡‘ {fund_code} æ€»å…±è·å– {len(df)} æ¡æŒä»“è®°å½•")
            return df
        else:
            logging.warning(f"åŸºé‡‘ {fund_code} æœªè·å–åˆ°ä»»ä½•æŒä»“æ•°æ®")
            return pd.DataFrame()
    
    def get_fund_holdings_selenium_backup(self, fund_code, years=None):
        """
        Seleniumå¤‡ä»½æ–¹æ³•ï¼Œå½“APIæ–¹æ³•å¤±è´¥æ—¶ä½¿ç”¨
        """
        if years is None:
            years = [datetime.now().year]
        
        if not self.driver:
            logging.warning("æµè§ˆå™¨é©±åŠ¨ä¸å¯ç”¨")
            return pd.DataFrame()
        
        all_holdings = []
        
        for year in years:
            try:
                logging.info(f"ä½¿ç”¨Seleniumçˆ¬å–åŸºé‡‘ {fund_code} {year}å¹´æŒä»“...")
                
                # è®¿é—®åŸºé‡‘æŒä»“é¡µé¢
                url = f"https://fundf10.eastmoney.com/ccmx_{fund_code}.html"
                self.driver.get(url)
                time.sleep(3)
                
                # åˆ‡æ¢åˆ°æŒ‡å®šå¹´ä»½
                try:
                    year_button = self.wait.until(
                        EC.element_to_be_clickable(
                            (By.XPATH, f"//*[@id='pagebar']/div/label[@value='{year}']")
                        )
                    )
                    year_button.click()
                    time.sleep(3)
                except Exception as e:
                    logging.warning(f"å¹´ä»½åˆ‡æ¢å¤±è´¥ {year}: {e}")
                    continue
                
                # è§£æé¡µé¢å†…å®¹
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                
                # æŸ¥æ‰¾æ‰€æœ‰æŒä»“è¡¨æ ¼
                tables = soup.select('#cctable > div > div table')
                
                if not tables:
                    logging.warning(f"æœªæ‰¾åˆ°åŸºé‡‘ {fund_code} {year}å¹´æŒä»“è¡¨æ ¼")
                    continue
                
                # è§£æè¡¨æ ¼
                for table in tables:
                    rows = table.find_all('tr')[1:]  # è·³è¿‡è¡¨å¤´
                    for row in rows:
                        cols = row.find_all(['td', 'th'])
                        if len(cols) >= 7:
                            holding = {
                                'fund_code': fund_code,
                                'year': year,
                                'rank': len(all_holdings) + 1,
                                'stock_code': cols[1].text.strip() if len(cols) > 1 else '',
                                'stock_name': cols[2].text.strip() if len(cols) > 2 else '',
                                'hold_ratio': cols[3].text.strip() if len(cols) > 3 else '',
                                'hold_shares': cols[4].text.strip() if len(cols) > 4 else '',
                                'hold_value': cols[5].text.strip() if len(cols) > 5 else '',
                            }
                            
                            # æ•°æ®æ¸…æ´—
                            holding['hold_ratio'] = float(holding['hold_ratio'].replace('%', '')) if holding['hold_ratio'] else 0
                            holding['hold_shares'] = float(holding['hold_shares'].replace(',', '')) if holding['hold_shares'] else 0
                            holding['hold_value'] = float(holding['hold_value'].replace(',', '')) if holding['hold_value'] else 0
                            
                            all_holdings.append(holding)
                
                logging.info(f"åŸºé‡‘ {fund_code} {year}å¹´è·å–åˆ° {len(rows)} æ¡æŒä»“è®°å½•")
                time.sleep(2)
                
            except Exception as e:
                logging.error(f"Seleniumçˆ¬å–åŸºé‡‘ {fund_code} {year}å¹´æŒä»“å¤±è´¥: {e}")
                continue
        
        if all_holdings:
            df = pd.DataFrame(all_holdings)
            return df
        else:
            return pd.DataFrame()
    
    def batch_crawl_fund_holdings(self, fund_list, max_funds=100, years=None):
        """
        æ‰¹é‡çˆ¬å–åŸºé‡‘æŒä»“æ•°æ® - ä¼˜åŒ–ç‰ˆæœ¬
        """
        if years is None:
            years = [datetime.now().year]
        
        all_data = []
        failed_funds = []
        
        for idx, fund in fund_list.iterrows():
            if idx >= max_funds:
                break
                
            fund_code = fund['fund_code']
            fund_name = fund.get('fund_name', '')
            
            logging.info(f"\n[{idx+1}/{min(max_funds, len(fund_list))}] æ­£åœ¨å¤„ç†: {fund_name} ({fund_code})")
            
            # è·å–åŸºé‡‘åŸºæœ¬ä¿¡æ¯
            fund_info = self.get_fund_info(fund_code)
            full_fund_name = fund_info.get('fund_name', fund_name)
            company = fund_info.get('company', '')
            
            # é¦–å…ˆå°è¯•APIæ–¹æ³•
            holdings = self.get_fund_holdings_from_api(fund_code, years)
            
            # å¦‚æœAPIæ–¹æ³•å¤±è´¥ï¼Œå°è¯•Seleniumæ–¹æ³•
            if holdings.empty:
                logging.warning(f"APIæ–¹æ³•å¤±è´¥ï¼Œå°è¯•Seleniumæ–¹æ³•...")
                holdings = self.get_fund_holdings_selenium_backup(fund_code, years)
            
            if not holdings.empty:
                # åˆå¹¶åŸºæœ¬ä¿¡æ¯å’ŒæŒä»“æ•°æ®
                holdings['fund_name'] = full_fund_name
                holdings['manager'] = fund_info.get('manager', '')
                holdings['company'] = company
                all_data.append(holdings)
                logging.info(f"âœ“ æˆåŠŸè·å– {len(holdings)} æ¡è®°å½•")
            else:
                failed_funds.append(fund_code)
                logging.error(f"âœ— è·å–å¤±è´¥: {fund_code}")
            
            # é¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(2)
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        if all_data:
            result_df = pd.concat(all_data, ignore_index=True)
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            logging.info(f"æ•°æ®è´¨é‡æ£€æŸ¥:")
            logging.info(f"  - æ€»è®°å½•æ•°: {len(result_df)}")
            logging.info(f"  - ç©ºè‚¡ç¥¨ä»£ç è®°å½•: {(result_df['stock_code'] == '').sum()}")
            logging.info(f"  - å¹³å‡æŒä»“æ¯”ä¾‹: {result_df['hold_ratio'].mean():.2f}%")
            
            # ä¿å­˜ç»“æœ
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'fund_holdings_{timestamp}.csv'
            output_path = os.path.join(self.output_dir, filename)
            result_df.to_csv(output_path, index=False, encoding='utf-8-sig')
            
            # ä¿å­˜å¤±è´¥çš„åŸºé‡‘åˆ—è¡¨
            if failed_funds:
                failed_file = os.path.join(self.output_dir, f'failed_funds_{timestamp}.txt')
                with open(failed_file, 'w', encoding='utf-8') as f:
                    f.write('\n'.join(failed_funds))
                logging.info(f"å¤±è´¥çš„åŸºé‡‘å·²ä¿å­˜è‡³: {failed_file}")
            
            logging.info(f"\næ‰¹é‡çˆ¬å–å®Œæˆï¼")
            logging.info(f"æˆåŠŸå¤„ç† {len(all_data)} åªåŸºé‡‘")
            logging.info(f"æ€»å…±è·å– {len(result_df)} æ¡æŒä»“è®°å½•")
            logging.info(f"æ•°æ®å·²ä¿å­˜è‡³: {output_path}")
            
            return result_df
        else:
            logging.error("æœªè·å–åˆ°ä»»ä½•æ•°æ®")
            return pd.DataFrame()
    
    def analyze_holdings(self, holdings_df):
        """
        åˆ†ææŒä»“æ•°æ® - å¢å¼ºç‰ˆ
        """
        if holdings_df.empty:
            logging.warning("æ²¡æœ‰æ•°æ®å¯åˆ†æ")
            return
        
        print("\n" + "="*60)
        print("                åŸºé‡‘æŒä»“æ•°æ®åˆ†ææŠ¥å‘Š")
        print("="*60)
        
        # 1. åŸºæœ¬ç»Ÿè®¡
        print(f"\nğŸ“Š åŸºæœ¬ç»Ÿè®¡:")
        print(f"   æ€»è®°å½•æ•°: {len(holdings_df):,}")
        print(f"   æ¶‰åŠåŸºé‡‘æ•°: {holdings_df['fund_code'].nunique()}")
        print(f"   æ¶‰åŠè‚¡ç¥¨æ•°: {holdings_df['stock_name'].nunique()}")
        print(f"   æ•°æ®æ—¶é—´è·¨åº¦: {holdings_df['year'].min()}-{holdings_df['year'].max()}")
        
        # 2. æŒ‰åŸºé‡‘å…¬å¸ç»Ÿè®¡
        if 'company' in holdings_df.columns and holdings_df['company'].notna().any():
            print(f"\nğŸ¢ å„åŸºé‡‘å…¬å¸æŒä»“æ¦‚è§ˆ (Top 5):")
            company_stats = holdings_df.groupby('company').agg({
                'stock_name': 'nunique',
                'hold_value': 'sum',
                'fund_code': 'nunique'
            }).round(2)
            company_stats.columns = ['æŒä»“è‚¡ç¥¨æ•°', 'æ€»æŒä»“å¸‚å€¼(ä¸‡å…ƒ)', 'ç®¡ç†åŸºé‡‘æ•°']
            company_stats = company_stats.sort_values('æ€»æŒä»“å¸‚å€¼(ä¸‡å…ƒ)', ascending=False).head(5)
            print(company_stats.to_string())
        
        # 3. çƒ­é—¨è‚¡ç¥¨ç»Ÿè®¡
        print(f"\nğŸ”¥ çƒ­é—¨æŒä»“è‚¡ç¥¨ Top 10:")
        hot_stocks = holdings_df.groupby('stock_name').agg({
            'fund_code': 'nunique',
            'hold_value': 'sum',
            'hold_ratio': lambda x: (x * holdings_df.loc[x.index, 'hold_shares'].sum()).sum() / len(x)
        }).round(2)
        hot_stocks.columns = ['æŒæœ‰åŸºé‡‘æ•°', 'æ€»æŒä»“å¸‚å€¼(ä¸‡å…ƒ)', 'å¹³å‡æŒä»“æ¯”ä¾‹(%)']
        hot_stocks = hot_stocks.sort_values('æ€»æŒä»“å¸‚å€¼(ä¸‡å…ƒ)', ascending=False).head(10)
        print(hot_stocks.to_string())
        
        # 4. æŒä»“é›†ä¸­åº¦åˆ†æ
        print(f"\nğŸ¯ æŒä»“é›†ä¸­åº¦åˆ†æ (Top 5):")
        concentration = holdings_df.groupby('fund_code').apply(
            lambda x: x['hold_ratio'].sum() if not x.empty else 0
        ).sort_values(ascending=False).head(5)
        
        for fund_code, conc in concentration.items():
            fund_name = holdings_df[holdings_df['fund_code'] == fund_code]['fund_name'].iloc[0]
            print(f"   {fund_name} ({fund_code}): {conc:.1f}%")
        
        # 5. è¡Œä¸š/æ¿å—åˆ†å¸ƒï¼ˆå¦‚æœæœ‰åˆ†ç±»ä¿¡æ¯ï¼‰
        if 'stock_code' in holdings_df.columns:
            # ç®€å•çš„å‰åå¤§/ä¸­å°æ¿/åˆ›ä¸šæ¿åˆ†ç±»
            def classify_stock(code):
                if pd.isna(code):
                    return 'æœªçŸ¥'
                code_str = str(code)
                if code_str.startswith('0') and len(code_str) >= 6:
                    if code_str.startswith('002'):
                        return 'ä¸­å°æ¿'
                    elif code_str.startswith('300'):
                        return 'åˆ›ä¸šæ¿'
                    else:
                        return 'ä¸»æ¿'
                elif code_str.startswith('6') or code_str.startswith('688'):
                    return 'ä¸»æ¿'
                elif code_str.startswith('11') or code_str.startswith('12'):
                    return 'æ¸¯è‚¡'
                else:
                    return 'å…¶ä»–'
            
            holdings_df['market'] = holdings_df['stock_code'].apply(classify_stock)
            
            print(f"\nğŸ“ˆ å¸‚åœºåˆ†å¸ƒ:")
            market_dist = holdings_df.groupby('market').agg({
                'hold_value': 'sum',
                'stock_name': 'nunique'
            }).round(2)
            market_dist.columns = ['æ€»æŒä»“å¸‚å€¼(ä¸‡å…ƒ)', 'è‚¡ç¥¨æ•°']
            market_dist['å æ¯”'] = (market_dist['æ€»æŒä»“å¸‚å€¼(ä¸‡å…ƒ)'] / market_dist['æ€»æŒä»“å¸‚å€¼(ä¸‡å…ƒ)'].sum() * 100).round(1)
            print(market_dist.sort_values('æ€»æŒä»“å¸‚å€¼(ä¸‡å…ƒ)', ascending=False).to_string())

def get_fund_codes_from_report(file_path):
    """
    ä»å¸‚åœºç›‘æ§æŠ¥å‘Šä¸­è¯»å–"å¼±ä¹°å…¥"å’Œ"å¼ºä¹°å…¥"çš„åŸºé‡‘ä»£ç ã€‚
    """
    fund_codes = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…"å¼±ä¹°å…¥"æˆ–"å¼ºä¹°å…¥"è¡Œ
            pattern = re.compile(r'\|\s*(\d{6})\s*\|.*?\s*\|\s*(å¼±ä¹°å…¥|å¼ºä¹°å…¥)\s*\|')
            matches = pattern.findall(content)
            for code, signal in matches:
                if code not in fund_codes:
                    fund_codes.append(code)
        logging.info(f"ä»æŠ¥å‘Šä¸­è·å–åˆ° {len(fund_codes)} ä¸ªå¾…çˆ¬å–çš„åŸºé‡‘ä»£ç ã€‚")
    except FileNotFoundError:
        logging.error(f"é”™è¯¯: æœªæ‰¾åˆ°æ–‡ä»¶ {file_path}")
        return []
    except Exception as e:
        logging.error(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return []
    
    return fund_codes

def main():
    """ä¸»ç¨‹åº - å¢å¼ºç‰ˆ"""
    crawler = FundDataCrawler()
    
    try:
        # æ­¥éª¤1: ä»æŠ¥å‘Šæ–‡ä»¶ä¸­è·å–åŸºé‡‘åˆ—è¡¨
        print("ğŸš€ å¯åŠ¨åŸºé‡‘æŒä»“æ•°æ®çˆ¬å–ç³»ç»Ÿ")
        print("=" * 50)
        
        print("\nğŸ“‹ æ­¥éª¤1: ä»æŠ¥å‘Šä¸­è¯»å–åŸºé‡‘åˆ—è¡¨")
        report_file = 'market_monitor_report.md'
        codes_to_crawl = get_fund_codes_from_report(report_file)
        
        if not codes_to_crawl:
            print("âŒ æœªæ‰¾åˆ°éœ€è¦çˆ¬å–çš„åŸºé‡‘ä»£ç ï¼Œå°è¯•è·å–å…¨å¸‚åœºå‰100åªåŸºé‡‘...")
            # å¤‡ç”¨æ–¹æ¡ˆï¼šè·å–å…¨å¸‚åœºå‰100åªåŸºé‡‘
            all_funds = crawler.get_all_fund_codes()
            if not all_funds.empty:
                fund_list_df = all_funds.head(100)
                print(f"ğŸ“ˆ ä½¿ç”¨å…¨å¸‚åœºå‰100åªåŸºé‡‘è¿›è¡Œæµ‹è¯•")
            else:
                print("âŒ æ— æ³•è·å–ä»»ä½•åŸºé‡‘æ•°æ®ï¼Œç¨‹åºé€€å‡º")
                return
        else:
            fund_list_df = pd.DataFrame({'fund_code': codes_to_crawl, 'fund_name': ''})
        
        # æ­¥éª¤2: æ‰¹é‡çˆ¬å–æŒä»“æ•°æ®
        print(f"\nğŸ” æ­¥éª¤2: æ‰¹é‡çˆ¬å–æŒä»“æ•°æ®")
        print(f"   ç›®æ ‡åŸºé‡‘æ•°: {len(fund_list_df)}")
        years_to_crawl = [2025, 2024, 2023]  # æŒ‡å®šçˆ¬å–å¹´ä»½
        print(f"   çˆ¬å–å¹´ä»½: {', '.join(map(str, years_to_crawl))}")
        
        holdings_data = crawler.batch_crawl_fund_holdings(
            fund_list_df, 
            max_funds=len(fund_list_df),
            years=years_to_crawl
        )
        
        # æ­¥éª¤3: æ•°æ®åˆ†æ
        if not holdings_data.empty:
            print(f"\nğŸ“Š æ­¥éª¤3: æ•°æ®åˆ†æ")
            crawler.analyze_holdings(holdings_data)
        else:
            print("âŒ æœªè·å–åˆ°ä»»ä½•æŒä»“æ•°æ®")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­ç¨‹åº")
    except Exception as e:
        logging.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
    finally:
        crawler.close_driver()
        print("\nğŸ‘‹ ç¨‹åºç»“æŸ")

if __name__ == "__main__":
    main()
