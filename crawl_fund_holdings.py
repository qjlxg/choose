# -*- coding: utf-8 -*-
"""
ä¸€ä¸ªç”¨äºçˆ¬å–å¤©å¤©åŸºé‡‘ç½‘å…¨å¸‚åœºåŸºé‡‘æŒä»“æ•°æ®çš„Pythonè„šæœ¬
"""
import os
import time
import requests
import pandas as pd
import re  # æ–°å¢ï¼šç”¨äºæ­£åˆ™è¡¨è¾¾å¼åŒ¹é…åŸºé‡‘ä»£ç 
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
# æ–°å¢çš„å¯¼å…¥ï¼Œç”¨äºæ˜¾å¼ç­‰å¾…
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException

# é…ç½®Selenium
def setup_driver():
    """é…ç½®å¹¶è¿”å›ä¸€ä¸ªæ— å¤´æ¨¡å¼çš„Chromeæµè§ˆå™¨é©±åŠ¨ã€‚"""
    chrome_options = Options()
    chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼ï¼Œä¸åœ¨çª—å£ä¸­æ˜¾ç¤º
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    
    # ä¿®æ­£åçš„ä»£ç ï¼šç›´æ¥æŒ‡å®š chromedriver çš„å®Œæ•´è·¯å¾„
    # åœ¨ GitHub Actions çš„ Ubuntu ç¯å¢ƒä¸­ï¼Œchromedriver é€šå¸¸ä½äºæ­¤è·¯å¾„
    service = Service('/usr/lib/chromium-browser/chromedriver')

    driver = webdriver.Chrome(service=service, options=chrome_options)
    return driver

# çˆ¬å–å…¨å¸‚åœºåŸºé‡‘ä»£ç åˆ—è¡¨ï¼ˆæ”¹è¿›ç‰ˆï¼šæ·»åŠ æ­£åˆ™åŒ¹é…å’Œç­›é€‰Cç±»åŸºé‡‘ï¼‰
def get_all_fund_codes():
    """ä»å¤©å¤©åŸºé‡‘ç½‘è·å–æ‰€æœ‰åŸºé‡‘çš„ä»£ç åˆ—è¡¨ï¼Œå¹¶ç­›é€‰å‡ºCç±»åŸºé‡‘ã€‚"""
    print("æ­£åœ¨çˆ¬å–å…¨å¸‚åœºåŸºé‡‘ä»£ç åˆ—è¡¨...")
    url = "http://fund.eastmoney.com/allfund.html"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        html = response.text
        soup = BeautifulSoup(html, 'html.parser')  # æ”¹è¿›ï¼šä½¿ç”¨html.parseræ›´ç¨³å®š
        
        fund_list = []
        # ä¿ç•™åŸæœ‰é€‰æ‹©å™¨ï¼Œå¹¶æ·»åŠ æ­£åˆ™åŒ¹é…ä½œä¸ºå¤‡é€‰
        for div in soup.select('#code_content > div > ul > li > div'):
            a_tag = div.find('a')
            if a_tag:
                code_name_text = a_tag.get_text(strip=True)
                # åŸæœ‰æå–æ–¹å¼
                if code_name_text and len(code_name_text) > 8:
                    code = code_name_text[1:7]
                    name = code_name_text[8:]
                    fund_list.append({'code': code, 'name': name})
                # æ–°å¢ï¼šä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä½œä¸ºå¤‡é€‰åŒ¹é…
                else:
                    match = re.match(r'\((\d{6})\)(.+)', code_name_text)
                    if match:
                        code, name = match.groups()
                        fund_list.append({'code': code, 'name': name.strip()})
        
        print(f"å·²è·å– {len(fund_list)} åªåŸºé‡‘çš„ä»£ç ã€‚")
        
        # --- æ–°å¢çš„ç­›é€‰é€»è¾‘ ---
        # ç­›é€‰å‡ºåç§°ä»¥ "C" ç»“å°¾çš„åŸºé‡‘ï¼Œå³åœºå¤–Cç±»
        c_fund_list = [fund for fund in fund_list if fund['name'].endswith('C')]
        print(f"å·²ç­›é€‰å‡º {len(c_fund_list)} åªåœºå¤–Cç±»åŸºé‡‘ã€‚")
        return c_fund_list
        # --- ç­›é€‰é€»è¾‘ç»“æŸ ---

    except requests.exceptions.RequestException as e:
        print(f"çˆ¬å–åŸºé‡‘ä»£ç åˆ—è¡¨å¤±è´¥ï¼š{e}")
        return []

# æ–°å¢ï¼šä¸“é—¨è§£ææŒä»“è¡¨æ ¼çš„å‡½æ•°
def parse_holdings_table(soup, fund_code, year):
    """ä¸“é—¨è§£ææŒä»“è¡¨æ ¼çš„å‡½æ•°"""
    holdings_table = soup.find(id="cctable")
    if not holdings_table:
        return []
    
    holdings = []
    rows = holdings_table.find_all('tr')
    if not rows or len(rows) <= 1:
        return []
    
    # è§£æè¡¨æ ¼æ•°æ®ï¼ˆä¿ç•™åŸæœ‰å­—æ®µï¼Œå¹¶æ·»åŠ æ›´å¤šå­—æ®µä»¥å¢å¼ºï¼‰
    for row in rows[1:]:  # è·³è¿‡è¡¨å¤´
        cols = row.find_all('td')
        if len(cols) >= 5:
            try:
                data = {
                    'fund_code': fund_code,
                    'year': year,
                    'stock_code': cols[1].text.strip() if len(cols) > 1 else '',
                    'stock_name': cols[2].text.strip() if len(cols) > 2 else '',
                    'proportion': cols[3].text.strip() if len(cols) > 3 else '',
                    'shares': cols[4].text.strip() if len(cols) > 4 else '',
                    'market_value': cols[5].text.strip() if len(cols) > 5 else '',
                    # æ–°å¢ï¼šå°è¯•è§£ææ›´å¤šå­—æ®µï¼Œå¦‚æŠ¥å‘Šæ—¥æœŸç­‰
                    'report_date': cols[0].text.strip() if len(cols) > 0 else ''
                }
                holdings.append(data)
            except Exception as e:
                print(f"è§£æè¡Œæ•°æ®å¤±è´¥: {e}")
                continue
    return holdings

# çˆ¬å–æŒ‡å®šåŸºé‡‘æŒä»“æ•°æ®ï¼ˆæ”¹è¿›ç‰ˆï¼šæ·»åŠ é‡è¯•æœºåˆ¶ã€å¤šä¸ªé€‰æ‹©å™¨ã€åˆ†ç¦»è§£æå‡½æ•°ï¼‰
def get_fund_holdings(driver, fund_code, years_to_crawl, max_retries=3):
    """
    çˆ¬å–æŒ‡å®šåŸºé‡‘åœ¨è¿‘Nå¹´å†…çš„æŒä»“æ•°æ®ã€‚
    :param driver: Selenium WebDriverå®ä¾‹
    :param fund_code: åŸºé‡‘ä»£ç 
    :param years_to_crawl: çˆ¬å–çš„å¹´ä»½åˆ—è¡¨
    :param max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    :return: åŒ…å«æŒä»“æ•°æ®çš„DataFrame
    """
    fund_holdings = []
    base_url = f"https://fundf10.eastmoney.com/ccmx_{fund_code}.html"
    
    # æ”¹è¿›ï¼šæ·»åŠ é¡µé¢åŠ è½½é‡è¯•
    for attempt in range(max_retries):
        try:
            print(f"å°è¯•è®¿é—®åŸºé‡‘ {fund_code} (ç¬¬{attempt+1}æ¬¡)...")
            driver.get(base_url)
            # ç”¨æ˜¾å¼ç­‰å¾…ä»£æ›¿å›ºå®šçš„ time.sleep()ï¼Œå¹¶æ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®
            WebDriverWait(driver, 15).until(
                EC.any_of(
                    EC.presence_of_element_located((By.ID, "cctable")),
                    EC.presence_of_element_located((By.CLASS_NAME, "placeholder"))
                )
            )
            
            # æ£€æŸ¥æ˜¯å¦æœ‰æŒä»“æ•°æ®
            page_source_check = driver.page_source
            if "æš‚æ— æ•°æ®" in page_source_check or "æ²¡æœ‰æ‰¾åˆ°" in page_source_check:
                print(f"åŸºé‡‘ {fund_code} æš‚æ— æŒä»“æ•°æ®")
                return pd.DataFrame()
            
            break  # æˆåŠŸåˆ™è·³å‡ºé‡è¯•å¾ªç¯
            
        except TimeoutException:
            if attempt == max_retries - 1:
                print(f"åŸºé‡‘ {fund_code} é¡µé¢åŠ è½½å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡")
                return pd.DataFrame()
            time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
        except Exception as e:
            print(f"è®¿é—®åŸºé‡‘ {fund_code} é¡µé¢å¤±è´¥ï¼š{e}")
            if attempt == max_retries - 1:
                return pd.DataFrame()
    
    # åŸæœ‰å¹´ä»½å¾ªç¯é€»è¾‘ï¼Œèå…¥æ”¹è¿›
    for year in years_to_crawl:
        print(f"æ­£åœ¨çˆ¬å–åŸºé‡‘ {fund_code} çš„ {year} å¹´æŒä»“æ•°æ®...")
        try:
            # å¯»æ‰¾å¹´ä»½é€‰æ‹©æŒ‰é’®å¹¶ç‚¹å‡»ï¼ˆæ”¹è¿›ï¼šå¤šä¸ªXPathé€‰æ‹©å™¨ï¼‰
            year_selectors = [
                f"//*[@id='pagebar']/div/label[@value='{year}']",
                f"//label[@value='{year}']",
                f"//input[@value='{year}']",
                f"//option[@value='{year}']"
            ]
            
            year_button = None
            for selector in year_selectors:
                try:
                    # ä½¿ç”¨æ˜¾å¼ç­‰å¾…ï¼Œç­‰å¾…æŒ‰é’®å¯ç‚¹å‡»
                    year_button = WebDriverWait(driver, 5).until(
                        EC.element_to_be_clickable((By.XPATH, selector))
                    )
                    break
                except TimeoutException:
                    continue
            
            if not year_button:
                print(f"æœªæ‰¾åˆ°åŸºé‡‘ {fund_code} åœ¨ {year} å¹´çš„æŒä»“æŒ‰é’®ï¼Œè·³è¿‡ã€‚")
                continue
            
            # æ”¹è¿›ï¼šæ»šåŠ¨åˆ°å…ƒç´ å¹¶ç‚¹å‡»
            driver.execute_script("arguments[0].scrollIntoView();", year_button)
            time.sleep(0.5)
            year_button.click()
            
            # ç­‰å¾…è¡¨æ ¼å†…å®¹æ›´æ–°
            WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.ID, "cctable"))
            )
            
            # è·å–é¡µé¢HTMLå†…å®¹
            page_source = driver.page_source
            soup = BeautifulSoup(page_source, 'lxml')
            
            # ä½¿ç”¨æ–°è§£æå‡½æ•°
            holdings = parse_holdings_table(soup, fund_code, year)
            fund_holdings.extend(holdings)
                    
        # å¦‚æœæ‰¾ä¸åˆ°æŒ‰é’®æˆ–è¶…æ—¶ï¼Œåˆ™æ•è·å¼‚å¸¸å¹¶è·³è¿‡
        except TimeoutException:
            print(f"åŸºé‡‘ {fund_code} åœ¨ {year} å¹´çš„æŒä»“æŒ‰é’®æˆ–è¡¨æ ¼ä¸å­˜åœ¨ï¼Œè·³è¿‡ã€‚")
            continue
        except Exception as e:
            print(f"çˆ¬å–åŸºé‡‘ {fund_code} çš„ {year} å¹´æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
            continue
            
    return pd.DataFrame(fund_holdings)


def main():
    """ä¸»å‡½æ•°ï¼Œæ‰§è¡Œçˆ¬å–ä»»åŠ¡ï¼ˆæ”¹è¿›ç‰ˆï¼šæ·»åŠ é…ç½®ã€é™åˆ¶ã€ç»Ÿè®¡å’Œæ—¥å¿—ï¼‰ã€‚"""
    # å®šä¹‰éœ€è¦çˆ¬å–çš„å¹´ä»½èŒƒå›´ï¼Œæ ¹æ®æ–‡ç« ï¼Œå¯çˆ¬å–æœ€æ–°åŠå†å²æŒä»“
    current_year = time.localtime().tm_year
    years_to_crawl = [str(current_year), str(current_year - 1), str(current_year - 2)]
    
    # æ”¹è¿›ï¼šé…ç½®å‚æ•°
    max_funds = 50  # æ–°å¢ï¼šé™åˆ¶æœ€å¤§åŸºé‡‘æ•°é‡
    request_delay = 1  # æ–°å¢ï¼šè¯·æ±‚å»¶æ—¶
    
    print("=== å¤©å¤©åŸºé‡‘æŒä»“æ•°æ®çˆ¬å–å™¨ ===")
    print(f"ç›®æ ‡å¹´ä»½: {', '.join(years_to_crawl)}")
    print(f"æœ€å¤§åŸºé‡‘æ•°é‡: {max_funds}")
    
    # è·å– C ç±»åŸºé‡‘çš„ä»£ç åˆ—è¡¨
    all_fund_data = get_all_fund_codes()
    if not all_fund_data:
        print("æ— æ³•è·å–åŸºé‡‘ä»£ç åˆ—è¡¨ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    # é™åˆ¶çˆ¬å–çš„åŸºé‡‘æ•°é‡ï¼Œåªå¤„ç†åˆ—è¡¨ä¸­çš„å‰ max_funds ä¸ª
    if len(all_fund_data) > max_funds:
        all_fund_data = all_fund_data[:max_funds]
        print(f"æ³¨æ„ï¼šåŸºé‡‘æ•°é‡å·²é™åˆ¶ä¸º {max_funds} åªã€‚")
    
    print(f"ğŸ“Š å‡†å¤‡çˆ¬å– {len(all_fund_data)} åªåŸºé‡‘")
    
    # è®¾ç½®ä¸€ä¸ªæ–‡ä»¶è·¯å¾„æ¥å­˜å‚¨ç»“æœ
    output_dir = "fund_data"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    timestamp = time.strftime('%Y%m%d_%H%M%S')
    # ä¿®æ”¹è¾“å‡ºæ–‡ä»¶åï¼Œä»¥åŒºåˆ† C ç±»åŸºé‡‘æ•°æ®
    output_filename = os.path.join(output_dir, f"fund_holdings_C_{timestamp}.csv")
    
    driver = setup_driver()
    all_holdings_df = pd.DataFrame()
    successful_funds = 0
    
    try:
        for i, fund in enumerate(all_fund_data, 1):
            fund_code = fund['code']
            fund_name = fund['name']
            
            print(f"\n[{i}/{len(all_fund_data)}] æ­£åœ¨å¤„ç†: {fund_name} ({fund_code})")
            
            holdings_df = get_fund_holdings(driver, fund_code, years_to_crawl)
            if not holdings_df.empty:
                all_holdings_df = pd.concat([all_holdings_df, holdings_df], ignore_index=True)
                successful_funds += 1
                print(f"âœ… æˆåŠŸè·å– {len(holdings_df)} æ¡æŒä»“è®°å½•")
            else:
                print("âŒ æœªè·å–åˆ°æ•°æ®")
            
            # æ”¹è¿›ï¼šé€‚å½“å»¶æ—¶ï¼Œé¿å…è¯·æ±‚è¿‡å¿«
            time.sleep(request_delay)
            
    finally:
        driver.quit()
    
    if not all_holdings_df.empty:
        all_holdings_df.to_csv(output_filename, index=False, encoding='utf-8-sig')
        print(f"\nğŸ‰ æ•°æ®çˆ¬å–å®Œæˆ!")
        print(f"ğŸ“ å·²ä¿å­˜åˆ°æ–‡ä»¶ï¼š{output_filename}")
        print(f"ğŸ“ˆ æ€»è®°å½•æ•°: {len(all_holdings_df)}")
        print(f"âœ… æˆåŠŸåŸºé‡‘: {successful_funds}/{len(all_fund_data)}")
        
        # æ–°å¢ï¼šæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        print("\n=== æ•°æ®æ¦‚è§ˆ ===")
        if 'year' in all_holdings_df.columns:
            print(all_holdings_df.groupby('year').size())
    else:
        print("æ²¡æœ‰çˆ¬å–åˆ°ä»»ä½•æ•°æ®ã€‚")

if __name__ == '__main__':
    main()
