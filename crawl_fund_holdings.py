# -*- coding: utf-8 -*-
"""
ä¸€ä¸ªç”¨äºçˆ¬å–å¤©å¤©åŸºé‡‘ç½‘å…¨å¸‚åœºåŸºé‡‘æŒä»“æ•°æ®çš„Pythonè„šæœ¬
ä¿®å¤ç‰ˆï¼šé’ˆå¯¹LoadStockPoså¼‚æ­¥åŠ è½½æœºåˆ¶çš„ä¼˜åŒ–
"""
import os
import time
import requests
import pandas as pd
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException, StaleElementReferenceException
from bs4 import BeautifulSoup
import logging
import random
from typing import List, Dict, Optional
import json

# --- é…ç½®æ—¥å¿—ç³»ç»Ÿ ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- è§£æMarkdownæ–‡ä»¶ï¼Œæå–åŸºé‡‘ä»£ç  ---
def parse_markdown_file(file_path: str) -> List[Dict[str, str]]:
    """è§£æMarkdownæ–‡ä»¶ï¼Œæå–"å¼±ä¹°å…¥"æˆ–"å¼ºä¹°å…¥"çš„åŸºé‡‘ä»£ç ã€‚"""
    if not os.path.exists(file_path):
        logging.error(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶æœªæ‰¾åˆ° -> {file_path}")
        return []
    
    fund_codes = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        lines = content.strip().split('\n')
        for line in lines:
            if '| è¡ŒåŠ¨ä¿¡å·' in line:
                continue
            
            parts = [p.strip() for p in line.split('|')]
            if len(parts) >= 9:
                fund_code = parts[1]
                action_signal = parts[8].lower()

                if "å¼±ä¹°å…¥" in action_signal or "å¼ºä¹°å…¥" in action_signal:
                    fund_codes.append({'code': fund_code, 'name': 'N/A'})
        
        logging.info(f"âœ… ä»æŠ¥å‘Šä¸­æˆåŠŸæå–äº† {len(fund_codes)} ä¸ªç›®æ ‡åŸºé‡‘ä»£ç ã€‚")
        return fund_codes

    except Exception as e:
        logging.error(f"âŒ è§£æMarkdownæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
        return []

# --- è‚¡ç¥¨ä¿¡æ¯ç¼“å­˜ ---
stock_info_cache = {}

# --- User-Agentæ±  ---
user_agent_pool = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0",
]

# --- è·å–è‚¡ç¥¨è¡Œä¸šå’Œä¸»é¢˜ä¿¡æ¯ ---
def get_stock_info(stock_code: str) -> Dict[str, str]:
    """æ ¹æ®è‚¡ç¥¨ä»£ç çˆ¬å–ä¸œæ–¹è´¢å¯Œç½‘ï¼Œè·å–æ‰€å±è¡Œä¸šå’Œæ¦‚å¿µä¸»é¢˜ã€‚"""
    if not stock_code or stock_code == '':
        return {'æ‰€å±è¡Œä¸š': 'æœªçŸ¥', 'æ¦‚å¿µä¸»é¢˜': 'æœªçŸ¥'}
    
    if stock_code in stock_info_cache:
        return stock_info_cache[stock_code]
    
    info = {'æ‰€å±è¡Œä¸š': 'æœªçŸ¥', 'æ¦‚å¿µä¸»é¢˜': 'æœªçŸ¥'}
    
    if not re.match(r'^\d{6}$', stock_code):
        stock_info_cache[stock_code] = info
        return info
    
    url = f"https://quote.eastmoney.com/sh{stock_code[:3]}.html" if stock_code.startswith(('60', '68')) else f"https://quote.eastmoney.com/sz{stock_code[3:]}.html"
    headers = {"User-Agent": random.choice(user_agent_pool)}
    
    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'lxml')

        # è·å–è¡Œä¸šä¿¡æ¯
        industry_elem = soup.find('span', string=re.compile(r'è¡Œä¸š'))
        if industry_elem:
            industry_text = industry_elem.find_next_sibling('a')
            if industry_text:
                info['æ‰€å±è¡Œä¸š'] = industry_text.text.strip()

        # è·å–æ¦‚å¿µä¿¡æ¯
        concept_section = soup.find('div', string=re.compile(r'æ¦‚å¿µæ¿å—'))
        if concept_section:
            concepts = concept_section.find_next_sibling('div').find_all('a')
            info['æ¦‚å¿µä¸»é¢˜'] = ', '.join([c.text.strip() for c in concepts[:5]])

        stock_info_cache[stock_code] = info

    except Exception as e:
        logging.warning(f"âŒ è‚¡ç¥¨ {stock_code} ä¿¡æ¯è·å–å¤±è´¥: {e}")
    
    time.sleep(random.uniform(0.5, 1.0))
    return info

# --- é…ç½®Seleniumï¼ˆä¿®å¤ç‰ˆï¼‰ ---
def setup_driver() -> Optional[webdriver.Chrome]:
    """é…ç½®å¹¶è¿”å›ä¸€ä¸ªæ— å¤´æ¨¡å¼çš„Chromeæµè§ˆå™¨é©±åŠ¨ã€‚"""
    logging.info("--- æ­£åœ¨å¯åŠ¨ ChromeDriver ---")
    try:
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--window-size=1920,1080')
        chrome_options.add_argument('--disable-images')
        chrome_options.add_argument('--disable-plugins-discovery')
        chrome_options.add_argument('--disable-extensions')
        # å…³é”®ï¼šå¯ç”¨æ€§èƒ½æ—¥å¿—ä»¥ç›‘æ§JavaScriptæ‰§è¡Œ
        chrome_options.set_capability('goog:loggingPrefs', {'performance': 'ALL'})
        
        chromedriver_path = os.getenv('CHROMEDRIVER_PATH', '/usr/bin/chromedriver')
        service = Service(chromedriver_path)
        
        driver = webdriver.Chrome(service=service, options=chrome_options)
        driver.set_page_load_timeout(60)
        driver.implicitly_wait(15)
        
        logging.info("ğŸ‰ ChromeDriver å¯åŠ¨æˆåŠŸï¼")
        return driver
    except WebDriverException as e:
        logging.error(f"âŒ ChromeDriver å¯åŠ¨å¤±è´¥ï¼š{e}")
        return None

# --- æ–°å¢ï¼šç­‰å¾…LoadStockPoså®Œæˆ ---
def wait_for_loadstockpos_complete(driver: webdriver.Chrome, fund_code: str, timeout: int = 60) -> bool:
    """
    ç­‰å¾…LoadStockPoså‡½æ•°å®Œæˆæ•°æ®åŠ è½½
    é€šè¿‡ç›‘æ§é¡µé¢å˜åŒ–å’Œç½‘ç»œè¯·æ±‚æ¥åˆ¤æ–­
    """
    logging.info(f"â³ ç­‰å¾… LoadStockPos å®ŒæˆåŠ è½½ (åŸºé‡‘ {fund_code})...")
    
    start_time = time.time()
    
    while time.time() - start_time < timeout:
        try:
            # 1. æ£€æŸ¥å¹´ä»½é€‰æ‹©å™¨æ˜¯å¦å·²å¡«å……
            year_select = WebDriverWait(driver, 5).until(
                EC.presence_of_element_located((By.ID, "jjcc"))
            )
            
            # æ£€æŸ¥selectæ˜¯å¦æœ‰option
            options = driver.find_elements(By.CSS_SELECTOR, "#jjcc option")
            if len(options) > 1:  # è‡³å°‘æœ‰ä¸€ä¸ªå¹´ä»½é€‰é¡¹
                logging.info(f"âœ… å¹´ä»½é€‰æ‹©å™¨å·²å¡«å……ï¼Œæ‰¾åˆ° {len(options)-1} ä¸ªå¹´ä»½é€‰é¡¹")
                break
                
        except TimeoutException:
            pass
        
        # 2. æ£€æŸ¥åŠ è½½åŠ¨ç”»æ˜¯å¦æ¶ˆå¤±
        try:
            loading_img = driver.find_element(By.XPATH, "//img[@src*='loading2.gif']")
            if not loading_img.is_displayed():
                logging.info("âœ… åŠ è½½åŠ¨ç”»å·²æ¶ˆå¤±")
                break
        except:
            logging.info("âœ… æ— åŠ è½½åŠ¨ç”»")
            break
        
        # 3. æ£€æŸ¥cctableæ˜¯å¦æœ‰å®é™…æ•°æ®
        try:
            cctable = driver.find_element(By.ID, "cctable")
            if "æ•°æ®åŠ è½½ä¸­" not in cctable.text:
                logging.info("âœ… æŒä»“è¡¨æ ¼å·²åŠ è½½æ•°æ®")
                break
        except:
            pass
        
        # 4. ç›‘æ§ç½‘ç»œè¯·æ±‚ï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®è¯·æ±‚å®Œæˆï¼‰
        logs = driver.get_log('performance')
        has_data_request = any(
            "fund.eastmoney.com" in log['message']['message']['params'].get('request', {}).get('url', '') 
            or "eastmoney.com" in log['message']['message']['params'].get('request', {}).get('url', '')
            for log in logs[-10:]  # æ£€æŸ¥æœ€è¿‘10æ¡æ—¥å¿—
        )
        
        if has_data_request:
            logging.info("ğŸ”„ æ£€æµ‹åˆ°æ•°æ®è¯·æ±‚ï¼Œç­‰å¾…å“åº”...")
        
        time.sleep(2)
    
    total_time = time.time() - start_time
    if total_time >= timeout:
        logging.warning(f"âš ï¸  LoadStockPos åŠ è½½è¶…æ—¶ ({total_time:.1f}s)")
        return False
    
    logging.info(f"âœ… LoadStockPos åŠ è½½å®Œæˆï¼Œè€—æ—¶ {total_time:.1f}s")
    return True

# --- æ–°å¢ï¼šæ™ºèƒ½å¹´ä»½é€‰æ‹© ---
def select_year_intelligently(driver: webdriver.Chrome, target_year: str) -> bool:
    """
    æ™ºèƒ½é€‰æ‹©å¹´ä»½ï¼šå…ˆè·å–æ‰€æœ‰å¯ç”¨å¹´ä»½ï¼Œç„¶åé€‰æ‹©ç›®æ ‡å¹´ä»½
    """
    try:
        # ç­‰å¾…å¹´ä»½é€‰é¡¹åŠ è½½
        WebDriverWait(driver, 10).until(
            EC.presence_of_element_located((By.ID, "jjcc"))
        )
        
        # è·å–æ‰€æœ‰å¹´ä»½é€‰é¡¹
        options = driver.find_elements(By.CSS_SELECTOR, "#jjcc option")
        available_years = []
        
        for option in options[1:]:  # è·³è¿‡é»˜è®¤é€‰é¡¹
            year_text = option.text.strip()
            if re.match(r'\d{4}', year_text):
                available_years.append(year_text)
        
        logging.info(f"ğŸ“‹ å¯ç”¨å¹´ä»½: {available_years}")
        
        if not available_years:
            logging.warning("âŒ æœªæ‰¾åˆ°ä»»ä½•å¹´ä»½é€‰é¡¹")
            return False
        
        # æ£€æŸ¥ç›®æ ‡å¹´ä»½æ˜¯å¦å¯ç”¨
        if target_year in available_years:
            logging.info(f"ğŸ¯ æ‰¾åˆ°ç›®æ ‡å¹´ä»½ {target_year}")
        else:
            # é€‰æ‹©æœ€æ¥è¿‘çš„å¹´ä»½
            target_year = available_years[0]  # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ªï¼ˆé€šå¸¸æ˜¯æœ€æ–°ï¼‰
            logging.warning(f"âš ï¸ ç›®æ ‡å¹´ä»½ {target_year} ä¸å¯ç”¨ï¼Œä½¿ç”¨ {target_year}")
        
        # æ‰§è¡Œé€‰æ‹©
        year_option = driver.find_element(By.XPATH, f"//select[@id='jjcc']/option[contains(text(), '{target_year}')]")
        
        # æ»šåŠ¨åˆ°é€‰æ‹©å™¨ä½ç½®
        driver.execute_script("arguments[0].scrollIntoView({block: 'center'});", year_option)
        time.sleep(1)
        
        # JavaScriptç‚¹å‡»é¿å…Seleniumçš„ç‚¹å‡»é—®é¢˜
        driver.execute_script("arguments[0].parentNode.selectedIndex = arguments[1];", 
                             driver.find_element(By.ID, "jjcc"), available_years.index(target_year))
        
        # è§¦å‘changeäº‹ä»¶
        driver.execute_script("arguments[0].dispatchEvent(new Event('change', {bubbles: true}));", 
                             driver.find_element(By.ID, "jjcc"))
        
        logging.info(f"âœ… å·²é€‰æ‹©å¹´ä»½: {target_year}")
        
        # ç­‰å¾…æ•°æ®é‡æ–°åŠ è½½
        time.sleep(5)
        return True
        
    except Exception as e:
        logging.error(f"âŒ å¹´ä»½é€‰æ‹©å¤±è´¥: {e}")
        return False

# --- è§£ææŒä»“è¡¨æ ¼ï¼ˆä¼˜åŒ–ç‰ˆï¼‰ ---
def parse_holdings_table(soup: BeautifulSoup, fund_code: str, year: str) -> List[Dict]:
    """ä¸“é—¨è§£ææŒä»“è¡¨æ ¼çš„å‡½æ•°"""
    holdings_table = soup.find(id="cctable")
    if not holdings_table:
        logging.warning(f"æœªæ‰¾åˆ°æŒä»“è¡¨æ ¼ #cctable")
        return []
    
    # æ£€æŸ¥åŠ è½½çŠ¶æ€
    loading_div = holdings_table.find('div', string=re.compile(r'æ•°æ®åŠ è½½ä¸­'))
    if loading_div:
        logging.warning(f"æŒä»“è¡¨æ ¼ä»åœ¨åŠ è½½ï¼Œè·³è¿‡ {fund_code} {year} å¹´æ•°æ®")
        return []
    
    holdings = []
    # æŸ¥æ‰¾è¡¨æ ¼è¡Œï¼ˆæ”¯æŒå¤šç§ç»“æ„ï¼‰
    rows = holdings_table.find_all('tr') or []
    
    if not rows or len(rows) <= 1:
        # å°è¯•divè¡¨æ ¼ç»“æ„
        div_rows = holdings_table.find_all('div', class_=re.compile(r'row|item'))
        if div_rows:
            logging.info(f"ä½¿ç”¨divè¡¨æ ¼ç»“æ„ï¼Œæ‰¾åˆ° {len(div_rows)} è¡Œ")
            for div_row in div_rows:
                cols = div_row.find_all(['span', 'div', 'td'])
                if len(cols) >= 4:  # è‡³å°‘4åˆ—
                    try:
                        stock_code = re.search(r'(\d{6})', cols[1].text).group(1) if re.search(r'(\d{6})', cols[1].text) else ''
                        stock_name = cols[2].text.strip() if len(cols) > 2 else ''
                        
                        stock_info = get_stock_info(stock_code)
                        
                        data = {
                            'åŸºé‡‘ä»£ç ': fund_code,
                            'å¹´ä»½': year,
                            'è‚¡ç¥¨ä»£ç ': stock_code,
                            'è‚¡ç¥¨åç§°': stock_name,
                            'æ‰€å±è¡Œä¸š': stock_info['æ‰€å±è¡Œä¸š'],
                            'æ¦‚å¿µä¸»é¢˜': stock_info['æ¦‚å¿µä¸»é¢˜'],
                            'æŒä»“å æ¯”': cols[3].text.strip() if len(cols) > 3 else '',
                            'æŒè‚¡æ•°': cols[4].text.strip() if len(cols) > 4 else '',
                            'å¸‚å€¼': cols[5].text.strip() if len(cols) > 5 else '',
                            'æŠ¥å‘Šæ—¥æœŸ': cols[0].text.strip() if len(cols) > 0 else ''
                        }
                        holdings.append(data)
                    except Exception as e:
                        logging.debug(f"è§£ædivè¡Œå¤±è´¥: {e}")
                        continue
        else:
            logging.warning(f"æœªæ‰¾åˆ°æœ‰æ•ˆçš„è¡¨æ ¼ç»“æ„")
            return []
    else:
        # æ ‡å‡†tableç»“æ„
        for row in rows[1:]:  # è·³è¿‡è¡¨å¤´
            cols = row.find_all('td')
            if len(cols) >= 4:
                try:
                    stock_code = re.search(r'(\d{6})', cols[1].text).group(1) if re.search(r'(\d{6})', cols[1].text) else ''
                    stock_name = cols[2].text.strip() if len(cols) > 2 else ''
                    
                    stock_info = get_stock_info(stock_code)
                    
                    data = {
                        'åŸºé‡‘ä»£ç ': fund_code,
                        'å¹´ä»½': year,
                        'è‚¡ç¥¨ä»£ç ': stock_code,
                        'è‚¡ç¥¨åç§°': stock_name,
                        'æ‰€å±è¡Œä¸š': stock_info['æ‰€å±è¡Œä¸š'],
                        'æ¦‚å¿µä¸»é¢˜': stock_info['æ¦‚å¿µä¸»é¢˜'],
                        'æŒä»“å æ¯”': cols[3].text.strip() if len(cols) > 3 else '',
                        'æŒè‚¡æ•°': cols[4].text.strip() if len(cols) > 4 else '',
                        'å¸‚å€¼': cols[5].text.strip() if len(cols) > 5 else '',
                        'æŠ¥å‘Šæ—¥æœŸ': cols[0].text.strip() if len(cols) > 0 else ''
                    }
                    holdings.append(data)
                except Exception as e:
                    logging.debug(f"è§£æè¡Œæ•°æ®å¤±è´¥: {e}")
                    continue
    
    logging.info(f"âœ… è§£æå®Œæˆ: {len(holdings)} æ¡ {year} å¹´æŒä»“è®°å½•")
    return holdings

# --- ä¿®å¤ç‰ˆï¼šçˆ¬å–æŒ‡å®šåŸºé‡‘æŒä»“æ•°æ® ---
def get_fund_holdings(driver: webdriver.Chrome, fund_code: str, years_to_crawl: List[str], max_retries: int = 3) -> pd.DataFrame:
    """
    çˆ¬å–æŒ‡å®šåŸºé‡‘åœ¨è¿‘Nå¹´å†…çš„æŒä»“æ•°æ®ã€‚
    ä¿®å¤ç‰ˆï¼šæ­£ç¡®å¤„ç†LoadStockPoså¼‚æ­¥åŠ è½½
    """
    if driver is None:
        logging.error("WebDriver å®ä¾‹ä¸å­˜åœ¨ï¼Œè·³è¿‡çˆ¬å–ã€‚")
        return pd.DataFrame()

    fund_holdings = []
    base_url = f"https://fundf10.eastmoney.com/ccmx_{fund_code}.html"

    logging.info(f"ğŸŒ è®¿é—®åŸºé‡‘ {fund_code} é¡µé¢: {base_url}")
    
    # é¡µé¢åŠ è½½é‡è¯•
    for attempt in range(max_retries):
        try:
            logging.info(f"ğŸš€ å°è¯•è®¿é—®é¡µé¢ (ç¬¬{attempt+1}æ¬¡)...")
            driver.get(base_url)
            
            # ç­‰å¾…åŸºæœ¬é¡µé¢ç»“æ„
            WebDriverWait(driver, 30).until(
                EC.presence_of_element_located((By.ID, "cctable"))
            )
            
            # å…³é”®ï¼šç­‰å¾…LoadStockPoså®Œæˆ
            if not wait_for_loadstockpos_complete(driver, fund_code):
                if attempt == max_retries - 1:
                    logging.error(f"âŒ LoadStockPos åŠ è½½å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡")
                    return pd.DataFrame()
                time.sleep(5)
                continue
            
            # éªŒè¯é¡µé¢æ˜¯å¦æ­£ç¡®åŠ è½½
            page_source = driver.page_source
            if "017484" not in page_source and fund_code not in page_source:
                logging.warning(f"é¡µé¢å¯èƒ½åŠ è½½é”™è¯¯ï¼Œæœªæ‰¾åˆ°åŸºé‡‘ {fund_code} æ ‡è¯†")
                if attempt == max_retries - 1:
                    return pd.DataFrame()
                time.sleep(3)
                continue
            
            logging.info("âœ… é¡µé¢å’Œæ•°æ®åŠ è½½æˆåŠŸï¼")
            break
            
        except TimeoutException:
            logging.warning(f"é¡µé¢åŠ è½½è¶…æ—¶ (ç¬¬{attempt+1}/{max_retries}æ¬¡)")
            if attempt == max_retries - 1:
                return pd.DataFrame()
            time.sleep(2 ** attempt)
        except Exception as e:
            logging.error(f"è®¿é—®é¡µé¢æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
            if attempt == max_retries - 1:
                return pd.DataFrame()
            time.sleep(2 ** attempt)

    # å¹´ä»½æ•°æ®çˆ¬å–
    for year in years_to_crawl:
        try:
            logging.info(f"ğŸ“… æ­£åœ¨å¤„ç† {year} å¹´æ•°æ®...")
            
            # æ™ºèƒ½é€‰æ‹©å¹´ä»½
            if not select_year_intelligently(driver, year):
                logging.warning(f"âš ï¸ æ— æ³•é€‰æ‹© {year} å¹´ï¼Œå°è¯•è§£æå½“å‰æ˜¾ç¤ºæ•°æ®")
                # è§£æå½“å‰é¡µé¢æ•°æ®
                page_source = driver.page_source
                soup = BeautifulSoup(page_source, 'lxml')
                current_holdings = parse_holdings_table(soup, fund_code, year)
                if current_holdings:
                    fund_holdings.extend(current_holdings)
                    logging.info(f"âœ… ä»å½“å‰é¡µé¢è·å– {len(current_holdings)} æ¡è®°å½•")
                continue
            
            # ç­‰å¾…å¹´ä»½åˆ‡æ¢åçš„æ•°æ®åŠ è½½
            time.sleep(5)
            
            # å†æ¬¡ç­‰å¾…LoadStockPoså®Œæˆï¼ˆå¹´ä»½åˆ‡æ¢è§¦å‘ï¼‰
            wait_for_loadstockpos_complete(driver, fund_code, timeout=30)
            
            # è§£ææ•°æ®
            page_source = driver.page_source
            soup = BeautifulSoup(page_source, 'lxml')
            
            holdings = parse_holdings_table(soup, fund_code, year)
            fund_holdings.extend(holdings)
            
            if holdings:
                logging.info(f"âœ… æˆåŠŸè·å– {len(holdings)} æ¡ {year} å¹´æŒä»“è®°å½•")
            else:
                logging.warning(f"âŒ {year} å¹´æ— æŒä»“æ•°æ®")
            
            time.sleep(random.uniform(1, 2))
            
        except Exception as e:
            logging.error(f"å¤„ç† {year} å¹´æ•°æ®æ—¶å‡ºé”™ï¼š{e}")
            continue
    
    return pd.DataFrame(fund_holdings)

# --- ä¸»å‡½æ•° ---
def main():
    """ä¸»å‡½æ•°ï¼Œæ‰§è¡Œçˆ¬å–ä»»åŠ¡ã€‚"""
    current_year = time.localtime().tm_year
    years_to_crawl = [str(current_year), str(current_year - 1), str(current_year - 2)]
    
    request_delay = random.uniform(2, 4)

    logging.info("=== å¤©å¤©åŸºé‡‘æŒä»“æ•°æ®çˆ¬å–å™¨ï¼ˆä¿®å¤ç‰ˆï¼‰ ===")
    logging.info(f"ğŸ¯ ç›®æ ‡å¹´ä»½: {', '.join(years_to_crawl)}")
    logging.info(f"â±ï¸  å»¶æ—¶è®¾ç½®: {request_delay:.1f}ç§’")
    
    report_file = 'market_monitor_report.md'
    fund_list_to_crawl = parse_markdown_file(report_file)
    if not fund_list_to_crawl:
        logging.error(f"âŒ æ— æ³•ä» '{report_file}' è·å–åŸºé‡‘åˆ—è¡¨ï¼Œç¨‹åºé€€å‡º")
        return

    logging.info(f"ğŸ“Š å‡†å¤‡çˆ¬å– {len(fund_list_to_crawl)} åªåŸºé‡‘")
    
    output_dir = "fund_data"
    os.makedirs(output_dir, exist_ok=True)
    
    timestamp = time.strftime('%Y%m%d_%H%M%S')
    output_filename = os.path.join(output_dir, f"fund_holdings_{timestamp}.csv")
    
    driver = setup_driver()
    if driver is None:
        logging.error("âŒ æµè§ˆå™¨å¯åŠ¨å¤±è´¥")
        return
    
    all_holdings_df = pd.DataFrame()
    successful_funds = 0
    
    try:
        for i, fund in enumerate(fund_list_to_crawl, 1):
            fund_code = fund['code']
            fund_name = fund['name']
            
            logging.info(f"\n{'='*60}")
            logging.info(f"ğŸ”„ [{i}/{len(fund_list_to_crawl)}] å¤„ç†: {fund_name} ({fund_code})")
            logging.info(f"{'='*60}")
            
            holdings_df = get_fund_holdings(driver, fund_code, years_to_crawl)
            
            if not holdings_df.empty:
                all_holdings_df = pd.concat([all_holdings_df, holdings_df], ignore_index=True)
                successful_funds += 1
                logging.info(f"âœ… æˆåŠŸè·å– {len(holdings_df)} æ¡è®°å½•")
                
                # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
                logging.info(f"ğŸ“‹ æ•°æ®é¢„è§ˆ:")
                for _, row in holdings_df.head(2).iterrows():
                    logging.info(f"   {row['è‚¡ç¥¨ä»£ç ']} - {row['è‚¡ç¥¨åç§°'][:20]}... ({row['æŒä»“å æ¯”']})")
            else:
                logging.info("âŒ æœªè·å–åˆ°æ•°æ®")
            
            # åŸºé‡‘é—´å»¶æ—¶
            logging.info(f"ğŸ’¤ ç­‰å¾… {request_delay:.1f} ç§’...")
            time.sleep(request_delay)
            
    finally:
        logging.info("ğŸ”š çˆ¬å–ä»»åŠ¡ç»“æŸï¼Œå…³é—­æµè§ˆå™¨")
        driver.quit()
    
    # ç»“æœä¿å­˜
    if not all_holdings_df.empty:
        logging.info("\n" + "ğŸ‰" * 20)
        logging.info("ğŸ“Š çˆ¬å–ç»Ÿè®¡:")
        logging.info(f"   æ€»è®°å½•æ•°: {len(all_holdings_df):,}")
        logging.info(f"   æˆåŠŸåŸºé‡‘: {successful_funds}/{len(fund_list_to_crawl)}")
        logging.info(f"   å”¯ä¸€è‚¡ç¥¨: {all_holdings_df['è‚¡ç¥¨ä»£ç '].nunique()}")
        
        try:
            all_holdings_df.to_csv(output_filename, index=False, encoding='utf-8-sig')
            file_size = os.path.getsize(output_filename) / 1024
            logging.info(f"ğŸ’¾ ä¿å­˜æˆåŠŸ: {output_filename} ({file_size:.1f} KB)")
        except Exception as e:
            logging.error(f"âŒ æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
    else:
        logging.warning("âŒ æœªè·å–åˆ°ä»»ä½•æ•°æ®ï¼Œåˆ›å»ºç©ºæ–‡ä»¶")
        empty_df = pd.DataFrame(columns=['åŸºé‡‘ä»£ç ', 'å¹´ä»½', 'è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨åç§°', 'æ‰€å±è¡Œä¸š', 
                                        'æ¦‚å¿µä¸»é¢˜', 'æŒä»“å æ¯”', 'æŒè‚¡æ•°', 'å¸‚å€¼', 'æŠ¥å‘Šæ—¥æœŸ'])
        empty_df.to_csv(output_filename, index=False, encoding='utf-8-sig')

if __name__ == '__main__':
    main()
