# -*- coding: utf-8 -*-
"""
ä¸€ä¸ªç”¨äºçˆ¬å–å¤©å¤©åŸºé‡‘ç½‘å…¨å¸‚åœºåŸºé‡‘æŒä»“æ•°æ®çš„Pythonè„šæœ¬
è¯¥ç‰ˆæœ¬å¢åŠ äº†ä»æœ¬åœ°Markdownæ–‡ä»¶è§£ææŒ‡å®šåŸºé‡‘ä»£ç çš„åŠŸèƒ½
"""
import os
import time
import requests
import pandas as pd
import re
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
from bs4 import BeautifulSoup
import logging

# --- é…ç½®æ—¥å¿—ç³»ç»Ÿ ---
# é…ç½®æ—¥å¿—è¾“å‡ºåˆ°æ§åˆ¶å°ï¼Œå¹¶è®¾ç½®çº§åˆ«ä¸º INFO
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# --- æ–°å¢ï¼šè§£æMarkdownæ–‡ä»¶ï¼Œæå–åŸºé‡‘ä»£ç  ---
def parse_markdown_file(file_path):
    """
    è§£æMarkdownæ–‡ä»¶ï¼Œæå–â€œå¼±ä¹°å…¥â€æˆ–â€œå¼ºä¹°å…¥â€çš„åŸºé‡‘ä»£ç ã€‚
    """
    if not os.path.exists(file_path):
        logging.error(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶æœªæ‰¾åˆ° -> {file_path}")
        return []

    fund_codes = []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…è¡¨æ ¼è¡Œ
        lines = content.strip().split('\n')
        for line in lines:
            # åŒ¹é…åŒ…å« '| è¡ŒåŠ¨ä¿¡å· |' çš„è¡¨å¤´ï¼Œè·³è¿‡
            if '| è¡ŒåŠ¨ä¿¡å·' in line:
                continue
            
            # ä½¿ç”¨æ›´å®½æ³›çš„åŒ¹é…æ¥å¤„ç†è¡¨æ ¼å†…å®¹
            parts = [p.strip() for p in line.split('|')]
            if len(parts) >= 9:
                fund_code = parts[1]
                action_signal = parts[8].lower() # è½¬æ¢ä¸ºå°å†™ï¼Œæ–¹ä¾¿åŒ¹é…

                if "å¼±ä¹°å…¥" in action_signal or "å¼ºä¹°å…¥" in action_signal:
                    fund_codes.append({'code': fund_code, 'name': 'N/A'}) # åç§°è®¾ä¸ºN/Aï¼Œå› ä¸ºæŠ¥å‘Šä¸­æœªæä¾›
        
        logging.info(f"âœ… ä»æŠ¥å‘Šä¸­æˆåŠŸæå–äº† {len(fund_codes)} ä¸ªç›®æ ‡åŸºé‡‘ä»£ç ã€‚")
        return fund_codes

    except Exception as e:
        logging.error(f"âŒ è§£æMarkdownæ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
        return []

# --- é…ç½®Selenium ---
def setup_driver():
    """é…ç½®å¹¶è¿”å›ä¸€ä¸ªæ— å¤´æ¨¡å¼çš„Chromeæµè§ˆå™¨é©±åŠ¨ã€‚"""
    logging.info("--- æ­£åœ¨å¯åŠ¨ ChromeDriver ---")
    try:
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument('--disable-dev-shm-usage')
        
        chromedriver_path = os.getenv('CHROMEDRIVER_PATH', '/usr/lib/chromium-browser/chromedriver')
        service = Service(chromedriver_path)
        
        driver = webdriver.Chrome(service=service, options=chrome_options)
        logging.info("ğŸ‰ ChromeDriver å¯åŠ¨æˆåŠŸï¼")
        return driver
    except WebDriverException as e:
        logging.error(f"âŒ ChromeDriver å¯åŠ¨å¤±è´¥ï¼š{e}")
        logging.error("è¯·æ£€æŸ¥ ChromeDriver è·¯å¾„ã€ç‰ˆæœ¬æ˜¯å¦ä¸ Chrome æµè§ˆå™¨åŒ¹é…ï¼Œä»¥åŠç³»ç»Ÿä¾èµ–æ˜¯å¦å®‰è£…ã€‚")
        return None

# --- çˆ¬å–å…¨å¸‚åœºåŸºé‡‘ä»£ç åˆ—è¡¨ï¼ˆä¿ç•™åŸåŠŸèƒ½ï¼Œä½†æ–°ç‰ˆæœ¬ä¸ä¼šè°ƒç”¨ï¼‰ ---
def get_all_fund_codes():
    """ä»å¤©å¤©åŸºé‡‘ç½‘è·å–æ‰€æœ‰åŸºé‡‘çš„ä»£ç åˆ—è¡¨ï¼Œå¹¶ç­›é€‰å‡ºCç±»åŸºé‡‘ã€‚"""
    logging.info("æ­£åœ¨çˆ¬å–å…¨å¸‚åœºåŸºé‡‘ä»£ç åˆ—è¡¨...")
    url = "http://fund.eastmoney.com/allfund.html"
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    
    try:
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()
        html = response.text
        soup = BeautifulSoup(html, 'lxml')
        
        fund_list = []
        for a_tag in soup.select('#code_content a'):
            code_name_text = a_tag.get_text(strip=True)
            match = re.match(r'\((\d{6})\)(.+)', code_name_text)
            if match:
                code, name = match.groups()
                fund_list.append({'code': code, 'name': name.strip()})
        
        logging.info(f"å·²è·å– {len(fund_list)} åªåŸºé‡‘çš„ä»£ç ã€‚")
        
        c_fund_list = [fund for fund in fund_list if fund['name'].endswith('C')]
        logging.info(f"å·²ç­›é€‰å‡º {len(c_fund_list)} åªåœºå¤–Cç±»åŸºé‡‘ã€‚")
        return c_fund_list

    except requests.exceptions.RequestException as e:
        logging.error(f"âŒ çˆ¬å–åŸºé‡‘ä»£ç åˆ—è¡¨å¤±è´¥ï¼š{e}")
        return []

# --- ä¸“é—¨è§£ææŒä»“è¡¨æ ¼çš„å‡½æ•° ---
def parse_holdings_table(soup, fund_code, year):
    """ä¸“é—¨è§£ææŒä»“è¡¨æ ¼çš„å‡½æ•°"""
    holdings_table = soup.find(id="cctable")
    if not holdings_table:
        return []
    
    holdings = []
    rows = holdings_table.find_all('tr')
    if not rows or len(rows) <= 1:
        return []
    
    for row in rows[1:]:
        cols = row.find_all('td')
        if len(cols) >= 5:
            try:
                data = {
                    'fund_code': fund_code,
                    'year': year,
                    'stock_code': cols[1].text.strip() if len(cols) > 1 else '',
                    'stock_name': cols[2].text.strip() if len(cols) > 2 else '',
                    'proportion': cols[3].text.strip() if len(cols) > 3 else '',
                    'shares': cols[4].text.strip() if len(cols) > 4 else '',
                    'market_value': cols[5].text.strip() if len(cols) > 5 else '',
                    'report_date': cols[0].text.strip() if len(cols) > 0 else ''
                }
                holdings.append(data)
            except Exception as e:
                logging.warning(f"è§£æè¡Œæ•°æ®å¤±è´¥: {e}")
                continue
    return holdings

# --- çˆ¬å–æŒ‡å®šåŸºé‡‘æŒä»“æ•°æ® ---
def get_fund_holdings(driver, fund_code, years_to_crawl, max_retries=3):
    """
    çˆ¬å–æŒ‡å®šåŸºé‡‘åœ¨è¿‘Nå¹´å†…çš„æŒä»“æ•°æ®ã€‚
    """
    if driver is None:
        logging.error("WebDriver å®ä¾‹ä¸å­˜åœ¨ï¼Œè·³è¿‡çˆ¬å–ã€‚")
        return pd.DataFrame()

    fund_holdings = []
    base_url = f"https://fundf10.eastmoney.com/ccmx_{fund_code}.html"

    logging.info(f"è®¿é—®åŸºé‡‘ {fund_code} é¡µé¢: {base_url}")
    
    for attempt in range(max_retries):
        try:
            logging.info(f"å°è¯•è®¿é—®é¡µé¢ (ç¬¬{attempt+1}æ¬¡)...")
            driver.get(base_url)
            
            wait = WebDriverWait(driver, 30)
            wait.until(
                EC.any_of(
                    EC.presence_of_element_located((By.ID, "cctable")),
                    EC.presence_of_element_located((By.CLASS_NAME, "placeholder"))
                )
            )
            
            page_source_check = driver.page_source
            if "æš‚æ— æ•°æ®" in page_source_check or "æ²¡æœ‰æ‰¾åˆ°" in page_source_check:
                logging.info(f"åŸºé‡‘ {fund_code} æš‚æ— æŒä»“æ•°æ®")
                return pd.DataFrame()
            
            logging.info("é¡µé¢åŠ è½½æˆåŠŸï¼Œå‡†å¤‡è§£ææ•°æ®ã€‚")
            break
        except TimeoutException:
            logging.warning(f"é¡µé¢åŠ è½½è¶…æ—¶ï¼ŒåŸºé‡‘ {fund_code} (ç¬¬{attempt+1}/{max_retries}æ¬¡é‡è¯•)")
            if attempt == max_retries - 1:
                logging.error(f"åŸºé‡‘ {fund_code} é¡µé¢åŠ è½½å¤±è´¥ï¼Œå·²é‡è¯•{max_retries}æ¬¡ï¼Œè·³è¿‡ã€‚")
                return pd.DataFrame()
            time.sleep(2 ** attempt)
        except Exception as e:
            logging.error(f"è®¿é—®åŸºé‡‘ {fund_code} é¡µé¢æ—¶å‘ç”Ÿæ„å¤–é”™è¯¯ï¼š{e}")
            if attempt == max_retries - 1:
                return pd.DataFrame()
            time.sleep(2 ** attempt)

    for year in years_to_crawl:
        try:
            logging.info(f"æ­£åœ¨çˆ¬å– {year} å¹´æŒä»“æ•°æ®...")
            
            year_selectors = [
                f"//label[@value='{year}']",
                f"//div[@id='pagebar']//label[@value='{year}']",
            ]
            
            year_button = None
            for selector in year_selectors:
                try:
                    year_button = WebDriverWait(driver, 10).until(
                        EC.element_to_be_clickable((By.XPATH, selector))
                    )
                    break
                except TimeoutException:
                    continue
            
            if not year_button:
                logging.warning(f"æœªæ‰¾åˆ°åŸºé‡‘ {fund_code} åœ¨ {year} å¹´çš„æŒä»“æŒ‰é’®ï¼Œè·³è¿‡ã€‚")
                continue
            
            driver.execute_script("arguments[0].scrollIntoView();", year_button)
            time.sleep(1)
            year_button.click()
            
            WebDriverWait(driver, 10).until(
                EC.presence_of_element_located((By.ID, "cctable"))
            )
            
            page_source = driver.page_source
            soup = BeautifulSoup(page_source, 'lxml')
            
            holdings = parse_holdings_table(soup, fund_code, year)
            fund_holdings.extend(holdings)
            logging.info(f"âœ… æˆåŠŸè·å– {len(holdings)} æ¡ {year} å¹´çš„æŒä»“è®°å½•ã€‚")
            
        except TimeoutException:
            logging.warning(f"åŸºé‡‘ {fund_code} åœ¨ {year} å¹´çš„æŒä»“æŒ‰é’®æˆ–è¡¨æ ¼åŠ è½½è¶…æ—¶ï¼Œè·³è¿‡ã€‚")
            continue
        except Exception as e:
            logging.error(f"çˆ¬å–åŸºé‡‘ {fund_code} çš„ {year} å¹´æ•°æ®æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
            continue
            
    return pd.DataFrame(fund_holdings)


def main():
    """ä¸»å‡½æ•°ï¼Œæ‰§è¡Œçˆ¬å–ä»»åŠ¡ã€‚"""
    # å®šä¹‰éœ€è¦çˆ¬å–çš„å¹´ä»½èŒƒå›´
    current_year = time.localtime().tm_year
    years_to_crawl = [str(current_year), str(current_year - 1), str(current_year - 2)]
    
    # æ”¹è¿›ï¼šé…ç½®å‚æ•°
    request_delay = 1  # è¯·æ±‚å»¶æ—¶

    logging.info("=== å¤©å¤©åŸºé‡‘æŒä»“æ•°æ®çˆ¬å–å™¨ ===")
    logging.info(f"ç›®æ ‡å¹´ä»½: {', '.join(years_to_crawl)}")
    
    # --- æ ¸å¿ƒä¿®æ”¹ï¼šä»Markdownæ–‡ä»¶è·å–åŸºé‡‘ä»£ç åˆ—è¡¨ ---
    report_file = 'market_monitor_report.md'
    fund_list_to_crawl = parse_markdown_file(report_file)
    if not fund_list_to_crawl:
        logging.error(f"æ— æ³•ä»æ–‡ä»¶ '{report_file}' è·å–åŸºé‡‘ä»£ç åˆ—è¡¨ï¼Œç¨‹åºé€€å‡ºã€‚")
        return

    logging.info(f"ğŸ“Š å‡†å¤‡çˆ¬å– {len(fund_list_to_crawl)} åªæŒ‡å®šåŸºé‡‘")
    
    # è®¾ç½®ä¸€ä¸ªæ–‡ä»¶è·¯å¾„æ¥å­˜å‚¨ç»“æœ
    output_dir = "fund_data"
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        
    timestamp = time.strftime('%Y%m%d_%H%M%S')
    output_filename = os.path.join(output_dir, f"target_fund_holdings_{timestamp}.csv")
    
    # å°è¯•å¯åŠ¨ WebDriverï¼Œå¦‚æœå¤±è´¥åˆ™ç›´æ¥é€€å‡º
    driver = setup_driver()
    if driver is None:
        return
        
    all_holdings_df = pd.DataFrame()
    successful_funds = 0
    
    try:
        for i, fund in enumerate(fund_list_to_crawl, 1):
            fund_code = fund['code']
            fund_name = fund['name']
            
            logging.info(f"\n--- [{i}/{len(fund_list_to_crawl)}] æ­£åœ¨å¤„ç†: {fund_name} ({fund_code}) ---")
            
            holdings_df = get_fund_holdings(driver, fund_code, years_to_crawl)
            if not holdings_df.empty:
                all_holdings_df = pd.concat([all_holdings_df, holdings_df], ignore_index=True)
                successful_funds += 1
                logging.info(f"âœ… æˆåŠŸè·å– {len(holdings_df)} æ¡æŒä»“è®°å½•")
            else:
                logging.info("âŒ æœªè·å–åˆ°æ•°æ®ï¼Œç»§ç»­ä¸‹ä¸€åªåŸºé‡‘ã€‚")
            
            time.sleep(request_delay)
            
    finally:
        logging.info("çˆ¬å–ä»»åŠ¡ç»“æŸï¼Œå…³é—­ WebDriverã€‚")
        driver.quit()
    
    if not all_holdings_df.empty:
        logging.info("\nğŸ‰ æ•°æ®çˆ¬å–å®Œæˆ!")
        logging.info(f"ğŸ“ å·²ä¿å­˜åˆ°æ–‡ä»¶ï¼š{output_filename}")
        logging.info(f"ğŸ“ˆ æ€»è®°å½•æ•°: {len(all_holdings_df)}")
        logging.info(f"âœ… æˆåŠŸåŸºé‡‘: {successful_funds}/{len(fund_list_to_crawl)}")
        
        try:
            all_holdings_df.to_csv(output_filename, index=False, encoding='utf-8-sig')
        except Exception as e:
            logging.error(f"ä¿å­˜æ–‡ä»¶æ—¶å‘ç”Ÿé”™è¯¯ï¼š{e}")
            
    else:
        logging.info("æ²¡æœ‰çˆ¬å–åˆ°ä»»ä½•æ•°æ®ã€‚")

if __name__ == '__main__':
    main()
