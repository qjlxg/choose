import requests
import pandas as pd
import numpy as np
import time
import os
import re
import json
from bs4 import BeautifulSoup
from datetime import datetime

class FundSignalCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://fund.eastmoney.com/'
        })
    
    def parse_signals_from_md(self, md_file='market_monitor_report.md'):
        """ä» Markdown è¡¨æ ¼è§£æä¹°å…¥ä¿¡å·åŸºé‡‘"""
        if not os.path.exists(md_file):
            print(f"âŒ æœªæ‰¾åˆ° {md_file} æ–‡ä»¶")
            return []
        
        with open(md_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print("ğŸ“– è§£æ Markdown è¡¨æ ¼...")
        
        # æŸ¥æ‰¾è¡¨æ ¼
        table_pattern = r'\|.*åŸºé‡‘ä»£ç .*\|.*è¡ŒåŠ¨ä¿¡å·.*\|(?:\n\|.*\|.*\|)*'
        table_match = re.search(table_pattern, content, re.DOTALL)
        
        if not table_match:
            print("âŒ æœªæ‰¾åˆ°åŸºé‡‘è¡¨æ ¼")
            return []
        
        table_content = table_match.group(0)
        
        # åˆ†å‰²è¡Œ
        lines = [line.strip() for line in table_content.split('\n') if line.strip()]
        
        # æ‰¾åˆ°è¡¨å¤´è¡Œï¼ˆåŒ…å«"åŸºé‡‘ä»£ç "å’Œ"è¡ŒåŠ¨ä¿¡å·"ï¼‰
        header_line = None
        for i, line in enumerate(lines):
            if 'åŸºé‡‘ä»£ç ' in line and 'è¡ŒåŠ¨ä¿¡å·' in line:
                header_line = i
                break
        
        if header_line is None:
            print("âŒ æœªæ‰¾åˆ°è¡¨å¤´è¡Œ")
            return []
        
        # è§£ææ•°æ®è¡Œ
        fund_signals = []
        for line in lines[header_line + 2:]:  # è·³è¿‡è¡¨å¤´ã€åˆ†éš”çº¿
            if not line.startswith('|') or line.endswith('|'):
                continue
                
            # æ¸…ç† Markdown è¡¨æ ¼æ ¼å¼
            cells = [cell.strip() for cell in line.split('|')[1:-1]]  # å»æ‰é¦–å°¾çš„ç©ºå•å…ƒæ ¼
            
            if len(cells) >= 8:  # ç¡®ä¿æœ‰è¶³å¤Ÿçš„åˆ—
                fund_code = cells[0].strip()  # ç¬¬ä¸€åˆ—ï¼šåŸºé‡‘ä»£ç 
                action_signal = cells[7].strip()  # æœ€åä¸€åˆ—ï¼šè¡ŒåŠ¨ä¿¡å·
                
                # éªŒè¯åŸºé‡‘ä»£ç æ ¼å¼
                if re.match(r'^\d{6}$', fund_code):
                    if 'ä¹°å…¥' in action_signal:  # åŒ…å«"å¼±ä¹°å…¥"æˆ–"å¼ºä¹°å…¥"
                        fund_signals.append({
                            'fund_code': fund_code,
                            'signal': action_signal
                        })
                        print(f"   âœ… {fund_code}: {action_signal}")
        
        fund_codes = [fs['fund_code'] for fs in fund_signals]
        print(f"ğŸ“Š æ‰¾åˆ° {len(fund_signals)} åªä¹°å…¥ä¿¡å·åŸºé‡‘: {fund_codes}")
        
        return fund_codes
    
    def extract_json_from_jsonp(self, text):
        """æå–å¹¶è§£æ JSONP"""
        try:
            # åŒ¹é… var apidata = {...};
            pattern = r'var\s+apidata\s*=\s*({.*?});?\s*$'
            match = re.search(pattern, text, re.DOTALL)
            if match:
                json_str = match.group(1)
                # å¤„ç†å•å¼•å·é—®é¢˜
                json_str = re.sub(r"(\w+)'?": r'"\1":', json_str)
                json_str = re.sub(r":\s*'([^']*)'?", r': "\1"', json_str)
                # æ¸…ç†è½¬ä¹‰å­—ç¬¦
                json_str = json_str.replace('\\"', '"').replace("\\'", "'")
                return json.loads(json_str)
        except Exception as e:
            print(f"JSONè§£æå¤±è´¥: {e}")
        return None
    
    def get_fund_name(self, fund_code):
        """è·å–åŸºé‡‘åç§°"""
        url = f"https://fund.eastmoney.com/{fund_code}.html"
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            name_elem = soup.select_one('h1') or soup.find('title')
            if name_elem:
                name = name_elem.get_text().strip()
                if ' - ' in name:
                    name = name.split(' - ')[0]
                return name
        except Exception as e:
            print(f"   âš ï¸ è·å– {fund_code} åç§°å¤±è´¥: {e}")
        return f"åŸºé‡‘{fund_code}"
    
    def crawl_year_holdings(self, fund_code, year):
        """çˆ¬å–å•å¹´æŒä»“æ•°æ®"""
        url = "https://fundf10.eastmoney.com/FundArchivesDatas.aspx"
        params = {
            'type': 'jjcc', 
            'code': fund_code, 
            'topline': '10',  # å‰10åªæŒä»“
            'year': year
        }
        
        try:
            print(f"      ğŸ“¡ è¯·æ±‚ {year}å¹´æ•°æ®...")
            response = self.session.get(url, params=params, timeout=10)
            response.raise_for_status()
            
            data = self.extract_json_from_jsonp(response.text)
            if not data or 'content' not in data:
                print(f"      âŒ æ—  {year}å¹´æ•°æ®")
                return []
            
            soup = BeautifulSoup(data['content'], 'html.parser')
            holdings = []
            
            # æŸ¥æ‰¾æ‰€æœ‰å­£åº¦å—
            boxes = soup.find_all('div', class_='box')
            print(f"      ğŸ“Š æ‰¾åˆ° {len(boxes)} ä¸ªå­£åº¦")
            
            for box in boxes:
                # æå–å­£åº¦æ ‡é¢˜
                title = box.find('h4', class_='t')
                if not title:
                    continue
                
                title_text = title.get_text().strip()
                quarter_match = re.search(r'(\d{4}å¹´)(\d)å­£åº¦', title_text)
                if not quarter_match:
                    continue
                
                year_str, quarter = quarter_match.groups()
                quarter = int(quarter)
                
                # æå–æŠ¥å‘Šæ—¥æœŸ
                date_elem = title.find('font', class_='px12')
                report_date = date_elem.get_text().strip() if date_elem else f"{year_str}Q{quarter}"
                
                # æŸ¥æ‰¾è¡¨æ ¼
                table = box.find('table', class_=re.compile(r'tzxq'))
                if not table:
                    continue
                
                # è§£æè¡¨æ ¼è¡Œ
                tbody = table.find('tbody')
                if tbody:
                    rows = tbody.find_all('tr')
                else:
                    rows = table.find_all('tr')[1:]  # è·³è¿‡è¡¨å¤´
                
                for row in rows:
                    cols = row.find_all('td')
                    if len(cols) < 7:
                        continue
                    
                    # æå–è‚¡ç¥¨ä»£ç 
                    code_link = cols[1].find('a')
                    stock_code = ''
                    if code_link and code_link.get('href'):
                        href = code_link['href']
                        code_match = re.search(r'r/[\d.]+(\d+)', href)
                        if code_match:
                            stock_code = code_match.group(1)
                    
                    if not stock_code or not stock_code.isdigit():
                        continue
                    
                    # æå–æŒä»“ä¿¡æ¯
                    holding = {
                        'fund_code': fund_code,
                        'year': year_str,
                        'quarter': quarter,
                        'report_date': report_date,
                        'stock_code': stock_code,
                        'stock_name': cols[2].get_text().strip(),
                        'ratio': cols[4].get_text().strip(),
                        'shares': cols[5].get_text().strip(),
                        'market_value': cols[6].get_text().strip().replace(',', '')
                    }
                    
                    # æ•°å€¼æ¸…æ´—
                    holding['ratio_clean'] = float(holding['ratio'].replace('%', '')) if holding['ratio'] else 0
                    holding['market_value_clean'] = float(holding['market_value']) if holding['market_value'] else 0
                    holding['shares_clean'] = float(holding['shares']) if holding['shares'] else 0
                    
                    holdings.append(holding)
            
            print(f"      âœ… {year}å¹´: {len(holdings)} æ¡è®°å½•")
            return holdings
            
        except requests.RequestException as e:
            print(f"      âŒ è¯·æ±‚å¤±è´¥: {e}")
            return []
        except Exception as e:
            print(f"      âŒ è§£æå¤±è´¥: {e}")
            return []
    
    def crawl_fund(self, fund_code):
        """çˆ¬å–å•åªåŸºé‡‘çš„æŒä»“"""
        print(f"\nğŸ“ˆ [{fund_code}] æ­£åœ¨çˆ¬å–...")
        fund_name = self.get_fund_name(fund_code)
        print(f"   ğŸ“‹ åç§°: {fund_name}")
        
        # ä¼˜å…ˆå°è¯•æœ€è¿‘å¹´ä»½
        years_to_try = [2024, 2023, 2022]
        all_holdings = []
        
        for year in years_to_try:
            year_holdings = self.crawl_year_holdings(fund_code, year)
            if year_holdings:
                all_holdings.extend(year_holdings)
                print(f"   ğŸ¯ {year}å¹´æˆåŠŸ: {len(year_holdings)} æ¡")
                break  # æˆåŠŸå°±åœæ­¢å°è¯•æ›´æ—©å¹´ä»½
            time.sleep(0.5)  # çŸ­æš‚å»¶æ—¶
        
        if not all_holdings:
            print(f"   âŒ æ— å¯ç”¨æŒä»“æ•°æ®")
            return pd.DataFrame()
        
        # ä¿å­˜å•åŸºé‡‘æ•°æ®
        df = pd.DataFrame(all_holdings)
        df['fund_name'] = fund_name
        
        os.makedirs('data', exist_ok=True)
        safe_name = re.sub(r'[^\w\s-]', '', fund_name)[:20]  # æ¸…ç†æ–‡ä»¶å
        filename = f"data/{fund_code}_{safe_name}_ä¹°å…¥ä¿¡å·.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        print(f"   ğŸ’¾ ä¿å­˜ {len(df)} æ¡ â†’ {filename}")
        
        # æ˜¾ç¤ºå‰5æ¡é¢„è§ˆ
        preview_cols = ['year', 'quarter', 'stock_code', 'stock_name', 'ratio']
        print(f"   ğŸ“Š å‰5æ¡:")
        print(df[preview_cols].head().to_string(index=False))
        
        return df

def main():
    """ä¸»å‡½æ•° - ç›´æ¥è¿è¡Œ"""
    print("ğŸš€ ä¹°å…¥ä¿¡å·åŸºé‡‘æŒä»“åˆ†æå·¥å…·")
    print("=" * 60)
    
    crawler = FundSignalCrawler()
    fund_codes = crawler.parse_signals_from_md()
    
    if not fund_codes:
        print("âŒ MD æ–‡ä»¶ä¸­æœªæ‰¾åˆ°å¼±ä¹°å…¥/å¼ºä¹°å…¥ä¿¡å·çš„åŸºé‡‘")
        print("è¯·æ£€æŸ¥ market_monitor_report.md æ–‡ä»¶æ ¼å¼")
        return
    
    print(f"\nğŸ¯ ç›®æ ‡: çˆ¬å– {len(fund_codes)} åªä¹°å…¥ä¿¡å·åŸºé‡‘")
    print("-" * 60)
    
    all_data = []
    success_count = 0
    
    for i, code in enumerate(fund_codes, 1):
        print(f"\n[{i:2d}/{len(fund_codes)}] {code}")
        df = crawler.crawl_fund(code)
        
        if not df.empty:
            all_data.append(df)
            success_count += 1
        
        # é˜²åçˆ¬å»¶æ—¶
        if i < len(fund_codes):
            wait_time = np.random.uniform(2, 4)
            print(f"   â³ ç­‰å¾… {wait_time:.1f} ç§’...")
            time.sleep(wait_time)
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    if all_data:
        combined_df = pd.concat(all_data, ignore_index=True)
        
        # åˆ›å»ºæ±‡æ€»æ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M')
        summary_file = f"data/ä¹°å…¥ä¿¡å·åŸºé‡‘æ±‡æ€»_{timestamp}.csv"
        combined_df.to_csv(summary_file, index=False, encoding='utf-8-sig')
        
        print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆï¼")
        print(f"âœ… æˆåŠŸ: {success_count}/{len(fund_codes)} åªåŸºé‡‘")
        print(f"ğŸ“Š æ€»æŒä»“è®°å½•: {len(combined_df):,} æ¡")
        print(f"ğŸ’¾ æ±‡æ€»æ–‡ä»¶: {summary_file}")
        
        # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
        print(f"\nğŸ“ˆ ä¹°å…¥ä¿¡å·åŸºé‡‘ç»Ÿè®¡:")
        stats = combined_df.groupby(['fund_code', 'fund_name']).agg({
            'stock_code': 'count',
            'ratio_clean': ['sum', 'mean']
        }).round(2)
        stats.columns = ['æŒä»“æ•°é‡', 'æ€»å æ¯”(%)', 'å¹³å‡å æ¯”(%)']
        print(stats.to_string())
        
        # ç”Ÿæˆç®€å•åˆ†æ
        print(f"\nğŸ’¡ å¿«é€Ÿåˆ†æ:")
        total_funds = len(fund_codes)
        avg_holdings = len(combined_df) / success_count if success_count > 0 else 0
        print(f"   â€¢ æ€»åŸºé‡‘æ•°: {total_funds} åª")
        print(f"   â€¢ æˆåŠŸè·å–: {success_count} åª")
        print(f"   â€¢ å¹³å‡æ¯åŸºé‡‘: {avg_holdings:.1f} æ¡æŒä»“")
        
    else:
        print("\nâŒ æ²¡æœ‰æˆåŠŸè·å–ä»»ä½•åŸºé‡‘æ•°æ®")
        print("å¯èƒ½åŸå› : ç½‘ç»œé—®é¢˜ã€APIå˜åŒ–ã€æˆ–åŸºé‡‘æ— æŒä»“æ•°æ®")

if __name__ == "__main__":
    main()
