import requests
import pandas as pd
import numpy as np
import time
import os
import re
import json
from bs4 import BeautifulSoup
from datetime import datetime

# æ£€æŸ¥å¹¶å¯¼å…¥æ‰€éœ€çš„åº“
try:
    import requests
    import pandas as pd
    import numpy as np
    from bs4 import BeautifulSoup
    from lxml import etree # å¼•å…¥lxmlä»¥æé«˜BeautifulSoupè§£æé€Ÿåº¦
except ImportError as e:
    print(f"âŒ ç¼ºå°‘å¿…è¦çš„Pythonåº“ï¼š{e}")
    print("è¯·ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼špip install requests pandas beautifulsoup4 lxml")
    exit()

class FundSignalCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://fund.eastmoney.com/'
        })

    def parse_signals_from_md(self, md_file='market_monitor_report.md'):
        """ä» Markdown è¡¨æ ¼è§£æä¹°å…¥ä¿¡å·åŸºé‡‘ï¼ˆå¢å¼ºç‰ˆï¼‰"""
        print("ğŸ” æ­£åœ¨æ£€æŸ¥ 'market_monitor_report.md' æ–‡ä»¶...")
        if not os.path.exists(md_file):
            print(f"âŒ æœªæ‰¾åˆ° {md_file} æ–‡ä»¶")
            print("è¯·ç¡®ä¿è¯¥æ–‡ä»¶ä¸è„šæœ¬åœ¨åŒä¸€ä¸ªç›®å½•ä¸‹ã€‚")
            # è°ƒè¯•ï¼šæ˜¾ç¤ºå½“å‰ç›®å½•æ–‡ä»¶
            print("ğŸ“ å½“å‰ç›®å½•æ–‡ä»¶:")
            for f in os.listdir('.'):
                print(f"    - {f}")
            return []

        with open(md_file, 'r', encoding='utf-8') as f:
            content = f.read()

        print("ğŸ“– è§£æ Markdown è¡¨æ ¼...")
        print(f"ğŸ“„ æ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦")
        print(f"ğŸ” å‰200å­—ç¬¦é¢„è§ˆ:\n{content[:200]}...")
        
        # åŒ¹é…åŒ…å«"åŸºé‡‘ä»£ç "å’Œ"è¡ŒåŠ¨ä¿¡å·"çš„è¡¨æ ¼
        table_pattern = r'(?s).*?\|.*?(?:åŸºé‡‘ä»£ç ).*?\|.*?\|.*?\|.*?\|.*?\|.*?\|.*?\|.*?\|.*?\|.*?\|.*?è¡ŒåŠ¨ä¿¡å·.*?\|.*?(?=\n\n|\Z)'
        table_match = re.search(table_pattern, content, re.DOTALL | re.IGNORECASE)

        if not table_match:
            print("âŒ æ­£åˆ™åŒ¹é…å¤±è´¥ï¼Œå°è¯•é€è¡Œè§£æ...")
            lines = content.split('\n')
            in_table = False
            table_lines = []

            for line in lines:
                line = line.strip()
                if line.startswith('|') and 'åŸºé‡‘ä»£ç ' in line and 'è¡ŒåŠ¨ä¿¡å·' in line:
                    in_table = True
                    table_lines = [line]
                    print(f"âœ… æ‰¾åˆ°è¡¨å¤´: {line[:80]}...")
                    continue
                if in_table:
                    if line.startswith('|') and len(line.split('|')) > 8:
                        table_lines.append(line)
                    elif not line.strip() and len(table_lines) > 1: # å½“é‡åˆ°ç©ºè¡Œæ—¶åœæ­¢
                        in_table = False
            
            if table_lines:
                print(f"âœ… æ‰¾åˆ° {len(table_lines)} è¡Œè¡¨æ ¼æ•°æ®")
                return self._parse_table_lines(table_lines)
            else:
                print("âŒ å¤‡ç”¨è§£æä¹Ÿå¤±è´¥")
                return []

        table_content = table_match.group(0)
        print(f"âœ… æ‰¾åˆ°è¡¨æ ¼: {len(table_content)} å­—ç¬¦")

        lines = [line.strip() for line in table_content.split('\n') if line.strip()]

        header_line_index = -1
        for i, line in enumerate(lines):
            if line.startswith('|') and 'åŸºé‡‘ä»£ç ' in line and 'è¡ŒåŠ¨ä¿¡å·' in line:
                header_line_index = i
                print(f"âœ… è¡¨å¤´è¡Œ {i}: {line}")
                break

        if header_line_index == -1:
            print("âŒ æœªæ‰¾åˆ°è¡¨å¤´è¡Œ")
            return []

        return self._parse_table_lines(lines[header_line_index:])

    def _parse_table_lines(self, table_lines):
        """è§£æè¡¨æ ¼è¡Œ"""
        fund_signals = []
        data_start = 2 if len(table_lines) > 2 and '|---' in table_lines[1] else 1
        
        print(f"ğŸ“Š å¼€å§‹è§£ææ•°æ®è¡Œ (ä»ç¬¬ {data_start} è¡Œ)")

        for i, line in enumerate(table_lines[data_start:], data_start):
            if not line.startswith('|'):
                continue
            
            parts = line.split('|')
            if len(parts) < 10:  # è‡³å°‘10ä¸ª | åˆ†éš”ç¬¦
                print(f"âš ï¸ è¡Œ {i} æ ¼å¼é”™è¯¯: {line[:50]}...")
                continue
            
            cells = [part.strip() for part in parts[1:-1]]  # å»æ‰é¦–å°¾ç©ºå•å…ƒæ ¼
            
            if len(cells) < 8:
                print(f"âš ï¸ è¡Œ {i} å•å…ƒæ ¼ä¸è¶³: {len(cells)} ä¸ª")
                continue
            
            fund_code = cells[0].strip()
            action_signal = cells[-1].strip()  # æœ€åä¸€åˆ—
            
            print(f"ğŸ” è¡Œ {i}: ä»£ç ={fund_code}, ä¿¡å·={action_signal}")
            
            if re.match(r'^\d{6}$', fund_code) and 'ä¹°å…¥' in action_signal:
                fund_signals.append({
                    'fund_code': fund_code,
                    'signal': action_signal
                })
                print(f"    âœ… æ·»åŠ : {fund_code} ({action_signal})")
            else:
                print(f"    âŒ è·³è¿‡: ä»£ç ={fund_code}, ä¿¡å·={action_signal}")
        
        fund_codes = [fs['fund_code'] for fs in fund_signals]
        print(f"ğŸ“Š æœ€ç»ˆç»“æœ: {len(fund_signals)} åªä¹°å…¥ä¿¡å·åŸºé‡‘")
        if fund_codes:
            print(f"    ğŸ“‹ åŸºé‡‘åˆ—è¡¨: {', '.join(fund_codes[:5])}{'...' if len(fund_codes) > 5 else ''}")
        
        return fund_codes

    def extract_json_from_jsonp(self, text):
        """æå– JSONP æ•°æ®"""
        try:
            pattern = r'var\s+apidata\s*=\s*(\{.*?\});?\s*$'
            match = re.search(pattern, text, re.DOTALL)
            if not match:
                print("âŒ æœªæ‰¾åˆ° apidata å˜é‡")
                return None
            
            json_str = match.group(1)
            json_str = re.sub(r"(\b\w+\b)'?\s*:", r'"\1":', json_str)
            json_str = re.sub(r":\s*'([^']*)'?", r': "\1"', json_str)
            json_str = json_str.replace('\\"', '"').replace("\\'", "'")
            json_str = re.sub(r'\\u003c', '<', json_str)
            json_str = re.sub(r'\\u003e', '>', json_str)
            
            return json.loads(json_str)
            
        except json.JSONDecodeError as e:
            print(f"âŒ JSON è§£æå¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"âŒ å…¶ä»–é”™è¯¯: {e}")
            return None

    def get_fund_name(self, fund_code):
        """è·å–åŸºé‡‘åç§°"""
        url = f"https://fund.eastmoney.com/{fund_code}.html"
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'lxml')
            name_elem = soup.select_one('h1') or soup.find('title')
            if name_elem:
                name = name_elem.get_text().strip()
                if ' - ' in name:
                    name = name.split(' - ')[0]
                return name
        except Exception:
            pass
        return f"åŸºé‡‘{fund_code}"

    def crawl_year_holdings(self, fund_code, year):
        """çˆ¬å–å•å¹´æŒä»“"""
        url = "https://fundf10.eastmoney.com/FundArchivesDatas.aspx"
        params = {'type': 'jjcc', 'code': fund_code, 'topline': '10', 'year': year}
        
        try:
            print(f"  è¯·æ±‚ {year} å¹´æŒä»“æ•°æ®...")
            response = self.session.get(url, params=params, timeout=10)
            data = self.extract_json_from_jsonp(response.text)

            if not data or 'content' not in data:
                print(f"  âŒ {year}å¹´æ— æ•°æ®")
                return []
            
            soup = BeautifulSoup(data['content'], 'lxml')
            holdings = []
            boxes = soup.find_all('div', class_='box')
            
            for box in boxes:
                title = box.find('h4', class_='t')
                if not title:
                    continue
                
                title_text = title.get_text()
                quarter_match = re.search(r'(\d{4}å¹´)(\d)å­£åº¦', title_text)
                if not quarter_match:
                    continue
                
                year_str, quarter = quarter_match.groups()
                quarter = int(quarter)
                
                table = box.find('table')
                if not table:
                    continue
                
                rows = table.find_all('tr')[1:]
                for row in rows:
                    cols = row.find_all('td')
                    if len(cols) < 7:
                        continue
                    
                    code_link = cols[1].find('a')
                    stock_code = ''
                    if code_link and code_link.get('href'):
                        href = code_link['href']
                        code_match = re.search(r'r/[\d.]+(\d+)', href)
                        if code_match:
                            stock_code = code_match.group(1)

                    if stock_code and stock_code.isdigit():
                        holding = {
                            'fund_code': fund_code,
                            'year': year_str,
                            'quarter': quarter,
                            'stock_code': stock_code,
                            'stock_name': cols[2].get_text().strip(),
                            'ratio': cols[4].get_text().strip(),
                            'shares': cols[5].get_text().strip(),
                            'market_value': cols[6].get_text().strip().replace(',', '')
                        }
                        
                        holding['ratio_clean'] = float(holding['ratio'].replace('%', '')) if holding['ratio'] else 0
                        holding['market_value_clean'] = float(holding['market_value']) if holding['market_value'] else 0
                        holding['shares_clean'] = float(holding['shares']) if holding['shares'] else 0
                        
                        holdings.append(holding)
            
            return holdings
        
        except Exception as e:
            print(f"  âŒ çˆ¬å– {year} å¹´æ•°æ®å¤±è´¥: {e}")
            return []

    def crawl_fund(self, fund_code):
        """çˆ¬å–å•åŸºé‡‘"""
        print(f"\nğŸ“ˆ [{fund_code}] æ­£åœ¨çˆ¬å–...")
        fund_name = self.get_fund_name(fund_code)
        print(f"  ğŸ“‹ åŸºé‡‘åç§°: {fund_name}")
        
        years_to_try = [datetime.now().year, datetime.now().year - 1]
        all_holdings = []
        
        for year in years_to_try:
            year_holdings = self.crawl_year_holdings(fund_code, year)
            if year_holdings:
                all_holdings.extend(year_holdings)
                print(f"  âœ… {year}å¹´: æ‰¾åˆ° {len(year_holdings)} æ¡è®°å½•")
                break
            time.sleep(0.5)
        
        if not all_holdings:
            print(f"  âŒ æ— æ•°æ®")
            return pd.DataFrame()
        
        df = pd.DataFrame(all_holdings)
        df['fund_name'] = fund_name
        
        os.makedirs('data', exist_ok=True)
        safe_name = re.sub(r'[^\w\s-]', '', fund_name)[:20]
        filename = f"data/{fund_code}_{safe_name}_ä¹°å…¥ä¿¡å·.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        print(f"  ğŸ’¾ {len(df)} æ¡è®°å½• â†’ {filename}")
        return df

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ä¹°å…¥ä¿¡å·åŸºé‡‘æŒä»“åˆ†æ")
    print("=" * 50)
    
    crawler = FundSignalCrawler()
    fund_codes = crawler.parse_signals_from_md()
    
    if not fund_codes:
        print("\nğŸ’¡ å»ºè®®æ£€æŸ¥:")
        print("1. market_monitor_report.md æ˜¯å¦åœ¨æ ¹ç›®å½•")
        print("2. æ–‡ä»¶ä¸­æ˜¯å¦åŒ…å«æ­£ç¡®çš„è¡¨æ ¼æ ¼å¼")
        print("3. è¡¨æ ¼æ˜¯å¦æœ‰ 'åŸºé‡‘ä»£ç ' å’Œ 'è¡ŒåŠ¨ä¿¡å·' åˆ—")
        print("4. 'è¡ŒåŠ¨ä¿¡å·' åˆ—æ˜¯å¦åŒ…å« 'ä¹°å…¥' å…³é”®è¯")
        return
    
    print(f"\nğŸ¯ å¼€å§‹çˆ¬å– {len(fund_codes)} åªåŸºé‡‘")
    
    all_data = []
    for i, code in enumerate(fund_codes, 1):
        df = crawler.crawl_fund(code)
        if not df.empty:
            all_data.append(df)
        
        if i < len(fund_codes):
            time.sleep(2)
    
    if all_data:
        combined = pd.concat(all_data, ignore_index=True)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        summary_file = f"data/ä¹°å…¥ä¿¡å·æ±‡æ€»_{timestamp}.csv"
        combined.to_csv(summary_file, index=False, encoding='utf-8-sig')
        
        print(f"\nğŸ‰ å®Œæˆï¼æ€» {len(combined)} æ¡è®°å½•")
        print(f"ğŸ’¾ æ±‡æ€»æ–‡ä»¶å·²ä¿å­˜è‡³: {summary_file}")
    else:
        print("\nâŒ çˆ¬å–å®Œæˆï¼Œä½†æ— æœ‰æ•ˆæ•°æ®ã€‚")

if __name__ == "__main__":
    main()

