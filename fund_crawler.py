import requests
import json
import pandas as pd
import numpy as np
import time
import os
import re
from bs4 import BeautifulSoup
from urllib.parse import urlencode
from datetime import datetime
import argparse

class FundHoldingsCrawler:
    def __init__(self, base_url="https://fundf10.eastmoney.com"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Referer': 'https://fund.eastmoney.com/'
        })
    
    def extract_json_from_jsonp(self, jsonp_response):
        """ä»JSONPå“åº”ä¸­æå–JSONæ•°æ®"""
        try:
            # ç§»é™¤JSONPåŒ…è£…ï¼Œæå– var apidata={...};
            pattern = r'var\s+apidata\s*=\s*({.*?});?\s*$'
            match = re.search(pattern, jsonp_response, re.DOTALL)
            if match:
                json_str = match.group(1)
                return json.loads(json_str)
            else:
                # å°è¯•ç›´æ¥è§£æï¼ˆå¦‚æœå·²ç»æ˜¯JSONæ ¼å¼ï¼‰
                return json.loads(jsonp_response)
        except json.JSONDecodeError as e:
            print(f"JSONè§£æå¤±è´¥: {e}")
            print(f"å“åº”é¢„è§ˆ: {jsonp_response[:200]}...")
            return None
    
    def crawl_fund_holdings_by_year(self, fund_code, year, topline=10):
        """çˆ¬å–æŒ‡å®šåŸºé‡‘æŒ‡å®šå¹´ä»½çš„æŒä»“æ•°æ®"""
        url = f"{self.base_url}/FundArchivesDatas.aspx"
        params = {
            'type': 'jjcc',  # åŸºé‡‘æŒä»“
            'code': fund_code,
            'topline': topline,
            'year': year
        }
        
        try:
            print(f"ğŸ“¡ æ­£åœ¨è¯·æ±‚ {fund_code} {year}å¹´æ•°æ®...")
            response = self.session.get(url, params=params, timeout=10)
            response.raise_for_status()
            
            # è§£æJSONPå“åº”
            data = self.extract_json_from_jsonp(response.text)
            if not data or 'content' not in data:
                print(f"âŒ æœªè·å–åˆ° {fund_code} {year}å¹´æ•°æ®")
                return []
            
            # è§£æHTMLå†…å®¹
            soup = BeautifulSoup(data['content'], 'html.parser')
            holdings = []
            
            # æŸ¥æ‰¾æ‰€æœ‰å­£åº¦è¡¨æ ¼
            quarter_sections = soup.find_all('div', class_='box')
            print(f"ğŸ“Š æ‰¾åˆ° {len(quarter_sections)} ä¸ªå­£åº¦æ•°æ®æ®µ")
            
            for i, section in enumerate(quarter_sections, 1):
                try:
                    # æå–å­£åº¦æ ‡é¢˜ï¼ˆ2023å¹´Xå­£åº¦ï¼‰
                    title_elem = section.find('h4', class_='t')
                    if title_elem:
                        title_text = title_elem.get_text().strip()
                        # æå–å­£åº¦ä¿¡æ¯ï¼Œå¦‚ "2023å¹´4å­£åº¦è‚¡ç¥¨æŠ•èµ„æ˜ç»†"
                        quarter_match = re.search(r'(\d{4}å¹´)(\d)å­£åº¦', title_text)
                        if quarter_match:
                            year_str, quarter = quarter_match.groups()
                            quarter = int(quarter)
                        else:
                            # é»˜è®¤å½“å‰å¹´ä»½çš„ç¬¬ä¸€ä¸ªå­£åº¦
                            year_str = str(year)
                            quarter = 1
                    else:
                        year_str = str(year)
                        quarter = i
                    
                    # æŸ¥æ‰¾è¡¨æ ¼
                    table = section.find('table', class_=re.compile(r'.*tzxq.*'))
                    if not table:
                        print(f"âš ï¸  æœªæ‰¾åˆ°ç¬¬{i}ä¸ªå­£åº¦çš„è¡¨æ ¼")
                        continue
                    
                    # è§£æè¡¨æ ¼è¡Œ
                    rows = table.find('tbody').find_all('tr') if table.find('tbody') else table.find_all('tr')
                    
                    for row in rows:
                        cols = row.find_all(['td', 'th'])
                        if len(cols) < 7:  # è‡³å°‘7åˆ—ï¼šåºå·ã€ä»£ç ã€åç§°ã€èµ„è®¯ã€æ¯”ä¾‹ã€æŒè‚¡æ•°ã€å¸‚å€¼
                            continue
                        
                        try:
                            # æå–è‚¡ç¥¨ä»£ç ï¼ˆä»é“¾æ¥ä¸­è·å–ï¼‰
                            code_link = cols[1].find('a')
                            stock_code = ''
                            if code_link and 'href' in code_link.attrs:
                                href = code_link['href']
                                # ä»URLä¸­æå–ä»£ç ï¼Œå¦‚ "1.688608" -> "688608"
                                code_match = re.search(r'r/[\d.]+(\d+)', href)
                                if code_match:
                                    stock_code = code_match.group(1)
                            
                            # æå–æ•°æ®
                            holding = {
                                'fund_code': fund_code,
                                'year': year_str,
                                'quarter': quarter,
                                'report_date': title_elem.find('font', class_='px12').get_text().strip() if title_elem and title_elem.find('font', class_='px12') else f"{year}-Q{quarter}",
                                'stock_code': stock_code,
                                'stock_name': cols[2].get_text().strip() if len(cols) > 2 else '',
                                'ratio': cols[4].get_text().strip() if len(cols) > 4 else '',
                                'shares': cols[5].get_text().strip() if len(cols) > 5 else '',
                                'market_value': cols[6].get_text().strip().replace(',', '') if len(cols) > 6 else ''
                            }
                            
                            # æ•°æ®æ¸…æ´—
                            if holding['ratio']:
                                holding['ratio_clean'] = holding['ratio'].replace('%', '').strip()
                            if holding['market_value']:
                                holding['market_value_clean'] = holding['market_value'].replace(',', '').strip()
                            if holding['shares']:
                                holding['shares_clean'] = holding['shares'].strip()
                            
                            # åªä¿ç•™æœ‰æ•ˆçš„è‚¡ç¥¨æŒä»“
                            if stock_code and stock_code.isdigit():
                                holdings.append(holding)
                                
                        except Exception as row_error:
                            print(f"âš ï¸  è§£æç¬¬{i}å­£åº¦ç¬¬{len(holdings)+1}è¡Œå¤±è´¥: {row_error}")
                            continue
                    
                    print(f"âœ… ç¬¬{i}å­£åº¦è§£æå®Œæˆ: {len([h for h in holdings[-10:] if 'stock_code' in h])} æ¡è®°å½•")
                    
                except Exception as section_error:
                    print(f"âŒ è§£æç¬¬{i}ä¸ªå­£åº¦å¤±è´¥: {section_error}")
                    continue
            
            print(f"ğŸ‰ {fund_code} {year}å¹´å…±è·å– {len(holdings)} æ¡æŒä»“è®°å½•")
            return holdings
            
        except requests.RequestException as e:
            print(f"âŒ è¯·æ±‚ {fund_code} {year}å¹´æ•°æ®å¤±è´¥: {e}")
            return []
        except Exception as e:
            print(f"âŒ è§£æ {fund_code} {year}å¹´æ•°æ®å¤±è´¥: {e}")
            return []
    
    def crawl_fund_holdings(self, fund_code, years_back=1, topline=10):
        """çˆ¬å–æŒ‡å®šåŸºé‡‘è¿‘Nå¹´çš„æŒä»“æ•°æ®"""
        print(f"ğŸš€ å¼€å§‹çˆ¬å–åŸºé‡‘ {fund_code} è¿‘ {years_back} å¹´æŒä»“æ•°æ®")
        
        current_year = datetime.now().year
        all_holdings = []
        
        for year_offset in range(years_back):
            year = current_year - year_offset
            year_holdings = self.crawl_fund_holdings_by_year(fund_code, year, topline)
            all_holdings.extend(year_holdings)
            
            # é˜²åçˆ¬å»¶æ—¶
            if year_offset < years_back - 1:
                wait_time = np.random.uniform(1, 3)
                print(f"â³ ç­‰å¾… {wait_time:.1f} ç§’...")
                time.sleep(wait_time)
        
        # ä¿å­˜ç»“æœ
        if all_holdings:
            df = pd.DataFrame(all_holdings)
            
            # æ•°æ®ç±»å‹è½¬æ¢
            numeric_cols = ['ratio_clean', 'shares_clean', 'market_value_clean']
            for col in numeric_cols:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = 'data'
            os.makedirs(output_dir, exist_ok=True)
            
            # ä¿å­˜è¯¦ç»†æ•°æ®
            output_file = f"{output_dir}/{fund_code}_holdings_{years_back}y.csv"
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
            
            # ä¿å­˜æ±‡æ€»ç»Ÿè®¡
            summary_stats = df.groupby(['year', 'quarter']).agg({
                'stock_code': 'count',
                'ratio_clean': ['sum', 'mean', 'max']
            }).round(4)
            summary_file = f"{output_dir}/{fund_code}_summary_{years_back}y.csv"
            summary_stats.to_csv(summary_file)
            
            print(f"ğŸ’¾ æ•°æ®å·²ä¿å­˜:")
            print(f"   ğŸ“‹ è¯¦ç»†æŒä»“: {output_file} ({len(df)} æ¡è®°å½•)")
            print(f"   ğŸ“Š å­£åº¦æ±‡æ€»: {summary_file}")
            
            # æ˜¾ç¤ºå‰10æ¡æ•°æ®é¢„è§ˆ
            print(f"\nğŸ“ˆ æ•°æ®é¢„è§ˆ (å‰10æ¡):")
            print(df[['year', 'quarter', 'stock_code', 'stock_name', 'ratio', 'market_value']].head(10).to_string(index=False))
            
            return df
        else:
            print(f"âŒ åŸºé‡‘ {fund_code} æœªè·å–åˆ°ä»»ä½•æŒä»“æ•°æ®")
            return pd.DataFrame()
    
    def get_fund_basic_info(self, fund_code):
        """è·å–åŸºé‡‘åŸºæœ¬ä¿¡æ¯"""
        url = f"{self.base_url}/F10/{fund_code}.html"
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å°è¯•æå–åŸºé‡‘åç§°
            name_selectors = [
                'h1',
                '.fund-name',
                '#fundInfo h1',
                '[class*="fund-name"]'
            ]
            
            fund_name = 'Unknown'
            for selector in name_selectors:
                name_elem = soup.select_one(selector)
                if name_elem:
                    fund_name = name_elem.get_text().strip()
                    break
            
            # å°è¯•æå–åŸºé‡‘ç±»å‹
            fund_type = 'Unknown'
            type_selectors = [
                '.fund-type',
                '[class*="fund-type"]',
                '.info-table td:contains("ç±»å‹") + td'
            ]
            
            return {
                'fund_code': fund_code,
                'fund_name': fund_name,
                'fund_type': fund_type
            }
            
        except Exception as e:
            print(f"âš ï¸  è·å– {fund_code} åŸºæœ¬ä¿¡æ¯å¤±è´¥: {e}")
            return {'fund_code': fund_code, 'fund_name': 'Unknown', 'fund_type': 'Unknown'}
    
    def get_all_fund_codes(self, max_codes=50):
        """è·å–å…¨å¸‚åœºåŸºé‡‘ä»£ç åˆ—è¡¨"""
        print("ğŸ“‹ è·å–å…¨å¸‚åœºåŸºé‡‘åˆ—è¡¨...")
        
        # ä½¿ç”¨å¤©å¤©åŸºé‡‘çš„åŸºé‡‘æ’è¡Œæ¥å£
        rank_url = "https://fund.eastmoney.com/data/rankhandler.aspx"
        params = {
            'op': 'ph',
            'dt': 'kf',  # å…¨éƒ¨åŸºé‡‘
            'ft': 'gp',  # è‚¡ç¥¨å‹
            'rs': '',
            'gs': '0',
            'sc': 'jn',  # è¿‘ä¸€å¹´æ”¶ç›Š
            'st': 'desc',
            'pi': '1',
            'pn': str(max_codes * 2),  # å¤šå–ä¸€äº›
            'dx': '0'
        }
        
        try:
            response = self.session.get(rank_url, params=params, timeout=10)
            data = self.extract_json_from_jsonp(response.text)
            
            if data and 'datas' in data:
                # è§£æè¿”å›çš„åŸºé‡‘åˆ—è¡¨
                fund_list = data['datas'].split('|') if isinstance(data['datas'], str) else []
                fund_codes = []
                
                for item in fund_list:
                    if '|' in item:
                        parts = item.split('|')
                        if len(parts) >= 2 and parts[0].isdigit() and len(parts[0]) == 6:
                            fund_codes.append(parts[0])
                
                print(f"âœ… è·å–åˆ° {len(fund_codes)} åªåŸºé‡‘")
                return fund_codes[:max_codes]
            else:
                print("âš ï¸  ä½¿ç”¨ç¤ºä¾‹åŸºé‡‘ä»£ç ")
                return ['002580', '000689', '001298', '000001', '000002'][:max_codes]
                
        except Exception as e:
            print(f"âŒ è·å–åŸºé‡‘åˆ—è¡¨å¤±è´¥: {e}")
            return ['002580', '000689', '001298', '000001', '000002'][:max_codes]

def main():
    parser = argparse.ArgumentParser(description='å¤©å¤©åŸºé‡‘ç½‘æŒä»“æ•°æ®çˆ¬å–å·¥å…·')
    parser.add_argument('--fund-code', type=str, default='002580', 
                       help='åŸºé‡‘ä»£ç  (é»˜è®¤: 002580)')
    parser.add_argument('--years', type=int, default=1, 
                       help='çˆ¬å–å¹´æ•° (é»˜è®¤: 1)')
    parser.add_argument('--topline', type=int, default=10, 
                       help='æ¯å­£åº¦å‰NåªæŒä»“ (é»˜è®¤: 10)')
    parser.add_argument('--all', action='store_true', 
                       help='çˆ¬å–æ‰€æœ‰åŸºé‡‘ (å‰20åª)')
    parser.add_argument('--max-codes', type=int, default=20,
                       help='æ‰¹é‡æ¨¡å¼æœ€å¤§åŸºé‡‘æ•°é‡ (é»˜è®¤: 20)')
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–çˆ¬è™«
    crawler = FundHoldingsCrawler()
    
    if args.all:
        # æ‰¹é‡æ¨¡å¼
        print("ğŸš€ æ‰¹é‡çˆ¬å–æ¨¡å¼å¯åŠ¨")
        fund_codes = crawler.get_all_fund_codes(args.max_codes)
        print(f"ğŸ“‹ å°†å¤„ç† {len(fund_codes)} åªåŸºé‡‘")
        
        all_results = []
        success_count = 0
        
        for i, code in enumerate(fund_codes, 1):
            print(f"\n{'='*60}")
            print(f"[{i:2d}/{len(fund_codes)}] æ­£åœ¨å¤„ç†: {code}")
            print(f"{'='*60}")
            
            # è·å–åŸºæœ¬ä¿¡æ¯
            info = crawler.get_fund_basic_info(code)
            print(f"ğŸ“„ åŸºé‡‘åç§°: {info['fund_name']}")
            
            # çˆ¬å–æŒä»“
            df = crawler.crawl_fund_holdings(code, args.years, args.topline)
            
            if not df.empty:
                # æ·»åŠ åŸºé‡‘åŸºæœ¬ä¿¡æ¯
                df['fund_name'] = info['fund_name']
                df['fund_type'] = info['fund_type']
                all_results.append(df)
                success_count += 1
                
                print(f"âœ… {code} å¤„ç†æˆåŠŸ: {len(df)} æ¡è®°å½•")
            else:
                print(f"âŒ {code} å¤„ç†å¤±è´¥")
            
            # å»¶æ—¶é˜²åçˆ¬
            if i < len(fund_codes):
                wait_time = np.random.uniform(2, 5)
                print(f"â³ ç­‰å¾… {wait_time:.1f} ç§’...")
                time.sleep(wait_time)
        
        # åˆå¹¶æ‰€æœ‰ç»“æœ
        if all_results:
            combined_df = pd.concat(all_results, ignore_index=True)
            
            # ä¿å­˜æ€»æ±‡æ€»
            total_file = f"data/all_funds_holdings_{args.years}y_{datetime.now().strftime('%Y%m%d')}.csv"
            combined_df.to_csv(total_file, index=False, encoding='utf-8-sig')
            
            # ç”Ÿæˆåˆ†ææŠ¥å‘Š
            print(f"\nğŸ‰ æ‰¹é‡ä»»åŠ¡å®Œæˆï¼")
            print(f"âœ… æˆåŠŸ: {success_count}/{len(fund_codes)} åªåŸºé‡‘")
            print(f"ğŸ“Š æ€»è®°å½•æ•°: {len(combined_df):,}")
            print(f"ğŸ’¾ æ€»æ±‡æ€»: {total_file}")
            
            # æ˜¾ç¤ºç»Ÿè®¡
            print(f"\nğŸ“ˆ æŒ‰åŸºé‡‘ç»Ÿè®¡:")
            fund_stats = combined_df.groupby('fund_code').size().reset_index(name='record_count')
            print(fund_stats.to_string(index=False))
            
    else:
        # å•åŸºé‡‘æ¨¡å¼
        print(f"ğŸ¯ å•åŸºé‡‘æ¨¡å¼: {args.fund_code}")
        df = crawler.crawl_fund_holdings(args.fund_code, args.years, args.topline)
        
        if not df.empty:
            print(f"\nâœ… çˆ¬å–å®Œæˆï¼å…± {len(df)} æ¡è®°å½•")
        else:
            print(f"\nâŒ çˆ¬å–å¤±è´¥")

if __name__ == "__main__":
    main()
