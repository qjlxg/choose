import requests
import pandas as pd
import numpy as np
import time
import os
import re
import ast
from bs4 import BeautifulSoup
from datetime import datetime

class FundHoldingsCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Referer': 'https://fund.eastmoney.com/'
        })
    
    def extract_json_from_jsonp(self, text):
        """æå–JSONPä¸­çš„JSONæ•°æ®"""
        try:
            # åŒ¹é… var XXX = {...};
            pattern = r'var\s+\w+\s*=\s*({.*?});?\s*$'
            match = re.search(pattern, text, re.DOTALL)
            if match:
                json_str = match.group(1)
                # æ›¿æ¢å•å¼•å·ä¸ºåŒå¼•å·ï¼Œå¤„ç†HTMLä¸­çš„å•å¼•å·é—®é¢˜
                json_str = re.sub(r"'([^']*)':", r'"\1":', json_str)
                json_str = re.sub(r":\s*'([^']*)'", r': "\1"', json_str)
                return json.loads(json_str)
        except Exception as e:
            print(f"è§£æå¤±è´¥: {e}")
            return None
        return None
    
    def get_fund_list(self):
        """è·å–åŸºé‡‘åˆ—è¡¨"""
        print("ğŸ“‹ è·å–åŸºé‡‘åˆ—è¡¨...")
        url = "https://fund.eastmoney.com/data/rankhandler.aspx"
        params = {
            'op': 'ph', 'dt': 'kf', 'ft': 'gp', 'rs': '', 'gs': '0',
            'sc': 'jn', 'st': 'desc', 'pi': '1', 'pn': '50', 'dx': '0'
        }
        
        try:
            response = self.session.get(url, params=params, timeout=10)
            data = self.extract_json_from_jsonp(response.text)
            
            if data and 'datas' in data:
                fund_list = data['datas'].split('|') if isinstance(data['datas'], str) else []
                fund_codes = []
                for item in fund_list:
                    if '|' in item:
                        parts = item.split('|')
                        if len(parts) >= 1 and parts[0].isdigit() and len(parts[0]) == 6:
                            fund_codes.append(parts[0])
                print(f"âœ… è·å–åˆ° {len(fund_codes)} åªåŸºé‡‘")
                return fund_codes[:5]  # é»˜è®¤å‰5åª
        except Exception as e:
            print(f"âŒ è·å–åˆ—è¡¨å¤±è´¥: {e}")
        
        # é»˜è®¤åŸºé‡‘åˆ—è¡¨
        print("ğŸ“‹ ä½¿ç”¨é»˜è®¤åŸºé‡‘åˆ—è¡¨")
        return ['002580', '000689', '001298', '000001', '000002']
    
    def get_fund_name(self, fund_code):
        """è·å–åŸºé‡‘åç§°"""
        url = f"https://fund.eastmoney.com/{fund_code}.html"
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            name_elem = soup.select_one('h1') or soup.find('title')
            if name_elem:
                name = name_elem.get_text().strip()
                # æ¸…ç†æ ‡é¢˜ä¸­çš„å¤šä½™æ–‡å­—
                if ' - ' in name:
                    name = name.split(' - ')[0]
                return name
        except:
            pass
        return f"åŸºé‡‘{fund_code}"
    
    def crawl_year_holdings(self, fund_code, year):
        """çˆ¬å–å•å¹´æŒä»“"""
        url = "https://fundf10.eastmoney.com/FundArchivesDatas.aspx"
        params = {'type': 'jjcc', 'code': fund_code, 'topline': '10', 'year': year}
        
        try:
            response = self.session.get(url, params=params, timeout=10)
            data = self.extract_json_from_jsonp(response.text)
            
            if not data or 'content' not in data:
                return []
            
            soup = BeautifulSoup(data['content'], 'html.parser')
            holdings = []
            
            # æŸ¥æ‰¾æ‰€æœ‰å­£åº¦
            boxes = soup.find_all('div', class_='box')
            for box in boxes:
                # æå–å­£åº¦ä¿¡æ¯
                title = box.find('h4')
                if not title:
                    continue
                
                title_text = title.get_text()
                quarter_match = re.search(r'(\d{4}å¹´)(\d)å­£åº¦', title_text)
                if not quarter_match:
                    continue
                
                year_str, quarter = quarter_match.groups()
                quarter = int(quarter)
                
                # æŸ¥æ‰¾è¡¨æ ¼
                table = box.find('table')
                if not table:
                    continue
                
                rows = table.find_all('tr')[1:]  # è·³è¿‡è¡¨å¤´
                for row in rows:
                    cols = row.find_all('td')
                    if len(cols) < 7:
                        continue
                    
                    # æå–è‚¡ç¥¨ä»£ç 
                    code_link = cols[1].find('a')
                    stock_code = ''
                    if code_link:
                        href = code_link.get('href', '')
                        code_match = re.search(r'r/[\d.]+(\d+)', href)
                        if code_match:
                            stock_code = code_match.group(1)
                    
                    if not stock_code or not stock_code.isdigit():
                        continue
                    
                    # æå–æ•°æ®
                    holding = {
                        'fund_code': fund_code,
                        'year': year_str,
                        'quarter': quarter,
                        'stock_code': stock_code,
                        'stock_name': cols[2].get_text().strip(),
                        'ratio': cols[4].get_text().strip(),
                        'shares': cols[5].get_text().strip(),
                        'market_value': cols[6].get_text().strip().replace(',', '')
                    }
                    
                    # æ•°æ®æ¸…æ´—
                    if holding['ratio']:
                        holding['ratio_clean'] = float(holding['ratio'].replace('%', ''))
                    if holding['market_value']:
                        holding['market_value_clean'] = float(holding['market_value'])
                    if holding['shares']:
                        holding['shares_clean'] = float(holding['shares'])
                    
                    holdings.append(holding)
            
            return holdings
        except Exception as e:
            print(f"âŒ {fund_code}-{year}å¤±è´¥: {e}")
            return []
    
    def crawl_fund(self, fund_code):
        """çˆ¬å–å•åªåŸºé‡‘ï¼ˆé»˜è®¤2024å¹´ï¼‰"""
        print(f"\nğŸ“ˆ æ­£åœ¨çˆ¬å– {fund_code}...")
        fund_name = self.get_fund_name(fund_code)
        print(f"   åŸºé‡‘åç§°: {fund_name}")
        
        # å°è¯•2024å¹´ï¼Œå¦‚æœæ²¡æœ‰åˆ™å°è¯•2023å¹´
        years_to_try = [2024, 2023]
        all_holdings = []
        
        for year in years_to_try:
            print(f"   ğŸ“… å°è¯•{year}å¹´æ•°æ®...")
            year_holdings = self.crawl_year_holdings(fund_code, year)
            if year_holdings:
                all_holdings.extend(year_holdings)
                print(f"   âœ… {year}å¹´è·å– {len(year_holdings)} æ¡")
                break  # æˆåŠŸè·å–å°±åœæ­¢
            time.sleep(1)
        
        if not all_holdings:
            print(f"   âŒ {fund_code} æ— å¯ç”¨æ•°æ®")
            return pd.DataFrame()
        
        # ä¿å­˜æ•°æ®
        df = pd.DataFrame(all_holdings)
        df['fund_name'] = fund_name
        
        os.makedirs('data', exist_ok=True)
        filename = f"data/{fund_code}_{fund_name[:20]}_holdings.csv"
        df.to_csv(filename, index=False, encoding='utf-8-sig')
        
        print(f"   ğŸ’¾ ä¿å­˜ {len(df)} æ¡åˆ° {filename}")
        print(f"   ğŸ“Š å‰3æ¡é¢„è§ˆ:")
        preview_cols = ['year', 'quarter', 'stock_code', 'stock_name', 'ratio']
        print(df[preview_cols].head(3).to_string(index=False))
        
        return df

def main():
    """ä¸»å‡½æ•° - æ— å‚æ•°ç›´æ¥è¿è¡Œ"""
    print("ğŸš€ åŸºé‡‘æŒä»“çˆ¬å–å·¥å…·å¯åŠ¨")
    print("ğŸ“… é»˜è®¤é…ç½®: å‰5åªåŸºé‡‘, 2024å¹´æ•°æ®, æ¯å­£åº¦å‰10åªæŒä»“")
    print("-" * 50)
    
    crawler = FundHoldingsCrawler()
    
    # è·å–åŸºé‡‘åˆ—è¡¨
    fund_codes = crawler.get_fund_list()
    
    # çˆ¬å–æ‰€æœ‰åŸºé‡‘
    all_data = []
    success_count = 0
    
    for i, code in enumerate(fund_codes, 1):
        print(f"\n[{i}/{len(fund_codes)}] {code}")
        df = crawler.crawl_fund(code)
        
        if not df.empty:
            all_data.append(df)
            success_count += 1
        
        # å»¶æ—¶é˜²åçˆ¬
        if i < len(fund_codes):
            wait = np.random.uniform(2, 4)
            print(f"   â³ ç­‰å¾… {wait:.1f}ç§’...")
            time.sleep(wait)
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    if all_data:
        combined = pd.concat(all_data, ignore_index=True)
        summary_file = f"data/æ±‡æ€»_{datetime.now().strftime('%Y%m%d')}.csv"
        combined.to_csv(summary_file, index=False, encoding='utf-8-sig')
        
        print(f"\nğŸ‰ ä»»åŠ¡å®Œæˆï¼")
        print(f"âœ… æˆåŠŸ: {success_count}/{len(fund_codes)} åªåŸºé‡‘")
        print(f"ğŸ“Š æ€»è®°å½•: {len(combined)} æ¡")
        print(f"ğŸ’¾ æ±‡æ€»æ–‡ä»¶: {summary_file}")
        
        # æ˜¾ç¤ºæ€»ä½“ç»Ÿè®¡
        print("\nğŸ“ˆ æŒ‰åŸºé‡‘ç»Ÿè®¡:")
        stats = combined.groupby('fund_code').agg({
            'stock_code': 'count',
            'ratio_clean': 'sum'
        }).round(2)
        stats.columns = ['æŒä»“æ•°é‡', 'æ€»å æ¯”']
        print(stats.to_string())
    else:
        print("\nâŒ æ²¡æœ‰è·å–åˆ°ä»»ä½•æ•°æ®")

if __name__ == "__main__":
    main()
