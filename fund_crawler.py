import requests
import pandas as pd
import numpy as np
import time
import os
import re
import ast  # ç”¨äºå®‰å…¨è§£æ
from bs4 import BeautifulSoup
from urllib.parse import urlencode
from datetime import datetime
import argparse

class FundHoldingsCrawler:
    def __init__(self, base_url="https://fundf10.eastmoney.com"):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Referer': 'https://fund.eastmoney.com/'
        })
    
    def extract_json_from_jsonp(self, jsonp_response):
        """ä»JSONPå“åº”ä¸­æå–å¹¶è§£æJSONæ•°æ®ï¼ˆä½¿ç”¨ast.literal_evalå®‰å…¨è§£æï¼‰"""
        try:
            # åŒ¹é… var XXX = {...};
            pattern = r'var\s+\w+\s*=\s*({.*?});?\s*$'
            match = re.search(pattern, jsonp_response, re.DOTALL | re.MULTILINE)
            if match:
                json_str = match.group(1)
                # ast.literal_eval èƒ½å¤„ç†å•å¼•å·å’Œç®€å•ç»“æ„
                data = ast.literal_eval(json_str)
                return data
            else:
                print(f"æœªåŒ¹é…åˆ°JSONPæ¨¡å¼: {jsonp_response[:100]}...")
                return None
        except (ast.literal_eval, SyntaxError, ValueError) as e:
            print(f"è§£æå¤±è´¥: {e}")
            print(f"å“åº”é¢„è§ˆ: {jsonp_response[:200]}...")
            return None
    
    def crawl_fund_holdings_by_year(self, fund_code, year, topline=10):
        """çˆ¬å–æŒ‡å®šåŸºé‡‘æŒ‡å®šå¹´ä»½çš„æŒä»“æ•°æ®"""
        url = f"{self.base_url}/FundArchivesDatas.aspx"
        params = {
            'type': 'jjcc',
            'code': fund_code,
            'topline': topline,
            'year': year
        }
        
        try:
            print(f"ğŸ“¡ è¯·æ±‚ {fund_code} {year}å¹´æ•°æ®...")
            response = self.session.get(url, params=params, timeout=10)
            response.raise_for_status()
            
            data = self.extract_json_from_jsonp(response.text)
            if not data or 'content' not in data:
                print(f"âŒ æ—  {fund_code} {year}å¹´æ•°æ®")
                return []
            
            soup = BeautifulSoup(data['content'], 'html.parser')
            holdings = []
            
            quarter_sections = soup.find_all('div', class_='box')
            print(f"ğŸ“Š æ‰¾åˆ° {len(quarter_sections)} ä¸ªå­£åº¦")
            
            for i, section in enumerate(quarter_sections, 1):
                title_elem = section.find('h4', class_='t')
                if title_elem:
                    title_text = title_elem.get_text().strip()
                    quarter_match = re.search(r'(\d{4}å¹´)(\d)å­£åº¦', title_text)
                    if quarter_match:
                        year_str, quarter = quarter_match.groups()
                        quarter = int(quarter)
                    else:
                        year_str = str(year)
                        quarter = i
                    report_date = title_elem.find('font', class_='px12').get_text().strip() if title_elem.find('font', class_='px12') else f"{year}-Q{quarter}"
                else:
                    year_str = str(year)
                    quarter = i
                    report_date = f"{year}-Q{quarter}"
                
                table = section.find('table', class_=re.compile(r'.*tzxq.*'))
                if not table:
                    continue
                
                rows = table.find('tbody').find_all('tr') if table.find('tbody') else table.find_all('tr')
                
                for row in rows:
                    cols = row.find_all(['td', 'th'])
                    if len(cols) < 7:
                        continue
                    
                    code_link = cols[1].find('a')
                    stock_code = ''
                    if code_link and 'href' in code_link.attrs:
                        href = code_link['href']
                        code_match = re.search(r'r/[\d.]+(\d+)', href)
                        if code_match:
                            stock_code = code_match.group(1)
                    
                    holding = {
                        'fund_code': fund_code,
                        'year': year_str,
                        'quarter': quarter,
                        'report_date': report_date,
                        'stock_code': stock_code,
                        'stock_name': cols[2].get_text().strip() if len(cols) > 2 else '',
                        'ratio': cols[4].get_text().strip() if len(cols) > 4 else '',
                        'shares': cols[5].get_text().strip() if len(cols) > 5 else '',
                        'market_value': cols[6].get_text().strip().replace(',', '') if len(cols) > 6 else ''
                    }
                    
                    if holding['ratio']:
                        holding['ratio_clean'] = holding['ratio'].replace('%', '').strip()
                    if holding['market_value']:
                        holding['market_value_clean'] = holding['market_value'].replace(',', '').strip()
                    if holding['shares']:
                        holding['shares_clean'] = holding['shares'].strip()
                    
                    if stock_code and stock_code.isdigit():
                        holdings.append(holding)
            
            print(f"âœ… {fund_code} {year}å¹´: {len(holdings)} æ¡è®°å½•")
            return holdings
            
        except Exception as e:
            print(f"âŒ {fund_code} {year}å¹´å¤±è´¥: {e}")
            return []
    
    def crawl_fund_holdings(self, fund_code, years_back=1, topline=10):
        """çˆ¬å–è¿‘Nå¹´æŒä»“ï¼ˆä»2024å¹´å¼€å§‹ï¼‰"""
        print(f"ğŸš€ çˆ¬å– {fund_code} è¿‘ {years_back} å¹´")
        base_year = 2024  # å›ºå®šä»2024å¼€å§‹
        all_holdings = []
        
        for year_offset in range(years_back):
            year = base_year - year_offset
            year_holdings = self.crawl_fund_holdings_by_year(fund_code, year, topline)
            all_holdings.extend(year_holdings)
            time.sleep(np.random.uniform(1, 2))
        
        if all_holdings:
            df = pd.DataFrame(all_holdings)
            numeric_cols = ['ratio_clean', 'shares_clean', 'market_value_clean']
            for col in numeric_cols:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            os.makedirs('data', exist_ok=True)
            output_file = f"data/{fund_code}_holdings_{years_back}y.csv"
            df.to_csv(output_file, index=False, encoding='utf-8-sig')
            
            print(f"ğŸ’¾ ä¿å­˜ {len(df)} æ¡åˆ° {output_file}")
            print(df[['year', 'quarter', 'stock_code', 'stock_name', 'ratio']].head(5).to_string(index=False))
            return df
        return pd.DataFrame()
    
    def get_fund_basic_info(self, fund_code):
        """è·å–åŸºé‡‘åŸºæœ¬ä¿¡æ¯ï¼ˆä¿®å¤ç¼–ç ï¼‰"""
        url = f"https://fund.eastmoney.com/{fund_code}.html"
        try:
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser', from_encoding='utf-8')
            
            name_elem = soup.select_one('h1') or soup.find('title')
            fund_name = name_elem.get_text().strip() if name_elem else 'Unknown'
            
            return {'fund_code': fund_code, 'fund_name': fund_name, 'fund_type': 'Unknown'}
        except Exception as e:
            print(f"âš ï¸ {fund_code} ä¿¡æ¯å¤±è´¥: {e}")
            return {'fund_code': fund_code, 'fund_name': 'Unknown', 'fund_type': 'Unknown'}
    
    def get_all_fund_codes(self, max_codes=5):
        """è·å–åŸºé‡‘åˆ—è¡¨ï¼ˆä¿®å¤JSONPï¼‰"""
        rank_url = "https://fund.eastmoney.com/data/rankhandler.aspx"
        params = {
            'op': 'ph', 'dt': 'kf', 'ft': 'gp', 'rs': '', 'gs': '0',
            'sc': 'jn', 'st': 'desc', 'pi': '1', 'pn': str(max_codes * 2), 'dx': '0'
        }
        
        try:
            response = self.session.get(rank_url, params=params, timeout=10)
            data = self.extract_json_from_jsonp(response.text)
            
            if data and 'datas' in data:
                fund_list = data['datas'].split(',') if isinstance(data['datas'], str) else data['datas']
                fund_codes = []
                for item in fund_list:
                    parts = re.split(r'[,|]', item)
                    if len(parts) >= 2 and parts[0].isdigit() and len(parts[0]) == 6:
                        fund_codes.append(parts[0])
                print(f"âœ… è·å– {len(fund_codes)} åªåŸºé‡‘")
                return fund_codes[:max_codes]
        except Exception as e:
            print(f"âŒ åˆ—è¡¨å¤±è´¥: {e}")
        
        return ['002580', '000689', '001298', '000001', '000002'][:max_codes]

def main():
    parser = argparse.ArgumentParser(description='åŸºé‡‘æŒä»“çˆ¬å–')
    parser.add_argument('--fund-code', type=str, default='002580')
    parser.add_argument('--years', type=int, default=1)
    parser.add_argument('--topline', type=int, default=10)
    parser.add_argument('--all', action='store_true')
    parser.add_argument('--max-codes', type=int, default=5)
    
    args = parser.parse_args()
    crawler = FundHoldingsCrawler()
    
    if args.all:
        fund_codes = crawler.get_all_fund_codes(args.max_codes)
        all_results = []
        for i, code in enumerate(fund_codes, 1):
            print(f"\n[ {i}/{len(fund_codes)} ] {code}")
            info = crawler.get_fund_basic_info(code)
            print(f"ğŸ“„ {info['fund_name']}")
            df = crawler.crawl_fund_holdings(code, args.years, args.topline)
            if not df.empty:
                df['fund_name'] = info['fund_name']
                all_results.append(df)
            time.sleep(np.random.uniform(2, 4))
        
        if all_results:
            combined = pd.concat(all_results, ignore_index=True)
            combined.to_csv(f"data/all_holdings_{args.years}y.csv", index=False, encoding='utf-8-sig')
            print(f"\nğŸ‰ æ€» {len(combined)} æ¡ä¿å­˜")
    else:
        crawler.crawl_fund_holdings(args.fund_code, args.years, args.topline)

if __name__ == "__main__":
    main()
