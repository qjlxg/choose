import pandas as pd
import glob
import os
import sys
import logging
from datetime import datetime
import yaml
from dataclasses import dataclass
from typing import Optional, Dict, List, Tuple
import re

# é…ç½®æ—¥å¿—
def setup_logging():
    """è®¾ç½®æ—¥å¿—é…ç½®"""
    log_filename = f'fund_analysis_{datetime.now().strftime("%Y%m%d")}.log'
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_filename, encoding='utf-8'),
            logging.StreamHandler(sys.stdout)
        ]
    )
    logging.info("æ—¥å¿—ç³»ç»Ÿå·²å¯åŠ¨")

@dataclass
class AnalysisConfig:
    """åˆ†æé…ç½®ç±»"""
    data_path: str = 'fund_data'
    category_path: str = 'åˆ†ç±»è¡¨'
    output_file: str = 'analysis_report.md'
    min_weight_threshold: float = 0.5  # æœ€å°æƒé‡é˜ˆå€¼
    top_n_holdings: int = 10  # åˆ†æå‰Nå¤§æŒä»“
    concentration_threshold: float = 10.0  # é›†ä¸­åº¦å˜åŒ–é˜ˆå€¼
    
    @classmethod
    def from_file(cls, config_path: str = 'config.yaml'):
        """ä»é…ç½®æ–‡ä»¶åŠ è½½é…ç½®"""
        try:
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    config_dict = yaml.safe_load(f) or {}
                logging.info(f"æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
                return cls(**config_dict)
            else:
                logging.warning(f"é…ç½®æ–‡ä»¶ {config_path} ä¸å­˜åœ¨ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                return cls()
        except Exception as e:
            logging.error(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
            return cls()

def safe_read_excel(file_path, **kwargs):
    """å®‰å…¨çš„Excelæ–‡ä»¶è¯»å–å‡½æ•°"""
    try:
        return pd.read_excel(file_path, **kwargs)
    except FileNotFoundError:
        logging.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return None
    except PermissionError:
        logging.error(f"æ–‡ä»¶æƒé™é”™è¯¯: {file_path}")
        return None
    except Exception as e:
        logging.error(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return None

def safe_read_csv(file_path, **kwargs):
    """å®‰å…¨çš„CSVæ–‡ä»¶è¯»å–å‡½æ•°"""
    try:
        # å°è¯•ä¸åŒçš„ç¼–ç 
        encodings = ['utf-8', 'gbk', 'utf-8-sig']
        for encoding in encodings:
            try:
                return pd.read_csv(file_path, encoding=encoding, **kwargs)
            except UnicodeDecodeError:
                continue
        # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼ŒæŠ›å‡ºå¼‚å¸¸
        raise UnicodeDecodeError("csv", b"", 0, 0, "æ— æ³•è§£ç ")
    except FileNotFoundError:
        logging.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return None
    except PermissionError:
        logging.error(f"æ–‡ä»¶æƒé™é”™è¯¯: {file_path}")
        return None
    except Exception as e:
        logging.error(f"è¯»å–æ–‡ä»¶ {file_path} æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
        return None

def load_stock_categories(category_path):
    """
    éå†æŒ‡å®šç›®å½•ï¼ŒåŠ è½½æ‰€æœ‰ .xlsx æ ¼å¼çš„è‚¡ç¥¨åˆ†ç±»è¡¨ã€‚
    
    Args:
        category_path (str): åŒ…å«åˆ†ç±»è¡¨çš„ç›®å½•è·¯å¾„ã€‚
        
    Returns:
        dict: ä¸€ä¸ªå­—å…¸ï¼Œé”®ä¸ºè‚¡ç¥¨ä»£ç ï¼Œå€¼ä¸ºå…¶æ‰€å±çš„åˆ†ç±»ã€‚
    """
    all_categories = {}
    xlsx_files = glob.glob(os.path.join(category_path, "*.xlsx"))
    
    if not xlsx_files:
        logging.warning(f"æœªåœ¨ '{category_path}' ç›®å½•ä¸­æ‰¾åˆ°ä»»ä½• XLSX æ–‡ä»¶ã€‚")
        return all_categories

    for f in xlsx_files:
        try:
            category_name = os.path.basename(f).split('.')[0]
            logging.info(f"æ­£åœ¨åŠ è½½åˆ†ç±»æ–‡ä»¶: {category_name}")
            
            df = safe_read_excel(f, header=0, engine='openpyxl')
            if df is None:
                continue
                
            if 'è‚¡ç¥¨ä»£ç ' not in df.columns or 'è‚¡ç¥¨åç§°' not in df.columns:
                logging.warning(f"æ–‡ä»¶ {f} ç¼ºå°‘å…³é”®åˆ— 'è‚¡ç¥¨ä»£ç ' æˆ– 'è‚¡ç¥¨åç§°'ï¼Œè·³è¿‡ã€‚")
                continue
            
            # æ•°æ®æ¸…æ´—
            df = df.dropna(subset=['è‚¡ç¥¨ä»£ç '])
            df['è‚¡ç¥¨ä»£ç '] = df['è‚¡ç¥¨ä»£ç '].astype(str).str.strip().str.zfill(6)
            
            # é¿å…é‡å¤è¦†ç›–ï¼Œä¿ç•™ç¬¬ä¸€ä¸ªåˆ†ç±»
            added_count = 0
            for code in df['è‚¡ç¥¨ä»£ç ']:
                if code not in all_categories:
                    all_categories[code] = category_name
                    added_count += 1
            
            logging.info(f"ä» {category_name} åŠ è½½äº† {added_count} ä¸ªè‚¡ç¥¨åˆ†ç±»")
        except Exception as e:
            logging.error(f"è¯»å–åˆ†ç±»æ–‡ä»¶ {f} æ—¶å‡ºé”™: {e}")
            continue
            
    logging.info(f"æ€»å…±åŠ è½½äº† {len(all_categories)} ä¸ªè‚¡ç¥¨çš„åˆ†ç±»ä¿¡æ¯")
    return all_categories

def preprocess_fund_data(df: pd.DataFrame, fund_code: str) -> Optional[pd.DataFrame]:
    """
    ç»Ÿä¸€çš„æ•°æ®é¢„å¤„ç†å‡½æ•°
    
    Args:
        df: åŸå§‹æ•°æ®æ¡†
        fund_code: åŸºé‡‘ä»£ç 
        
    Returns:
        å¤„ç†åçš„æ•°æ®æ¡†
    """
    try:
        # åˆ—åæ˜ å°„
        column_mapping = {
            'å å‡€å€¼ æ¯”ä¾‹': 'å å‡€å€¼æ¯”ä¾‹', 
            'å å‡€å€¼æ¯”ä¾‹': 'å å‡€å€¼æ¯”ä¾‹',
            'æŒä»“å¸‚å€¼ ï¼ˆä¸‡å…ƒï¼‰': 'æŒä»“å¸‚å€¼',
            'æŒä»“å¸‚å€¼': 'æŒä»“å¸‚å€¼',
            'å¸‚å€¼': 'æŒä»“å¸‚å€¼',
            'æŒä»“å¸‚å€¼ ï¼ˆä¸‡å…ƒäººæ°‘å¸ï¼‰': 'æŒä»“å¸‚å€¼',
            'è‚¡ç¥¨åç§°': 'è‚¡ç¥¨åç§°',
            'è‚¡ç¥¨ä»£ç ': 'è‚¡ç¥¨ä»£ç ',
            'å­£åº¦': 'å­£åº¦'
        }
        
        df.columns = [column_mapping.get(col, col) for col in df.columns]

        # éªŒè¯å¿…è¦åˆ—
        required_cols = ['è‚¡ç¥¨ä»£ç ', 'è‚¡ç¥¨åç§°', 'å å‡€å€¼æ¯”ä¾‹', 'å­£åº¦']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise KeyError(f"ç¼ºå°‘å…³é”®åˆ—: {missing_cols}")

        # åˆ é™¤ç©ºè¡Œ
        df = df.dropna(subset=required_cols)
        if df.empty:
            logging.warning(f"æ•°æ®é¢„å¤„ç†åä¸ºç©ºï¼Œè·³è¿‡æ–‡ä»¶")
            return None

        # è‚¡ç¥¨ä»£ç æ ‡å‡†åŒ–
        df['è‚¡ç¥¨ä»£ç '] = df['è‚¡ç¥¨ä»£ç '].astype(str).str.strip().str.zfill(6)
        
        # å å‡€å€¼æ¯”ä¾‹æ¸…æ´—
        df['å å‡€å€¼æ¯”ä¾‹'] = (df['å å‡€å€¼æ¯”ä¾‹']
                           .astype(str)
                           .str.replace(r'[%ï¼Œ]', '', regex=True)
                           .str.replace(',', '', regex=False)
                           .pipe(pd.to_numeric, errors='coerce'))
        
        # åˆ é™¤æ— æ•ˆçš„å å‡€å€¼æ¯”ä¾‹æ•°æ®
        df = df.dropna(subset=['å å‡€å€¼æ¯”ä¾‹'])
        df = df[df['å å‡€å€¼æ¯”ä¾‹'] >= 0]  # è¿‡æ»¤è´Ÿå€¼
        
        # å­£åº¦æ ¼å¼æ ‡å‡†åŒ–
        df['å­£åº¦'] = df['å­£åº¦'].astype(str).str.strip()
        df['å­£åº¦'] = df['å­£åº¦'].str.replace('å¹´', '-Q')
        df['å­£åº¦'] = df['å­£åº¦'].str.replace('ç¬¬', '').str.replace('å­£åº¦', 'Q')
        
        # æ·»åŠ åŸºé‡‘ä»£ç 
        df['åŸºé‡‘ä»£ç '] = fund_code
        
        # è¿‡æ»¤æœ€å°æƒé‡
        df = df[df['å å‡€å€¼æ¯”ä¾‹'] >= 0.01]  # è¿‡æ»¤æƒé‡å°äº0.01%çš„æŒä»“
        
        logging.info(f"æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œä¿ç•™ {len(df)} æ¡æœ‰æ•ˆè®°å½•")
        return df
        
    except Exception as e:
        logging.error(f"æ•°æ®é¢„å¤„ç†å¤±è´¥: {e}")
        return None

def get_stock_industry(stock_code: str, stock_categories: Dict, sector_mapping: Dict) -> str:
    """è·å–è‚¡ç¥¨è¡Œä¸šåˆ†ç±»ï¼Œæ”¯æŒå¤šçº§åˆ†ç±»"""
    code_str = str(stock_code).zfill(6)
    
    # ä¼˜å…ˆä½¿ç”¨è¯¦ç»†åˆ†ç±»
    if stock_categories and code_str in stock_categories:
        return stock_categories[code_str]
    
    # ä½¿ç”¨æ¿å—åˆ†ç±»
    prefix = code_str[:3]
    return sector_mapping.get(prefix, 'å…¶ä»–')

def parse_filename(filename: str) -> Optional[Tuple[str, str]]:
    """
    è§£ææ–‡ä»¶åï¼Œæå–å­£åº¦å’ŒåŸºé‡‘ä»£ç 
    
    Args:
        filename: æ–‡ä»¶å
        
    Returns:
        (å­£åº¦, åŸºé‡‘ä»£ç ) å…ƒç»„ï¼Œå¦‚æœè§£æå¤±è´¥è¿”å› None
    """
    try:
        # æœŸæœ›æ ¼å¼ï¼šå­£åº¦_åŸºé‡‘ä»£ç _æè¿°.csv
        # æˆ–è€…ï¼šåŸºé‡‘ä»£ç _å­£åº¦_æè¿°.csv
        parts = filename.split('_')
        if len(parts) < 2:
            return None
        
        # å°è¯•è¯†åˆ«å­£åº¦ï¼ˆåŒ…å«å¹´ä»½æˆ–å­£åº¦ä¿¡æ¯ï¼‰
        quarter_pattern = re.compile(r'\d{4}|Q[1-4]')
        
        quarter_candidate = None
        fund_code_candidate = None
        
        for i, part in enumerate(parts):
            if quarter_pattern.search(part):
                quarter_candidate = part
                fund_code_candidate = parts[i-1] if i > 0 else parts[0]
                break
            elif len(part) == 6 and part.isdigit():  # 6ä½æ•°å­—å¯èƒ½æ˜¯åŸºé‡‘ä»£ç 
                fund_code_candidate = part
                # æ£€æŸ¥å‰é¢çš„éƒ¨åˆ†æ˜¯å¦åŒ…å«å­£åº¦ä¿¡æ¯
                if i > 0 and quarter_pattern.search(parts[i-1]):
                    quarter_candidate = parts[i-1]
        
        if quarter_candidate and fund_code_candidate:
            return quarter_candidate, fund_code_candidate
        else:
            # å›é€€åˆ°åŸé€»è¾‘
            fund_code = parts[1] if len(parts) > 1 else parts[0]
            return "æœªçŸ¥å­£åº¦", fund_code
            
    except Exception as e:
        logging.warning(f"è§£ææ–‡ä»¶å {filename} å¤±è´¥: {e}")
        return None

def analyze_holdings():
    """
    éå† fund_data ç›®å½•ï¼Œå¯¹æ¯ä¸ªåŸºé‡‘ä»£ç çš„æŒä»“æ•°æ®è¿›è¡Œåˆå¹¶å’Œåˆ†æï¼Œ
    å¹¶å°†ç»“æœè¾“å‡ºåˆ° analysis_report.md æ–‡ä»¶ä¸­ã€‚
    """
    # åŠ è½½é…ç½®
    config = AnalysisConfig.from_file()
    setup_logging()
    
    logging.info(f"å¼€å§‹åŸºé‡‘æŒä»“åˆ†æï¼Œæ•°æ®è·¯å¾„: {config.data_path}")
    logging.info(f"åˆ†ç±»è¡¨è·¯å¾„: {config.category_path}")
    
    base_path = config.data_path
    category_path = config.category_path
    all_files = glob.glob(os.path.join(base_path, "*.csv"))

    if not all_files:
        logging.error("æœªåœ¨ 'fund_data' ç›®å½•ä¸­æ‰¾åˆ°ä»»ä½• CSV æ–‡ä»¶ã€‚")
        return

    # åŠ è½½è‚¡ç¥¨åˆ†ç±»
    stock_categories = load_stock_categories(category_path)
    if not stock_categories:
        logging.warning("æœªåŠ è½½åˆ°ä»»ä½•è‚¡ç¥¨åˆ†ç±»æ•°æ®ï¼Œå°†ä½¿ç”¨é»˜è®¤æ¿å—åˆ†æã€‚")
        sector_mapping = {
            '688': 'ç§‘åˆ›æ¿', '300': 'åˆ›ä¸šæ¿', '002': 'ä¸­å°æ¿',
            '000': 'ä¸»æ¿', '600': 'ä¸»æ¿', '601': 'ä¸»æ¿',
            '603': 'ä¸»æ¿', '605': 'ä¸»æ¿', '005': 'ä¸»æ¿', '006': 'ä¸»æ¿',
        }
        use_detailed_categories = False
    else:
        sector_mapping = {
            '688': 'ç§‘åˆ›æ¿', '300': 'åˆ›ä¸šæ¿', '002': 'ä¸­å°æ¿',
            '000': 'ä¸»æ¿', '600': 'ä¸»æ¿', '601': 'ä¸»æ¿',
            '603': 'ä¸»æ¿', '605': 'ä¸»æ¿', '005': 'ä¸»æ¿', '006': 'ä¸»æ¿',
        }
        use_detailed_categories = True

    # æŒ‰åŸºé‡‘ä»£ç åˆ†ç»„æ–‡ä»¶ï¼ˆå¢å¼ºç‰ˆï¼‰
    fund_files = {}
    for f in all_files:
        try:
            filename = os.path.basename(f)
            parsed = parse_filename(filename)
            
            if parsed:
                quarter, fund_code = parsed
                if fund_code not in fund_files:
                    fund_files[fund_code] = {}
                if quarter not in fund_files[fund_code]:
                    fund_files[fund_code][quarter] = []
                fund_files[fund_code][quarter].append(f)
            else:
                # å›é€€åˆ°åŸé€»è¾‘
                fund_code = os.path.basename(f).split('_')[1]
                if fund_code not in fund_files:
                    fund_files[fund_code] = {}
                if 'æœªçŸ¥å­£åº¦' not in fund_files[fund_code]:
                    fund_files[fund_code]['æœªçŸ¥å­£åº¦'] = []
                fund_files[fund_code]['æœªçŸ¥å­£åº¦'].append(f)
                
        except IndexError:
            logging.warning(f"æ–‡ä»¶åæ ¼å¼ä¸æ­£ç¡®ï¼Œè·³è¿‡ï¼š{f}")
            continue

    if not fund_files:
        logging.error("æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆåŸºé‡‘æ–‡ä»¶ã€‚")
        return

    logging.info(f"å‘ç° {len(fund_files)} ä¸ªåŸºé‡‘ï¼Œ{sum(len(files) for files in fund_files.values())} ä¸ªæ–‡ä»¶")

    report = []
    report.append(f"# åŸºé‡‘æŒä»“ç»¼åˆåˆ†ææŠ¥å‘Š")
    report.append(f"**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report.append(f"**åˆ†æåŸºé‡‘æ•°é‡**: {len(fund_files)}")
    report.append(f"**æ•°æ®è·¯å¾„**: {base_path}")
    report.append(f"**åˆ†ç±»æ•°æ®**: {'è¯¦ç»†åˆ†ç±»' if use_detailed_categories else 'é»˜è®¤æ¿å—åˆ†ç±»'}")
    report.append("---")

    for fund_code, quarter_files in fund_files.items():
        logging.info(f"æ­£åœ¨åˆ†æåŸºé‡‘ {fund_code}ï¼Œæ¶‰åŠ {len(quarter_files)} ä¸ªå­£åº¦")
        
        df_list = []
        for quarter, files in quarter_files.items():
            for f in files:
                try:
                    df = safe_read_csv(f, engine='python')
                    if df is None:
                        continue
                        
                    # é¢„å¤„ç†æ•°æ®
                    processed_df = preprocess_fund_data(df, fund_code)
                    if processed_df is not None:
                        processed_df['å­£åº¦'] = quarter
                        df_list.append(processed_df)
                        
                except KeyError as e:
                    logging.error(f"è¯»å–æ–‡ä»¶ {f} æ—¶å‡ºé”™ï¼šç¼ºå°‘å…³é”®åˆ— {e}")
                    continue
                except Exception as e:
                    logging.error(f"è¯»å–æ–‡ä»¶ {f} æ—¶å‡ºé”™ï¼š{e}")
                    continue
        
        if not df_list:
            logging.warning(f"åŸºé‡‘ {fund_code} æ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡")
            continue
            
        # åˆå¹¶æ•°æ®
        combined_df = pd.concat(df_list, ignore_index=True)
        
        # æ ‡å‡†åŒ–å­£åº¦æ ¼å¼
        combined_df['å­£åº¦'] = combined_df['å­£åº¦'].str.replace('å¹´', '-Q')
        combined_df['å­£åº¦'] = combined_df['å­£åº¦'].str.replace('ç¬¬', '').str.replace('å­£åº¦', 'Q')
        
        # æå–å¹´ä»½å’Œå­£åº¦ç¼–å·
        combined_df['å¹´ä»½'] = combined_df['å­£åº¦'].str.extract(r'(\d{4})').astype('Int64')
        combined_df['å­£åº¦ç¼–å·'] = combined_df['å­£åº¦'].str.extract(r'Q([1-4])').astype('Int64')
        combined_df.sort_values(by=['å¹´ä»½', 'å­£åº¦ç¼–å·'], inplace=True)
        combined_df.reset_index(drop=True, inplace=True)

        # ç”Ÿæˆå•ä¸ªåŸºé‡‘æŠ¥å‘Š
        fund_report = generate_fund_report(fund_code, combined_df, stock_categories, 
                                         sector_mapping, use_detailed_categories, config)
        report.extend(fund_report)
        report.append("\n" + "="*80 + "\n")

    # å†™å…¥æŠ¥å‘Š
    try:
        with open(config.output_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report))
        logging.info(f"åˆ†ææŠ¥å‘Šå·²ä¿å­˜è‡³: {config.output_file}")
    except Exception as e:
        logging.error(f"ä¿å­˜æŠ¥å‘Šå¤±è´¥: {e}")

def generate_fund_report(fund_code: str, combined_df: pd.DataFrame, 
                        stock_categories: Dict, sector_mapping: Dict,
                        use_detailed_categories: bool, config: AnalysisConfig) -> List[str]:
    """ç”Ÿæˆå•ä¸ªåŸºé‡‘çš„è¯¦ç»†åˆ†ææŠ¥å‘Š"""
    
    report = []
    report.append(f"\n## åŸºé‡‘ä»£ç : {fund_code} æŒä»“åˆ†ææŠ¥å‘Š")
    report.append(f"**åˆ†ææ—¥æœŸ**: {datetime.now().strftime('%Y-%m-%d')}")
    report.append("---")
    
    # åŸºæœ¬ä¿¡æ¯ç»Ÿè®¡
    total_quarters = combined_df['å­£åº¦'].nunique()
    total_stocks = combined_df['è‚¡ç¥¨ä»£ç '].nunique()
    total_weight = combined_df['å å‡€å€¼æ¯”ä¾‹'].sum()
    avg_quarter_weight = total_weight / total_quarters
    
    report.append(f"**åŸºæœ¬ä¿¡æ¯**:")
    report.append(f"- åˆ†ææœŸé—´: {total_quarters} ä¸ªå­£åº¦")
    report.append(f"- æ¶‰åŠè‚¡ç¥¨: {total_stocks} åª")
    report.append(f"- æ€»æŒä»“æƒé‡: {total_weight:.1f}%")
    report.append(f"- å¹³å‡æ¯å­£åº¦æƒé‡: {avg_quarter_weight:.1f}%")
    report.append("")
    
    # æ·»åŠ è¡Œä¸šåˆ†ç±»
    combined_df['è¡Œä¸š'] = combined_df['è‚¡ç¥¨ä»£ç '].apply(
        lambda x: get_stock_industry(x, stock_categories, sector_mapping)
    )
    
    # æœªåˆ†ç±»è‚¡ç¥¨å¤„ç†
    unclassified_stocks = combined_df[combined_df['è¡Œä¸š'] == 'å…¶ä»–']
    if not unclassified_stocks.empty:
        report.append("\n### ğŸš¨ æœªèƒ½åŒ¹é…åˆ°è¡Œä¸šåˆ†ç±»çš„è‚¡ç¥¨åˆ—è¡¨")
        report.append("---")
        unclassified_summary = unclassified_stocks.groupby('å­£åº¦')['å å‡€å€¼æ¯”ä¾‹'].sum().round(2)
        
        for quarter, group in unclassified_stocks.groupby('å­£åº¦'):
            quarter_weight = unclassified_summary.get(quarter, 0)
            report.append(f"#### {quarter} (æƒé‡: {quarter_weight:.2f}%)")
            # åªæ˜¾ç¤ºæƒé‡è¶…è¿‡1%çš„æœªåˆ†ç±»è‚¡ç¥¨
            significant_stocks = group[group['å å‡€å€¼æ¯”ä¾‹'] > 1].sort_values('å å‡€å€¼æ¯”ä¾‹', ascending=False)
            for index, row in significant_stocks.iterrows():
                report.append(f"- **{row['è‚¡ç¥¨åç§°']}** ({row['è‚¡ç¥¨ä»£ç ']}): {row['å å‡€å€¼æ¯”ä¾‹']:.2f}%")
            if len(significant_stocks) < len(group):
                report.append(f"- ... è¿˜æœ‰ {len(group) - len(significant_stocks)} åªæƒé‡è¾ƒä½çš„è‚¡ç¥¨")
        report.append("---\n")

    # 1. é‡ä»“è‚¡å˜åŠ¨åˆ†æ
    report.extend(analyze_top_holdings_detailed(combined_df, config.top_n_holdings))
    
    # 2. è¡Œä¸šåå¥½å’ŒæŒä»“é›†ä¸­åº¦åˆ†æ
    report.extend(analyze_sector_and_concentration(combined_df))
    
    # 3. è¶‹åŠ¿æ€»ç»“å’ŒæŠ•èµ„å»ºè®®
    report.extend(generate_trend_insights_detailed(combined_df, use_detailed_categories))
    
    return report

def analyze_top_holdings_detailed(df: pd.DataFrame, top_n: int = 10) -> List[str]:
    """è¯¦ç»†çš„é‡ä»“è‚¡å˜åŠ¨åˆ†æ"""
    report = []
    report.append("\n### 1. é‡ä»“è‚¡å˜åŠ¨")
    report.append("---")
    
    quarters = sorted(df['å­£åº¦'].unique())
    if len(quarters) < 2:
        report.append("**æ³¨æ„**: æ•°æ®ä¸è¶³ä»¥åˆ†ææŒä»“å˜åŠ¨ï¼ˆå°‘äº2ä¸ªå­£åº¦ï¼‰")
        return report
    
    for i in range(len(quarters) - 1):
        current_q = quarters[i]
        next_q = quarters[i+1]
        
        current_holdings = set(df[df['å­£åº¦'] == current_q]['è‚¡ç¥¨ä»£ç '])
        next_holdings = set(df[df['å­£åº¦'] == next_q]['è‚¡ç¥¨ä»£ç '])
        
        new_additions = next_holdings - current_holdings
        removed = current_holdings - next_holdings
        common = current_holdings & next_holdings
        
        report.append(f"#### ä» {current_q} åˆ° {next_q} çš„å˜åŠ¨")
        report.append(f"- **æ–°å¢è‚¡ç¥¨**: {len(new_additions)} åª")
        report.append(f"- **ç§»é™¤è‚¡ç¥¨**: {len(removed)} åª") 
        report.append(f"- **ä¿æŒæŒä»“**: {len(common)} åª")
        
        # è¯¦ç»†åˆ—å‡ºå˜åŠ¨
        if new_additions:
            new_add_stocks = (df[(df['å­£åº¦'] == next_q) & 
                               (df['è‚¡ç¥¨ä»£ç '].isin(new_additions))]
                            .nlargest(5, 'å å‡€å€¼æ¯”ä¾‹')[['è‚¡ç¥¨åç§°', 'è‚¡ç¥¨ä»£ç ', 'å å‡€å€¼æ¯”ä¾‹', 'è¡Œä¸š']])
            report.append("\n**æ–°å¢é‡ä»“è‚¡** (å‰5å):")
            for _, row in new_add_stocks.iterrows():
                report.append(f"  - **{row['è‚¡ç¥¨åç§°']}** ({row['è‚¡ç¥¨ä»£ç ']}, {row['è¡Œä¸š']}): {row['å å‡€å€¼æ¯”ä¾‹']:.2f}%")
            if len(new_additions) > 5:
                report.append(f"  - ... è¿˜æœ‰ {len(new_additions) - 5} åªæ–°å¢è‚¡ç¥¨")
        
        if removed:
            removed_stocks = (df[(df['å­£åº¦'] == current_q) & 
                               (df['è‚¡ç¥¨ä»£ç '].isin(removed))]
                            .nlargest(5, 'å å‡€å€¼æ¯”ä¾‹')[['è‚¡ç¥¨åç§°', 'è‚¡ç¥¨ä»£ç ', 'å å‡€å€¼æ¯”ä¾‹', 'è¡Œä¸š']])
            report.append("\n**ç§»é™¤é‡ä»“è‚¡** (å‰5å):")
            for _, row in removed_stocks.iterrows():
                report.append(f"  - **{row['è‚¡ç¥¨åç§°']}** ({row['è‚¡ç¥¨ä»£ç ']}, {row['è¡Œä¸š']}): {row['å å‡€å€¼æ¯”ä¾‹']:.2f}%")
            if len(removed) > 5:
                report.append(f"  - ... è¿˜æœ‰ {len(removed) - 5} åªç§»é™¤è‚¡ç¥¨")
        
        report.append("")
    
    # å„å­£åº¦å‰Nå¤§é‡ä»“è‚¡
    report.append(f"#### å„å­£åº¦å‰{top_n}å¤§é‡ä»“è‚¡")
    for quarter in quarters:
        quarter_data = df[df['å­£åº¦'] == quarter].nlargest(top_n, 'å å‡€å€¼æ¯”ä¾‹')
        total_weight = quarter_data['å å‡€å€¼æ¯”ä¾‹'].sum()
        
        report.append(f"\n**{quarter} å‰{top_n}å¤§æŒä»“** (æ€»æƒé‡: {total_weight:.1f}%):")
        for _, row in quarter_data.iterrows():
            report.append(f"  - **{row['è‚¡ç¥¨åç§°']}** ({row['è‚¡ç¥¨ä»£ç ']}, {row['è¡Œä¸š']}): {row['å å‡€å€¼æ¯”ä¾‹']:.2f}%")
    
    return report

def analyze_sector_and_concentration(df: pd.DataFrame) -> List[str]:
    """è¡Œä¸šåå¥½å’ŒæŒä»“é›†ä¸­åº¦åˆ†æ"""
    report = []
    report.append("\n### 2. è¡Œä¸šåå¥½å’ŒæŒä»“é›†ä¸­åº¦")
    report.append("---")
    
    # è¡Œä¸šåˆ†æ
    report.append("#### 2.1 è¡Œä¸šåå¥½ï¼ˆå å‡€å€¼æ¯”ä¾‹ä¹‹å’Œï¼‰")
    
    # æŒ‰å¹´ä»½å’Œè¡Œä¸šæ±‡æ€»
    sector_summary = df.groupby(['å¹´ä»½', 'è¡Œä¸š'])['å å‡€å€¼æ¯”ä¾‹'].sum().unstack(fill_value=0)
    
    if not sector_summary.empty:
        sector_summary = sector_summary.astype(float)
        
        # æ ¼å¼åŒ–è¡¨æ ¼
        formatted_sector_summary = sector_summary.map(lambda x: f"{x:.2f}%" if x > 0 else "")
        report.append(formatted_sector_summary.to_markdown())
        
        # è¡Œä¸šå¹³å‡æƒé‡
        avg_sector_weights = sector_summary.mean().sort_values(ascending=False)
        top_sectors = avg_sector_weights[avg_sector_weights > 2]  # å¹³å‡æƒé‡>2%
        
        if len(top_sectors) > 0:
            report.append(f"\n**ä¸»è¦è¡Œä¸šé…ç½®** (å¹³å‡æƒé‡ï¼Œå‰{len(top_sectors)}å):")
            for sector, weight in top_sectors.items():
                max_weight = sector_summary[sector].max()
                min_weight = sector_summary[sector].min()
                report.append(f"- **{sector}**: å¹³å‡ {weight:.1f}%ï¼ŒèŒƒå›´ [{min_weight:.1f}% - {max_weight:.1f}%]")
    else:
        report.append("**æ•°æ®ä¸è¶³**: æ— æ³•ç”Ÿæˆè¡Œä¸šåˆ†æè¡¨æ ¼")
    
    report.append("")
    
    # é›†ä¸­åº¦åˆ†æ
    report.append("#### 2.2 å‰åå¤§æŒä»“é›†ä¸­åº¦ï¼ˆå å‡€å€¼æ¯”ä¾‹ä¹‹å’Œï¼‰")
    concentration_summary = df.groupby('å­£åº¦')['å å‡€å€¼æ¯”ä¾‹'].sum()
    
    if not concentration_summary.empty:
        formatted_concentration_summary = pd.DataFrame({
            'å­£åº¦': concentration_summary.index,
            'å‰åå¤§æŒä»“é›†ä¸­åº¦': concentration_summary.map(lambda x: f"{x:.2f}%")
        })
        report.append(formatted_concentration_summary.to_markdown(index=False))
        
        # å„å­£åº¦å‰10å¤§æŒä»“æ˜ç»†
        report.append(f"\n**å„å­£åº¦å‰10å¤§æŒä»“æ˜ç»†**:")
        for quarter in sorted(df['å­£åº¦'].unique()):
            quarter_top10 = (df[df['å­£åº¦'] == quarter]
                           .nlargest(10, 'å å‡€å€¼æ¯”ä¾‹')[['è‚¡ç¥¨åç§°', 'è‚¡ç¥¨ä»£ç ', 'å å‡€å€¼æ¯”ä¾‹', 'è¡Œä¸š']]
                           .round({'å å‡€å€¼æ¯”ä¾‹': 2}))
            total_weight = quarter_top10['å å‡€å€¼æ¯”ä¾‹'].sum()
            
            report.append(f"\n**{quarter}** (æ€»æƒé‡: {total_weight:.1f}%):")
            for _, row in quarter_top10.iterrows():
                report.append(f"  - **{row['è‚¡ç¥¨åç§°']}** ({row['è‚¡ç¥¨ä»£ç ']}, {row['è¡Œä¸š']}): {row['å å‡€å€¼æ¯”ä¾‹']:.2f}%")
    else:
        report.append("**æ•°æ®ä¸è¶³**: æ— æ³•ç”Ÿæˆé›†ä¸­åº¦åˆ†æ")
    
    report.append("")
    return report

def generate_trend_insights_detailed(df: pd.DataFrame, use_detailed_categories: bool) -> List[str]:
    """ç”Ÿæˆè¯¦ç»†çš„è¶‹åŠ¿æ€»ç»“å’ŒæŠ•èµ„å»ºè®®"""
    report = []
    report.append("\n### 3. è¶‹åŠ¿æ€»ç»“å’ŒæŠ•èµ„å»ºè®®")
    report.append("---")
    
    report.append("> **å…è´£å£°æ˜**ï¼šæœ¬æŠ¥å‘ŠåŸºäºå†å²æŒä»“æ•°æ®è¿›è¡Œåˆ†æï¼Œä¸æ„æˆä»»ä½•æŠ•èµ„å»ºè®®ã€‚æŠ•èµ„æœ‰é£é™©ï¼Œå…¥å¸‚éœ€è°¨æ…ã€‚")
    report.append("")
    
    concentration_summary = df.groupby('å­£åº¦')['å å‡€å€¼æ¯”ä¾‹'].sum()
    
    if len(concentration_summary) > 1:
        first_concentration = concentration_summary.iloc[0]
        last_concentration = concentration_summary.iloc[-1]
        concentration_diff = last_concentration - first_concentration
        
        if concentration_diff > 10:
            report.append("- **ğŸ”´ æŒä»“é›†ä¸­åº¦**ï¼šåœ¨åˆ†ææœŸå†…ï¼Œè¯¥åŸºé‡‘çš„æŒä»“é›†ä¸­åº¦æ˜¾è‘—**ä¸Šå‡**ï¼ˆ+{:.1f}%ï¼‰ï¼Œæ˜¾ç¤ºå‡ºæ›´å¼ºçš„é€‰è‚¡ä¿¡å¿ƒã€‚".format(concentration_diff))
        elif concentration_diff < -10:
            report.append(f"- **ğŸŸ¢ æŒä»“é›†ä¸­åº¦**ï¼šåœ¨åˆ†ææœŸå†…ï¼Œè¯¥åŸºé‡‘çš„æŒä»“é›†ä¸­åº¦æ˜¾è‘—**ä¸‹é™**ï¼ˆ{concentration_diff:.1f}%ï¼‰ï¼Œå¯èƒ½åœ¨åˆ†æ•£é£é™©ã€‚")
        else:
            report.append(f"- **ğŸŸ¡ æŒä»“é›†ä¸­åº¦**ï¼šè¯¥åŸºé‡‘çš„æŒä»“é›†ä¸­åº¦åœ¨åˆ†ææœŸå†…ç›¸å¯¹**ç¨³å®š**ï¼ˆå˜åŒ– {concentration_diff:.1f}%ï¼‰ã€‚")
    else:
        report.append("- **æŒä»“é›†ä¸­åº¦**ï¼šæ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†æé›†ä¸­åº¦å˜åŒ–è¶‹åŠ¿ã€‚")
    
    # è¡Œä¸šåå¥½åˆ†æ
    sector_summary = df.groupby(['å¹´ä»½', 'è¡Œä¸š'])['å å‡€å€¼æ¯”ä¾‹'].sum().unstack(fill_value=0)
    
    if len(sector_summary) > 1 and not sector_summary.empty:
        first_year_summary = sector_summary.iloc[0]
        last_year_summary = sector_summary.iloc[-1]
        
        try:
            first_dominant_sector = first_year_summary.idxmax()
            last_dominant_sector = last_year_summary.idxmax()
            
            if first_dominant_sector != last_dominant_sector:
                report.append(f"- **ğŸ”„ è¡Œä¸šåå¥½**ï¼šåŸºé‡‘çš„æŠ•èµ„åå¥½å‘ç”Ÿäº†æ˜æ˜¾å˜åŒ–ï¼Œä»**{first_dominant_sector}**ï¼ˆ{first_year_summary[first_dominant_sector]:.1f}%ï¼‰è½¬å‘äº†**{last_dominant_sector}**ï¼ˆ{last_year_summary[last_dominant_sector]:.1f}%ï¼‰ã€‚")
            else:
                report.append(f"- **ğŸ¯ è¡Œä¸šåå¥½**ï¼šè¯¥åŸºé‡‘åœ¨åˆ†ææœŸå†…ä¸»è¦åå‘äº**{first_dominant_sector}**è¡Œä¸šï¼ˆå¹³å‡æƒé‡ {first_year_summary[first_dominant_sector]:.1f}%ï¼‰ã€‚")
        except ValueError:
            report.append("- **è¡Œä¸šåå¥½**ï¼šç”±äºæ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†æè¡Œä¸šåå¥½å˜åŒ–ã€‚")
    elif not sector_summary.empty:
        dominant_sector = sector_summary.iloc[-1].idxmax()
        report.append(f"- **ğŸ¯ è¡Œä¸šåå¥½**ï¼šè¯¥åŸºé‡‘ä¸»è¦é…ç½®åœ¨**{dominant_sector}**è¡Œä¸šï¼ˆæƒé‡ {sector_summary.iloc[-1][dominant_sector]:.1f}%ï¼‰ã€‚")
    else:
        report.append("- **è¡Œä¸šåå¥½**ï¼šæ•°æ®ä¸è¶³ï¼Œæ— æ³•åˆ†æè¡Œä¸šé…ç½®æƒ…å†µã€‚")
    
    # æŒä»“ç¨³å®šæ€§åˆ†æ
    quarters = sorted(df['å­£åº¦'].unique())
    if len(quarters) >= 2:
        first_quarter_holdings = set(df[df['å­£åº¦'] == quarters[0]]['è‚¡ç¥¨ä»£ç '])
        last_quarter_holdings = set(df[df['å­£åº¦'] == quarters[-1]]['è‚¡ç¥¨ä»£ç '])
        
        retention_rate = len(first_quarter_holdings & last_quarter_holdings) / len(first_quarter_holdings) if first_quarter_holdings else 0
        
        if retention_rate > 0.7:
            report.append(f"- **ğŸ›¡ï¸ æŒä»“ç¨³å®šæ€§**ï¼šè¯¥åŸºé‡‘æŒä»“ç›¸å¯¹**ç¨³å®š**ï¼Œé¦–å°¾ä¸¤æœŸé‡å ç‡ {retention_rate:.0f}%ï¼Œæ˜¾ç¤ºå‡ºè¾ƒå¼ºçš„é•¿æœŸæŒè‚¡é£æ ¼ã€‚")
        elif retention_rate > 0.3:
            report.append(f"- **âš–ï¸ æŒä»“ç¨³å®šæ€§**ï¼šè¯¥åŸºé‡‘æŒä»“**ä¸­ç­‰è°ƒæ•´**ï¼Œé¦–å°¾ä¸¤æœŸé‡å ç‡ {retention_rate:.0f}%ï¼Œæ˜¾ç¤ºé€‚åº¦çš„åŠ¨æ€è°ƒæ•´ã€‚")
        else:
            report.append(f"- **ğŸ”€ æŒä»“ç¨³å®šæ€§**ï¼šè¯¥åŸºé‡‘æŒä»“**é¢‘ç¹è°ƒæ•´**ï¼Œé¦–å°¾ä¸¤æœŸé‡å ç‡ä»… {retention_rate:.0f}%ï¼Œæ˜¾ç¤ºè¾ƒå¼ºçš„äº¤æ˜“æ´»è·ƒåº¦ã€‚")
    
    # æ€»ç»“ä¸å»ºè®®
    report.append("\n**ğŸ“‹ æŠ•èµ„æ€»ç»“ä¸å»ºè®®ï¼š**")
    report.append("1. **æŠ•èµ„é£æ ¼**ï¼šæ ¹æ®æŒä»“é›†ä¸­åº¦å’Œå˜åŠ¨é¢‘ç‡åˆ¤æ–­è¯¥åŸºé‡‘çš„æŠ•èµ„é£æ ¼ï¼ˆé›†ä¸­å‹/åˆ†æ•£å‹ï¼Œç¨³å®šå‹/æ´»è·ƒå‹ï¼‰")
    report.append("2. **è¡Œä¸šåŒ¹é…**ï¼šè¯„ä¼°åŸºé‡‘çš„ä¸»è¦è¡Œä¸šé…ç½®æ˜¯å¦ä¸æ‚¨çš„æŠ•èµ„åå¥½å’Œé£é™©æ‰¿å—èƒ½åŠ›åŒ¹é…")
    report.append("3. **é£é™©ç®¡ç†**ï¼šå…³æ³¨æŒä»“é›†ä¸­åº¦å˜åŒ–å¯¹ç»„åˆé£é™©çš„å½±å“")
    report.append("4. **ç»¼åˆè€ƒé‡**ï¼šå»ºè®®ç»“åˆåŸºé‡‘çš„å†å²ä¸šç»©ã€åŸºé‡‘ç»ç†ç»éªŒã€è´¹ç‡ç»“æ„ç­‰å› ç´ è¿›è¡Œç»¼åˆè¯„ä¼°")
    report.append("5. **åŠ¨æ€è·Ÿè¸ª**ï¼šå®šæœŸå…³æ³¨åŸºé‡‘æŒä»“è°ƒæ•´ï¼ŒåŠæ—¶äº†è§£æŠ•èµ„ç­–ç•¥çš„å˜åŒ–")
    
    return report

if __name__ == "__main__":
    try:
        analyze_holdings()
        print(f"\nâœ… åˆ†æå®Œæˆï¼")
        print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ç”Ÿæˆï¼šanalysis_report.md")
        print(f"ğŸ“‹ æ—¥å¿—æ–‡ä»¶ï¼šfund_analysis_{datetime.now().strftime('%Y%m%d')}.log")
        print("\n" + "="*50)
        print("ğŸ“ æ•°æ®å‡†å¤‡å»ºè®®ï¼š")
        print("â€¢ ç¡®ä¿ fund_data ç›®å½•ä¸­çš„CSVæ–‡ä»¶å‘½åæ ¼å¼ä¸ºï¼šå­£åº¦_åŸºé‡‘ä»£ç _æè¿°.csv")
        print("â€¢ åˆ†ç±»è¡¨ç›®å½•ä¸­çš„Excelæ–‡ä»¶åº”åŒ…å«'è‚¡ç¥¨ä»£ç 'å’Œ'è‚¡ç¥¨åç§°'åˆ—")
        print("\nâš™ï¸ é…ç½®ä¼˜åŒ–å»ºè®®ï¼š")
        print("â€¢ å¯åˆ›å»º config.yaml æ–‡ä»¶æ¥è‡ªå®šä¹‰åˆ†æå‚æ•°")
        print("â€¢ å¯æ·»åŠ æ›´å¤šè‡ªå®šä¹‰çš„è¡Œä¸šåˆ†ç±»è§„åˆ™")
        print("\nğŸš€ æ‰©å±•åŠŸèƒ½å»ºè®®ï¼š")
        print("â€¢ å¯ä»¥æ·»åŠ ä¸šç»©å¯¹æ¯”åˆ†æ")
        print("â€¢ æ”¯æŒæ›´å¤šçš„æ•°æ®å¯è§†åŒ–è¾“å‡º") 
        print("â€¢ å¢åŠ åŸºé‡‘ç»ç†é£æ ¼åˆ†æ")
        print("="*50)
    except KeyboardInterrupt:
        print("\nâš ï¸  ç”¨æˆ·ä¸­æ–­åˆ†æè¿‡ç¨‹")
    except Exception as e:
        logging.error(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        print(f"\nâŒ åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        print("è¯·æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶è·å–è¯¦ç»†ä¿¡æ¯")
